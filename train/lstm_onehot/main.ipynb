{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os, sys\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from functions import build_vocab, encode_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50000 text samples from CSV files.\n",
      "Number of names replaced: 20105\n",
      "Vocabulary size: 5001\n",
      "Number of sequences: 325969\n",
      "Using device: cuda\n",
      "Starting model training...\n",
      "Epoch [1/10], Batch [100/10187], Loss: 6.6217\n",
      "Epoch [1/10], Batch [200/10187], Loss: 6.5695\n",
      "Epoch [1/10], Batch [300/10187], Loss: 5.6999\n",
      "Epoch [1/10], Batch [400/10187], Loss: 5.7570\n",
      "Epoch [1/10], Batch [500/10187], Loss: 5.8981\n",
      "Epoch [1/10], Batch [600/10187], Loss: 6.0360\n",
      "Epoch [1/10], Batch [700/10187], Loss: 6.2257\n",
      "Epoch [1/10], Batch [800/10187], Loss: 5.8280\n",
      "Epoch [1/10], Batch [900/10187], Loss: 5.9156\n",
      "Epoch [1/10], Batch [1000/10187], Loss: 6.0553\n",
      "Epoch [1/10], Batch [1100/10187], Loss: 5.7014\n",
      "Epoch [1/10], Batch [1200/10187], Loss: 5.7064\n",
      "Epoch [1/10], Batch [1300/10187], Loss: 6.4004\n",
      "Epoch [1/10], Batch [1400/10187], Loss: 5.9957\n",
      "Epoch [1/10], Batch [1500/10187], Loss: 6.3230\n",
      "Epoch [1/10], Batch [1600/10187], Loss: 6.0752\n",
      "Epoch [1/10], Batch [1700/10187], Loss: 6.0973\n",
      "Epoch [1/10], Batch [1800/10187], Loss: 6.4305\n",
      "Epoch [1/10], Batch [1900/10187], Loss: 6.3281\n",
      "Epoch [1/10], Batch [2000/10187], Loss: 6.4627\n",
      "Epoch [1/10], Batch [2100/10187], Loss: 5.1107\n",
      "Epoch [1/10], Batch [2200/10187], Loss: 5.4752\n",
      "Epoch [1/10], Batch [2300/10187], Loss: 4.9551\n",
      "Epoch [1/10], Batch [2400/10187], Loss: 6.6882\n",
      "Epoch [1/10], Batch [2500/10187], Loss: 5.3074\n",
      "Epoch [1/10], Batch [2600/10187], Loss: 6.5375\n",
      "Epoch [1/10], Batch [2700/10187], Loss: 5.3442\n",
      "Epoch [1/10], Batch [2800/10187], Loss: 5.8649\n",
      "Epoch [1/10], Batch [2900/10187], Loss: 4.7779\n",
      "Epoch [1/10], Batch [3000/10187], Loss: 6.4098\n",
      "Epoch [1/10], Batch [3100/10187], Loss: 4.8883\n",
      "Epoch [1/10], Batch [3200/10187], Loss: 5.0588\n",
      "Epoch [1/10], Batch [3300/10187], Loss: 5.8689\n",
      "Epoch [1/10], Batch [3400/10187], Loss: 5.4809\n",
      "Epoch [1/10], Batch [3500/10187], Loss: 6.6135\n",
      "Epoch [1/10], Batch [3600/10187], Loss: 5.6577\n",
      "Epoch [1/10], Batch [3700/10187], Loss: 4.9896\n",
      "Epoch [1/10], Batch [3800/10187], Loss: 4.9582\n",
      "Epoch [1/10], Batch [3900/10187], Loss: 5.8127\n",
      "Epoch [1/10], Batch [4000/10187], Loss: 4.5390\n",
      "Epoch [1/10], Batch [4100/10187], Loss: 5.9391\n",
      "Epoch [1/10], Batch [4200/10187], Loss: 5.8935\n",
      "Epoch [1/10], Batch [4300/10187], Loss: 5.7494\n",
      "Epoch [1/10], Batch [4400/10187], Loss: 5.1233\n",
      "Epoch [1/10], Batch [4500/10187], Loss: 4.8869\n",
      "Epoch [1/10], Batch [4600/10187], Loss: 5.0380\n",
      "Epoch [1/10], Batch [4700/10187], Loss: 5.2262\n",
      "Epoch [1/10], Batch [4800/10187], Loss: 5.7894\n",
      "Epoch [1/10], Batch [4900/10187], Loss: 4.2785\n",
      "Epoch [1/10], Batch [5000/10187], Loss: 5.3367\n",
      "Epoch [1/10], Batch [5100/10187], Loss: 5.9225\n",
      "Epoch [1/10], Batch [5200/10187], Loss: 4.5010\n",
      "Epoch [1/10], Batch [5300/10187], Loss: 6.3284\n",
      "Epoch [1/10], Batch [5400/10187], Loss: 5.2442\n",
      "Epoch [1/10], Batch [5500/10187], Loss: 5.8884\n",
      "Epoch [1/10], Batch [5600/10187], Loss: 5.2867\n",
      "Epoch [1/10], Batch [5700/10187], Loss: 4.5220\n",
      "Epoch [1/10], Batch [5800/10187], Loss: 4.6232\n",
      "Epoch [1/10], Batch [5900/10187], Loss: 5.0844\n",
      "Epoch [1/10], Batch [6000/10187], Loss: 6.0096\n",
      "Epoch [1/10], Batch [6100/10187], Loss: 5.4251\n",
      "Epoch [1/10], Batch [6200/10187], Loss: 5.0236\n",
      "Epoch [1/10], Batch [6300/10187], Loss: 6.0688\n",
      "Epoch [1/10], Batch [6400/10187], Loss: 5.0137\n",
      "Epoch [1/10], Batch [6500/10187], Loss: 5.6607\n",
      "Epoch [1/10], Batch [6600/10187], Loss: 5.6453\n",
      "Epoch [1/10], Batch [6700/10187], Loss: 5.4720\n",
      "Epoch [1/10], Batch [6800/10187], Loss: 5.9119\n",
      "Epoch [1/10], Batch [6900/10187], Loss: 5.4939\n",
      "Epoch [1/10], Batch [7000/10187], Loss: 6.0511\n",
      "Epoch [1/10], Batch [7100/10187], Loss: 4.9934\n",
      "Epoch [1/10], Batch [7200/10187], Loss: 4.9266\n",
      "Epoch [1/10], Batch [7300/10187], Loss: 5.4479\n",
      "Epoch [1/10], Batch [7400/10187], Loss: 5.3999\n",
      "Epoch [1/10], Batch [7500/10187], Loss: 5.8547\n",
      "Epoch [1/10], Batch [7600/10187], Loss: 4.8540\n",
      "Epoch [1/10], Batch [7700/10187], Loss: 4.3793\n",
      "Epoch [1/10], Batch [7800/10187], Loss: 5.0002\n",
      "Epoch [1/10], Batch [7900/10187], Loss: 4.9176\n",
      "Epoch [1/10], Batch [8000/10187], Loss: 5.5525\n",
      "Epoch [1/10], Batch [8100/10187], Loss: 5.1715\n",
      "Epoch [1/10], Batch [8200/10187], Loss: 5.6121\n",
      "Epoch [1/10], Batch [8300/10187], Loss: 4.5144\n",
      "Epoch [1/10], Batch [8400/10187], Loss: 5.1199\n",
      "Epoch [1/10], Batch [8500/10187], Loss: 5.2635\n",
      "Epoch [1/10], Batch [8600/10187], Loss: 4.8523\n",
      "Epoch [1/10], Batch [8700/10187], Loss: 5.0401\n",
      "Epoch [1/10], Batch [8800/10187], Loss: 5.3936\n",
      "Epoch [1/10], Batch [8900/10187], Loss: 4.6445\n",
      "Epoch [1/10], Batch [9000/10187], Loss: 5.3993\n",
      "Epoch [1/10], Batch [9100/10187], Loss: 4.9883\n",
      "Epoch [1/10], Batch [9200/10187], Loss: 5.0811\n",
      "Epoch [1/10], Batch [9300/10187], Loss: 5.2593\n",
      "Epoch [1/10], Batch [9400/10187], Loss: 5.4489\n",
      "Epoch [1/10], Batch [9500/10187], Loss: 4.3979\n",
      "Epoch [1/10], Batch [9600/10187], Loss: 5.6976\n",
      "Epoch [1/10], Batch [9700/10187], Loss: 5.1229\n",
      "Epoch [1/10], Batch [9800/10187], Loss: 4.6549\n",
      "Epoch [1/10], Batch [9900/10187], Loss: 6.1637\n",
      "Epoch [1/10], Batch [10000/10187], Loss: 5.6783\n",
      "Epoch [1/10], Batch [10100/10187], Loss: 4.1765\n",
      "Epoch [1/10] completed, Average Loss: 5.4709\n",
      "Epoch [2/10], Batch [100/10187], Loss: 4.8479\n",
      "Epoch [2/10], Batch [200/10187], Loss: 4.2618\n",
      "Epoch [2/10], Batch [300/10187], Loss: 5.4553\n",
      "Epoch [2/10], Batch [400/10187], Loss: 4.4387\n",
      "Epoch [2/10], Batch [500/10187], Loss: 4.3466\n",
      "Epoch [2/10], Batch [600/10187], Loss: 4.8697\n",
      "Epoch [2/10], Batch [700/10187], Loss: 5.1680\n",
      "Epoch [2/10], Batch [800/10187], Loss: 5.2068\n",
      "Epoch [2/10], Batch [900/10187], Loss: 5.3761\n",
      "Epoch [2/10], Batch [1000/10187], Loss: 5.5962\n",
      "Epoch [2/10], Batch [1100/10187], Loss: 5.0852\n",
      "Epoch [2/10], Batch [1200/10187], Loss: 5.3818\n",
      "Epoch [2/10], Batch [1300/10187], Loss: 5.5173\n",
      "Epoch [2/10], Batch [1400/10187], Loss: 5.8570\n",
      "Epoch [2/10], Batch [1500/10187], Loss: 5.3863\n",
      "Epoch [2/10], Batch [1600/10187], Loss: 5.0327\n",
      "Epoch [2/10], Batch [1700/10187], Loss: 5.0894\n",
      "Epoch [2/10], Batch [1800/10187], Loss: 5.1185\n",
      "Epoch [2/10], Batch [1900/10187], Loss: 5.0639\n",
      "Epoch [2/10], Batch [2000/10187], Loss: 3.9998\n",
      "Epoch [2/10], Batch [2100/10187], Loss: 4.8099\n",
      "Epoch [2/10], Batch [2200/10187], Loss: 5.6869\n",
      "Epoch [2/10], Batch [2300/10187], Loss: 5.3282\n",
      "Epoch [2/10], Batch [2400/10187], Loss: 4.6914\n",
      "Epoch [2/10], Batch [2500/10187], Loss: 5.4394\n",
      "Epoch [2/10], Batch [2600/10187], Loss: 5.5443\n",
      "Epoch [2/10], Batch [2700/10187], Loss: 5.4318\n",
      "Epoch [2/10], Batch [2800/10187], Loss: 6.0016\n",
      "Epoch [2/10], Batch [2900/10187], Loss: 5.3900\n",
      "Epoch [2/10], Batch [3000/10187], Loss: 4.9583\n",
      "Epoch [2/10], Batch [3100/10187], Loss: 4.8472\n",
      "Epoch [2/10], Batch [3200/10187], Loss: 4.9380\n",
      "Epoch [2/10], Batch [3300/10187], Loss: 4.3690\n",
      "Epoch [2/10], Batch [3400/10187], Loss: 5.0511\n",
      "Epoch [2/10], Batch [3500/10187], Loss: 4.5298\n",
      "Epoch [2/10], Batch [3600/10187], Loss: 4.9536\n",
      "Epoch [2/10], Batch [3700/10187], Loss: 4.8901\n",
      "Epoch [2/10], Batch [3800/10187], Loss: 5.1011\n",
      "Epoch [2/10], Batch [3900/10187], Loss: 5.6751\n",
      "Epoch [2/10], Batch [4000/10187], Loss: 5.1800\n",
      "Epoch [2/10], Batch [4100/10187], Loss: 5.2140\n",
      "Epoch [2/10], Batch [4200/10187], Loss: 5.7313\n",
      "Epoch [2/10], Batch [4300/10187], Loss: 5.1589\n",
      "Epoch [2/10], Batch [4400/10187], Loss: 4.2611\n",
      "Epoch [2/10], Batch [4500/10187], Loss: 4.8138\n",
      "Epoch [2/10], Batch [4600/10187], Loss: 4.9186\n",
      "Epoch [2/10], Batch [4700/10187], Loss: 4.9589\n",
      "Epoch [2/10], Batch [4800/10187], Loss: 5.3521\n",
      "Epoch [2/10], Batch [4900/10187], Loss: 5.4031\n",
      "Epoch [2/10], Batch [5000/10187], Loss: 4.0037\n",
      "Epoch [2/10], Batch [5100/10187], Loss: 4.9033\n",
      "Epoch [2/10], Batch [5200/10187], Loss: 5.6127\n",
      "Epoch [2/10], Batch [5300/10187], Loss: 4.0004\n",
      "Epoch [2/10], Batch [5400/10187], Loss: 5.4564\n",
      "Epoch [2/10], Batch [5500/10187], Loss: 5.2547\n",
      "Epoch [2/10], Batch [5600/10187], Loss: 5.9558\n",
      "Epoch [2/10], Batch [5700/10187], Loss: 6.0937\n",
      "Epoch [2/10], Batch [5800/10187], Loss: 4.6093\n",
      "Epoch [2/10], Batch [5900/10187], Loss: 4.6789\n",
      "Epoch [2/10], Batch [6000/10187], Loss: 5.2969\n",
      "Epoch [2/10], Batch [6100/10187], Loss: 5.4829\n",
      "Epoch [2/10], Batch [6200/10187], Loss: 5.0517\n",
      "Epoch [2/10], Batch [6300/10187], Loss: 5.4926\n",
      "Epoch [2/10], Batch [6400/10187], Loss: 5.8106\n",
      "Epoch [2/10], Batch [6500/10187], Loss: 4.8894\n",
      "Epoch [2/10], Batch [6600/10187], Loss: 5.4995\n",
      "Epoch [2/10], Batch [6700/10187], Loss: 5.4155\n",
      "Epoch [2/10], Batch [6800/10187], Loss: 4.7357\n",
      "Epoch [2/10], Batch [6900/10187], Loss: 5.2352\n",
      "Epoch [2/10], Batch [7000/10187], Loss: 5.0896\n",
      "Epoch [2/10], Batch [7100/10187], Loss: 4.4885\n",
      "Epoch [2/10], Batch [7200/10187], Loss: 3.9818\n",
      "Epoch [2/10], Batch [7300/10187], Loss: 4.3616\n",
      "Epoch [2/10], Batch [7400/10187], Loss: 5.1735\n",
      "Epoch [2/10], Batch [7500/10187], Loss: 5.4615\n",
      "Epoch [2/10], Batch [7600/10187], Loss: 4.8632\n",
      "Epoch [2/10], Batch [7700/10187], Loss: 4.5643\n",
      "Epoch [2/10], Batch [7800/10187], Loss: 4.7368\n",
      "Epoch [2/10], Batch [7900/10187], Loss: 5.3127\n",
      "Epoch [2/10], Batch [8000/10187], Loss: 4.6547\n",
      "Epoch [2/10], Batch [8100/10187], Loss: 4.8177\n",
      "Epoch [2/10], Batch [8200/10187], Loss: 5.2302\n",
      "Epoch [2/10], Batch [8300/10187], Loss: 5.4076\n",
      "Epoch [2/10], Batch [8400/10187], Loss: 5.4914\n",
      "Epoch [2/10], Batch [8500/10187], Loss: 4.5453\n",
      "Epoch [2/10], Batch [8600/10187], Loss: 4.7855\n",
      "Epoch [2/10], Batch [8700/10187], Loss: 4.7301\n",
      "Epoch [2/10], Batch [8800/10187], Loss: 4.6826\n",
      "Epoch [2/10], Batch [8900/10187], Loss: 5.1224\n",
      "Epoch [2/10], Batch [9000/10187], Loss: 4.7551\n",
      "Epoch [2/10], Batch [9100/10187], Loss: 4.4403\n",
      "Epoch [2/10], Batch [9200/10187], Loss: 4.8722\n",
      "Epoch [2/10], Batch [9300/10187], Loss: 5.3416\n",
      "Epoch [2/10], Batch [9400/10187], Loss: 5.6196\n",
      "Epoch [2/10], Batch [9500/10187], Loss: 5.4518\n",
      "Epoch [2/10], Batch [9600/10187], Loss: 4.7703\n",
      "Epoch [2/10], Batch [9700/10187], Loss: 4.4199\n",
      "Epoch [2/10], Batch [9800/10187], Loss: 4.4388\n",
      "Epoch [2/10], Batch [9900/10187], Loss: 4.1322\n",
      "Epoch [2/10], Batch [10000/10187], Loss: 4.1220\n",
      "Epoch [2/10], Batch [10100/10187], Loss: 4.7866\n",
      "Epoch [2/10] completed, Average Loss: 4.9161\n",
      "Epoch [3/10], Batch [100/10187], Loss: 4.0584\n",
      "Epoch [3/10], Batch [200/10187], Loss: 4.1128\n",
      "Epoch [3/10], Batch [300/10187], Loss: 4.2607\n",
      "Epoch [3/10], Batch [400/10187], Loss: 4.7456\n",
      "Epoch [3/10], Batch [500/10187], Loss: 5.0208\n",
      "Epoch [3/10], Batch [600/10187], Loss: 4.6637\n",
      "Epoch [3/10], Batch [700/10187], Loss: 4.3015\n",
      "Epoch [3/10], Batch [800/10187], Loss: 4.8186\n",
      "Epoch [3/10], Batch [900/10187], Loss: 4.9979\n",
      "Epoch [3/10], Batch [1000/10187], Loss: 4.2509\n",
      "Epoch [3/10], Batch [1100/10187], Loss: 4.6272\n",
      "Epoch [3/10], Batch [1200/10187], Loss: 4.4464\n",
      "Epoch [3/10], Batch [1300/10187], Loss: 4.5682\n",
      "Epoch [3/10], Batch [1400/10187], Loss: 3.8968\n",
      "Epoch [3/10], Batch [1500/10187], Loss: 4.5055\n",
      "Epoch [3/10], Batch [1600/10187], Loss: 4.9774\n",
      "Epoch [3/10], Batch [1700/10187], Loss: 3.8592\n",
      "Epoch [3/10], Batch [1800/10187], Loss: 4.8283\n",
      "Epoch [3/10], Batch [1900/10187], Loss: 5.8734\n",
      "Epoch [3/10], Batch [2000/10187], Loss: 4.9987\n",
      "Epoch [3/10], Batch [2100/10187], Loss: 4.2912\n",
      "Epoch [3/10], Batch [2200/10187], Loss: 4.3296\n",
      "Epoch [3/10], Batch [2300/10187], Loss: 4.5196\n",
      "Epoch [3/10], Batch [2400/10187], Loss: 4.3494\n",
      "Epoch [3/10], Batch [2500/10187], Loss: 5.9123\n",
      "Epoch [3/10], Batch [2600/10187], Loss: 4.8683\n",
      "Epoch [3/10], Batch [2700/10187], Loss: 3.4658\n",
      "Epoch [3/10], Batch [2800/10187], Loss: 4.4352\n",
      "Epoch [3/10], Batch [2900/10187], Loss: 4.3396\n",
      "Epoch [3/10], Batch [3000/10187], Loss: 4.2799\n",
      "Epoch [3/10], Batch [3100/10187], Loss: 4.1211\n",
      "Epoch [3/10], Batch [3200/10187], Loss: 4.2538\n",
      "Epoch [3/10], Batch [3300/10187], Loss: 4.8142\n",
      "Epoch [3/10], Batch [3400/10187], Loss: 3.4575\n",
      "Epoch [3/10], Batch [3500/10187], Loss: 4.4744\n",
      "Epoch [3/10], Batch [3600/10187], Loss: 4.9139\n",
      "Epoch [3/10], Batch [3700/10187], Loss: 3.9923\n",
      "Epoch [3/10], Batch [3800/10187], Loss: 4.6870\n",
      "Epoch [3/10], Batch [3900/10187], Loss: 4.5189\n",
      "Epoch [3/10], Batch [4000/10187], Loss: 4.5966\n",
      "Epoch [3/10], Batch [4100/10187], Loss: 4.6548\n",
      "Epoch [3/10], Batch [4200/10187], Loss: 4.6306\n",
      "Epoch [3/10], Batch [4300/10187], Loss: 4.7086\n",
      "Epoch [3/10], Batch [4400/10187], Loss: 4.7808\n",
      "Epoch [3/10], Batch [4500/10187], Loss: 4.5008\n",
      "Epoch [3/10], Batch [4600/10187], Loss: 4.9325\n",
      "Epoch [3/10], Batch [4700/10187], Loss: 4.0638\n",
      "Epoch [3/10], Batch [4800/10187], Loss: 4.1950\n",
      "Epoch [3/10], Batch [4900/10187], Loss: 4.6221\n",
      "Epoch [3/10], Batch [5000/10187], Loss: 5.0954\n",
      "Epoch [3/10], Batch [5100/10187], Loss: 4.5750\n",
      "Epoch [3/10], Batch [5200/10187], Loss: 5.0238\n",
      "Epoch [3/10], Batch [5300/10187], Loss: 4.7621\n",
      "Epoch [3/10], Batch [5400/10187], Loss: 4.3559\n",
      "Epoch [3/10], Batch [5500/10187], Loss: 4.4461\n",
      "Epoch [3/10], Batch [5600/10187], Loss: 4.1887\n",
      "Epoch [3/10], Batch [5700/10187], Loss: 4.8083\n",
      "Epoch [3/10], Batch [5800/10187], Loss: 4.9795\n",
      "Epoch [3/10], Batch [5900/10187], Loss: 4.5456\n",
      "Epoch [3/10], Batch [6000/10187], Loss: 4.3203\n",
      "Epoch [3/10], Batch [6100/10187], Loss: 4.6558\n",
      "Epoch [3/10], Batch [6200/10187], Loss: 3.7818\n",
      "Epoch [3/10], Batch [6300/10187], Loss: 4.0950\n",
      "Epoch [3/10], Batch [6400/10187], Loss: 4.3723\n",
      "Epoch [3/10], Batch [6500/10187], Loss: 4.5187\n",
      "Epoch [3/10], Batch [6600/10187], Loss: 4.0437\n",
      "Epoch [3/10], Batch [6700/10187], Loss: 5.3945\n",
      "Epoch [3/10], Batch [6800/10187], Loss: 3.4093\n",
      "Epoch [3/10], Batch [6900/10187], Loss: 5.2784\n",
      "Epoch [3/10], Batch [7000/10187], Loss: 4.1351\n",
      "Epoch [3/10], Batch [7100/10187], Loss: 4.3278\n",
      "Epoch [3/10], Batch [7200/10187], Loss: 4.3029\n",
      "Epoch [3/10], Batch [7300/10187], Loss: 4.2317\n",
      "Epoch [3/10], Batch [7400/10187], Loss: 3.8624\n",
      "Epoch [3/10], Batch [7500/10187], Loss: 4.7300\n",
      "Epoch [3/10], Batch [7600/10187], Loss: 4.0897\n",
      "Epoch [3/10], Batch [7700/10187], Loss: 4.3590\n",
      "Epoch [3/10], Batch [7800/10187], Loss: 5.2425\n",
      "Epoch [3/10], Batch [7900/10187], Loss: 4.8242\n",
      "Epoch [3/10], Batch [8000/10187], Loss: 4.2009\n",
      "Epoch [3/10], Batch [8100/10187], Loss: 4.8210\n",
      "Epoch [3/10], Batch [8200/10187], Loss: 4.2647\n",
      "Epoch [3/10], Batch [8300/10187], Loss: 5.0724\n",
      "Epoch [3/10], Batch [8400/10187], Loss: 4.4436\n",
      "Epoch [3/10], Batch [8500/10187], Loss: 5.4340\n",
      "Epoch [3/10], Batch [8600/10187], Loss: 4.3588\n",
      "Epoch [3/10], Batch [8700/10187], Loss: 3.9040\n",
      "Epoch [3/10], Batch [8800/10187], Loss: 4.8495\n",
      "Epoch [3/10], Batch [8900/10187], Loss: 4.9338\n",
      "Epoch [3/10], Batch [9000/10187], Loss: 3.3573\n",
      "Epoch [3/10], Batch [9100/10187], Loss: 4.1138\n",
      "Epoch [3/10], Batch [9200/10187], Loss: 4.0325\n",
      "Epoch [3/10], Batch [9300/10187], Loss: 3.9169\n",
      "Epoch [3/10], Batch [9400/10187], Loss: 4.5886\n",
      "Epoch [3/10], Batch [9500/10187], Loss: 4.3917\n",
      "Epoch [3/10], Batch [9600/10187], Loss: 5.4256\n",
      "Epoch [3/10], Batch [9700/10187], Loss: 4.9609\n",
      "Epoch [3/10], Batch [9800/10187], Loss: 4.7917\n",
      "Epoch [3/10], Batch [9900/10187], Loss: 4.3421\n",
      "Epoch [3/10], Batch [10000/10187], Loss: 4.8157\n",
      "Epoch [3/10], Batch [10100/10187], Loss: 5.3806\n",
      "Epoch [3/10] completed, Average Loss: 4.6102\n",
      "Epoch [4/10], Batch [100/10187], Loss: 4.8899\n",
      "Epoch [4/10], Batch [200/10187], Loss: 4.2932\n",
      "Epoch [4/10], Batch [300/10187], Loss: 3.9095\n",
      "Epoch [4/10], Batch [400/10187], Loss: 4.5708\n",
      "Epoch [4/10], Batch [500/10187], Loss: 4.3718\n",
      "Epoch [4/10], Batch [600/10187], Loss: 4.2996\n",
      "Epoch [4/10], Batch [700/10187], Loss: 3.7068\n",
      "Epoch [4/10], Batch [800/10187], Loss: 4.1127\n",
      "Epoch [4/10], Batch [900/10187], Loss: 3.5124\n",
      "Epoch [4/10], Batch [1000/10187], Loss: 4.5323\n",
      "Epoch [4/10], Batch [1100/10187], Loss: 3.7535\n",
      "Epoch [4/10], Batch [1200/10187], Loss: 4.1873\n",
      "Epoch [4/10], Batch [1300/10187], Loss: 3.8497\n",
      "Epoch [4/10], Batch [1400/10187], Loss: 3.4207\n",
      "Epoch [4/10], Batch [1500/10187], Loss: 4.8622\n",
      "Epoch [4/10], Batch [1600/10187], Loss: 4.4268\n",
      "Epoch [4/10], Batch [1700/10187], Loss: 4.4654\n",
      "Epoch [4/10], Batch [1800/10187], Loss: 3.6666\n",
      "Epoch [4/10], Batch [1900/10187], Loss: 3.9840\n",
      "Epoch [4/10], Batch [2000/10187], Loss: 4.3599\n",
      "Epoch [4/10], Batch [2100/10187], Loss: 4.2456\n",
      "Epoch [4/10], Batch [2200/10187], Loss: 3.8034\n",
      "Epoch [4/10], Batch [2300/10187], Loss: 3.9990\n",
      "Epoch [4/10], Batch [2400/10187], Loss: 3.4798\n",
      "Epoch [4/10], Batch [2500/10187], Loss: 4.1779\n",
      "Epoch [4/10], Batch [2600/10187], Loss: 4.1317\n",
      "Epoch [4/10], Batch [2700/10187], Loss: 5.0708\n",
      "Epoch [4/10], Batch [2800/10187], Loss: 3.7058\n",
      "Epoch [4/10], Batch [2900/10187], Loss: 5.8414\n",
      "Epoch [4/10], Batch [3000/10187], Loss: 4.5519\n",
      "Epoch [4/10], Batch [3100/10187], Loss: 4.7678\n",
      "Epoch [4/10], Batch [3200/10187], Loss: 4.3880\n",
      "Epoch [4/10], Batch [3300/10187], Loss: 3.8624\n",
      "Epoch [4/10], Batch [3400/10187], Loss: 3.9525\n",
      "Epoch [4/10], Batch [3500/10187], Loss: 4.6319\n",
      "Epoch [4/10], Batch [3600/10187], Loss: 4.6589\n",
      "Epoch [4/10], Batch [3700/10187], Loss: 4.1280\n",
      "Epoch [4/10], Batch [3800/10187], Loss: 3.8392\n",
      "Epoch [4/10], Batch [3900/10187], Loss: 4.7601\n",
      "Epoch [4/10], Batch [4000/10187], Loss: 4.9648\n",
      "Epoch [4/10], Batch [4100/10187], Loss: 4.2052\n",
      "Epoch [4/10], Batch [4200/10187], Loss: 4.7299\n",
      "Epoch [4/10], Batch [4300/10187], Loss: 5.3664\n",
      "Epoch [4/10], Batch [4400/10187], Loss: 4.8390\n",
      "Epoch [4/10], Batch [4500/10187], Loss: 3.8370\n",
      "Epoch [4/10], Batch [4600/10187], Loss: 4.7936\n",
      "Epoch [4/10], Batch [4700/10187], Loss: 4.4984\n",
      "Epoch [4/10], Batch [4800/10187], Loss: 3.9918\n",
      "Epoch [4/10], Batch [4900/10187], Loss: 4.3671\n",
      "Epoch [4/10], Batch [5000/10187], Loss: 4.2821\n",
      "Epoch [4/10], Batch [5100/10187], Loss: 4.2672\n",
      "Epoch [4/10], Batch [5200/10187], Loss: 4.4462\n",
      "Epoch [4/10], Batch [5300/10187], Loss: 4.7804\n",
      "Epoch [4/10], Batch [5400/10187], Loss: 4.9895\n",
      "Epoch [4/10], Batch [5500/10187], Loss: 4.4242\n",
      "Epoch [4/10], Batch [5600/10187], Loss: 4.5311\n",
      "Epoch [4/10], Batch [5700/10187], Loss: 4.5071\n",
      "Epoch [4/10], Batch [5800/10187], Loss: 3.8113\n",
      "Epoch [4/10], Batch [5900/10187], Loss: 4.4507\n",
      "Epoch [4/10], Batch [6000/10187], Loss: 4.5372\n",
      "Epoch [4/10], Batch [6100/10187], Loss: 4.3129\n",
      "Epoch [4/10], Batch [6200/10187], Loss: 4.3295\n",
      "Epoch [4/10], Batch [6300/10187], Loss: 3.9496\n",
      "Epoch [4/10], Batch [6400/10187], Loss: 4.7265\n",
      "Epoch [4/10], Batch [6500/10187], Loss: 4.7810\n",
      "Epoch [4/10], Batch [6600/10187], Loss: 4.3788\n",
      "Epoch [4/10], Batch [6700/10187], Loss: 4.7227\n",
      "Epoch [4/10], Batch [6800/10187], Loss: 3.7067\n",
      "Epoch [4/10], Batch [6900/10187], Loss: 4.9371\n",
      "Epoch [4/10], Batch [7000/10187], Loss: 4.6423\n",
      "Epoch [4/10], Batch [7100/10187], Loss: 4.9808\n",
      "Epoch [4/10], Batch [7200/10187], Loss: 4.6445\n",
      "Epoch [4/10], Batch [7300/10187], Loss: 4.4593\n",
      "Epoch [4/10], Batch [7400/10187], Loss: 3.8644\n",
      "Epoch [4/10], Batch [7500/10187], Loss: 4.1644\n",
      "Epoch [4/10], Batch [7600/10187], Loss: 4.3471\n",
      "Epoch [4/10], Batch [7700/10187], Loss: 4.8770\n",
      "Epoch [4/10], Batch [7800/10187], Loss: 4.7044\n",
      "Epoch [4/10], Batch [7900/10187], Loss: 4.1568\n",
      "Epoch [4/10], Batch [8000/10187], Loss: 4.1653\n",
      "Epoch [4/10], Batch [8100/10187], Loss: 3.9681\n",
      "Epoch [4/10], Batch [8200/10187], Loss: 3.7928\n",
      "Epoch [4/10], Batch [8300/10187], Loss: 4.5335\n",
      "Epoch [4/10], Batch [8400/10187], Loss: 4.4859\n",
      "Epoch [4/10], Batch [8500/10187], Loss: 4.0495\n",
      "Epoch [4/10], Batch [8600/10187], Loss: 4.1811\n",
      "Epoch [4/10], Batch [8700/10187], Loss: 3.9737\n",
      "Epoch [4/10], Batch [8800/10187], Loss: 4.7041\n",
      "Epoch [4/10], Batch [8900/10187], Loss: 4.2317\n",
      "Epoch [4/10], Batch [9000/10187], Loss: 3.3871\n",
      "Epoch [4/10], Batch [9100/10187], Loss: 5.2539\n",
      "Epoch [4/10], Batch [9200/10187], Loss: 4.5951\n",
      "Epoch [4/10], Batch [9300/10187], Loss: 4.8999\n",
      "Epoch [4/10], Batch [9400/10187], Loss: 4.4204\n",
      "Epoch [4/10], Batch [9500/10187], Loss: 4.5777\n",
      "Epoch [4/10], Batch [9600/10187], Loss: 3.7801\n",
      "Epoch [4/10], Batch [9700/10187], Loss: 4.5623\n",
      "Epoch [4/10], Batch [9800/10187], Loss: 4.4229\n",
      "Epoch [4/10], Batch [9900/10187], Loss: 4.9672\n",
      "Epoch [4/10], Batch [10000/10187], Loss: 3.9205\n",
      "Epoch [4/10], Batch [10100/10187], Loss: 4.0389\n",
      "Epoch [4/10] completed, Average Loss: 4.3276\n",
      "Epoch [5/10], Batch [100/10187], Loss: 3.4924\n",
      "Epoch [5/10], Batch [200/10187], Loss: 3.5779\n",
      "Epoch [5/10], Batch [300/10187], Loss: 4.1532\n",
      "Epoch [5/10], Batch [400/10187], Loss: 3.7716\n",
      "Epoch [5/10], Batch [500/10187], Loss: 4.0632\n",
      "Epoch [5/10], Batch [600/10187], Loss: 3.4802\n",
      "Epoch [5/10], Batch [700/10187], Loss: 3.4238\n",
      "Epoch [5/10], Batch [800/10187], Loss: 3.7932\n",
      "Epoch [5/10], Batch [900/10187], Loss: 4.0172\n",
      "Epoch [5/10], Batch [1000/10187], Loss: 3.6790\n",
      "Epoch [5/10], Batch [1100/10187], Loss: 4.1463\n",
      "Epoch [5/10], Batch [1200/10187], Loss: 3.5615\n",
      "Epoch [5/10], Batch [1300/10187], Loss: 3.9864\n",
      "Epoch [5/10], Batch [1400/10187], Loss: 4.6676\n",
      "Epoch [5/10], Batch [1500/10187], Loss: 3.5689\n",
      "Epoch [5/10], Batch [1600/10187], Loss: 4.1324\n",
      "Epoch [5/10], Batch [1700/10187], Loss: 4.4671\n",
      "Epoch [5/10], Batch [1800/10187], Loss: 3.2646\n",
      "Epoch [5/10], Batch [1900/10187], Loss: 4.4173\n",
      "Epoch [5/10], Batch [2000/10187], Loss: 4.1847\n",
      "Epoch [5/10], Batch [2100/10187], Loss: 3.7784\n",
      "Epoch [5/10], Batch [2200/10187], Loss: 3.8717\n",
      "Epoch [5/10], Batch [2300/10187], Loss: 3.7676\n",
      "Epoch [5/10], Batch [2400/10187], Loss: 3.9404\n",
      "Epoch [5/10], Batch [2500/10187], Loss: 3.6332\n",
      "Epoch [5/10], Batch [2600/10187], Loss: 4.0862\n",
      "Epoch [5/10], Batch [2700/10187], Loss: 3.8677\n",
      "Epoch [5/10], Batch [2800/10187], Loss: 4.2534\n",
      "Epoch [5/10], Batch [2900/10187], Loss: 4.3835\n",
      "Epoch [5/10], Batch [3000/10187], Loss: 4.0248\n",
      "Epoch [5/10], Batch [3100/10187], Loss: 4.3062\n",
      "Epoch [5/10], Batch [3200/10187], Loss: 3.5032\n",
      "Epoch [5/10], Batch [3300/10187], Loss: 3.7538\n",
      "Epoch [5/10], Batch [3400/10187], Loss: 3.4026\n",
      "Epoch [5/10], Batch [3500/10187], Loss: 4.2827\n",
      "Epoch [5/10], Batch [3600/10187], Loss: 3.2946\n",
      "Epoch [5/10], Batch [3700/10187], Loss: 4.3644\n",
      "Epoch [5/10], Batch [3800/10187], Loss: 4.3822\n",
      "Epoch [5/10], Batch [3900/10187], Loss: 3.7454\n",
      "Epoch [5/10], Batch [4000/10187], Loss: 4.5341\n",
      "Epoch [5/10], Batch [4100/10187], Loss: 4.3092\n",
      "Epoch [5/10], Batch [4200/10187], Loss: 4.0832\n",
      "Epoch [5/10], Batch [4300/10187], Loss: 3.9739\n",
      "Epoch [5/10], Batch [4400/10187], Loss: 4.5835\n",
      "Epoch [5/10], Batch [4500/10187], Loss: 2.6770\n",
      "Epoch [5/10], Batch [4600/10187], Loss: 3.5033\n",
      "Epoch [5/10], Batch [4700/10187], Loss: 3.6280\n",
      "Epoch [5/10], Batch [4800/10187], Loss: 4.1636\n",
      "Epoch [5/10], Batch [4900/10187], Loss: 4.1452\n",
      "Epoch [5/10], Batch [5000/10187], Loss: 2.9929\n",
      "Epoch [5/10], Batch [5100/10187], Loss: 3.6579\n",
      "Epoch [5/10], Batch [5200/10187], Loss: 3.7631\n",
      "Epoch [5/10], Batch [5300/10187], Loss: 3.9231\n",
      "Epoch [5/10], Batch [5400/10187], Loss: 3.2451\n",
      "Epoch [5/10], Batch [5500/10187], Loss: 4.0368\n",
      "Epoch [5/10], Batch [5600/10187], Loss: 4.1699\n",
      "Epoch [5/10], Batch [5700/10187], Loss: 3.2495\n",
      "Epoch [5/10], Batch [5800/10187], Loss: 3.6557\n",
      "Epoch [5/10], Batch [5900/10187], Loss: 3.8107\n",
      "Epoch [5/10], Batch [6000/10187], Loss: 3.4494\n",
      "Epoch [5/10], Batch [6100/10187], Loss: 4.3081\n",
      "Epoch [5/10], Batch [6200/10187], Loss: 3.7643\n",
      "Epoch [5/10], Batch [6300/10187], Loss: 4.2951\n",
      "Epoch [5/10], Batch [6400/10187], Loss: 4.4546\n",
      "Epoch [5/10], Batch [6500/10187], Loss: 4.6957\n",
      "Epoch [5/10], Batch [6600/10187], Loss: 4.4877\n",
      "Epoch [5/10], Batch [6700/10187], Loss: 4.9467\n",
      "Epoch [5/10], Batch [6800/10187], Loss: 4.2861\n",
      "Epoch [5/10], Batch [6900/10187], Loss: 3.5948\n",
      "Epoch [5/10], Batch [7000/10187], Loss: 3.9894\n",
      "Epoch [5/10], Batch [7100/10187], Loss: 3.9199\n",
      "Epoch [5/10], Batch [7200/10187], Loss: 3.9924\n",
      "Epoch [5/10], Batch [7300/10187], Loss: 3.8337\n",
      "Epoch [5/10], Batch [7400/10187], Loss: 4.2939\n",
      "Epoch [5/10], Batch [7500/10187], Loss: 4.1906\n",
      "Epoch [5/10], Batch [7600/10187], Loss: 3.4855\n",
      "Epoch [5/10], Batch [7700/10187], Loss: 4.9217\n",
      "Epoch [5/10], Batch [7800/10187], Loss: 4.2311\n",
      "Epoch [5/10], Batch [7900/10187], Loss: 4.5722\n",
      "Epoch [5/10], Batch [8000/10187], Loss: 3.1587\n",
      "Epoch [5/10], Batch [8100/10187], Loss: 4.1081\n",
      "Epoch [5/10], Batch [8200/10187], Loss: 4.0298\n",
      "Epoch [5/10], Batch [8300/10187], Loss: 3.9720\n",
      "Epoch [5/10], Batch [8400/10187], Loss: 4.8263\n",
      "Epoch [5/10], Batch [8500/10187], Loss: 3.7629\n",
      "Epoch [5/10], Batch [8600/10187], Loss: 3.5746\n",
      "Epoch [5/10], Batch [8700/10187], Loss: 4.9600\n",
      "Epoch [5/10], Batch [8800/10187], Loss: 3.9893\n",
      "Epoch [5/10], Batch [8900/10187], Loss: 4.4339\n",
      "Epoch [5/10], Batch [9000/10187], Loss: 4.1565\n",
      "Epoch [5/10], Batch [9100/10187], Loss: 4.3706\n",
      "Epoch [5/10], Batch [9200/10187], Loss: 4.1300\n",
      "Epoch [5/10], Batch [9300/10187], Loss: 4.5039\n",
      "Epoch [5/10], Batch [9400/10187], Loss: 3.1028\n",
      "Epoch [5/10], Batch [9500/10187], Loss: 3.9013\n",
      "Epoch [5/10], Batch [9600/10187], Loss: 4.2385\n",
      "Epoch [5/10], Batch [9700/10187], Loss: 4.0156\n",
      "Epoch [5/10], Batch [9800/10187], Loss: 3.5010\n",
      "Epoch [5/10], Batch [9900/10187], Loss: 4.7128\n",
      "Epoch [5/10], Batch [10000/10187], Loss: 4.9635\n",
      "Epoch [5/10], Batch [10100/10187], Loss: 5.2494\n",
      "Epoch [5/10] completed, Average Loss: 4.0637\n",
      "Epoch [6/10], Batch [100/10187], Loss: 3.0951\n",
      "Epoch [6/10], Batch [200/10187], Loss: 3.7212\n",
      "Epoch [6/10], Batch [300/10187], Loss: 3.5493\n",
      "Epoch [6/10], Batch [400/10187], Loss: 3.2549\n",
      "Epoch [6/10], Batch [500/10187], Loss: 2.7863\n",
      "Epoch [6/10], Batch [600/10187], Loss: 3.7977\n",
      "Epoch [6/10], Batch [700/10187], Loss: 4.5321\n",
      "Epoch [6/10], Batch [800/10187], Loss: 3.2757\n",
      "Epoch [6/10], Batch [900/10187], Loss: 3.5945\n",
      "Epoch [6/10], Batch [1000/10187], Loss: 3.8177\n",
      "Epoch [6/10], Batch [1100/10187], Loss: 3.8785\n",
      "Epoch [6/10], Batch [1200/10187], Loss: 4.1301\n",
      "Epoch [6/10], Batch [1300/10187], Loss: 2.8124\n",
      "Epoch [6/10], Batch [1400/10187], Loss: 4.2879\n",
      "Epoch [6/10], Batch [1500/10187], Loss: 2.9631\n",
      "Epoch [6/10], Batch [1600/10187], Loss: 3.8233\n",
      "Epoch [6/10], Batch [1700/10187], Loss: 4.0872\n",
      "Epoch [6/10], Batch [1800/10187], Loss: 3.5802\n",
      "Epoch [6/10], Batch [1900/10187], Loss: 3.7599\n",
      "Epoch [6/10], Batch [2000/10187], Loss: 3.5552\n",
      "Epoch [6/10], Batch [2100/10187], Loss: 3.4588\n",
      "Epoch [6/10], Batch [2200/10187], Loss: 4.0284\n",
      "Epoch [6/10], Batch [2300/10187], Loss: 4.1872\n",
      "Epoch [6/10], Batch [2400/10187], Loss: 3.6347\n",
      "Epoch [6/10], Batch [2500/10187], Loss: 3.9752\n",
      "Epoch [6/10], Batch [2600/10187], Loss: 3.6741\n",
      "Epoch [6/10], Batch [2700/10187], Loss: 4.0778\n",
      "Epoch [6/10], Batch [2800/10187], Loss: 3.9482\n",
      "Epoch [6/10], Batch [2900/10187], Loss: 4.4382\n",
      "Epoch [6/10], Batch [3000/10187], Loss: 3.6815\n",
      "Epoch [6/10], Batch [3100/10187], Loss: 3.8276\n",
      "Epoch [6/10], Batch [3200/10187], Loss: 3.3593\n",
      "Epoch [6/10], Batch [3300/10187], Loss: 3.5379\n",
      "Epoch [6/10], Batch [3400/10187], Loss: 3.6522\n",
      "Epoch [6/10], Batch [3500/10187], Loss: 4.1080\n",
      "Epoch [6/10], Batch [3600/10187], Loss: 3.3179\n",
      "Epoch [6/10], Batch [3700/10187], Loss: 3.9333\n",
      "Epoch [6/10], Batch [3800/10187], Loss: 3.8077\n",
      "Epoch [6/10], Batch [3900/10187], Loss: 4.0432\n",
      "Epoch [6/10], Batch [4000/10187], Loss: 4.1898\n",
      "Epoch [6/10], Batch [4100/10187], Loss: 4.3831\n",
      "Epoch [6/10], Batch [4200/10187], Loss: 3.5933\n",
      "Epoch [6/10], Batch [4300/10187], Loss: 3.3797\n",
      "Epoch [6/10], Batch [4400/10187], Loss: 4.0238\n",
      "Epoch [6/10], Batch [4500/10187], Loss: 3.7980\n",
      "Epoch [6/10], Batch [4600/10187], Loss: 3.5549\n",
      "Epoch [6/10], Batch [4700/10187], Loss: 4.1412\n",
      "Epoch [6/10], Batch [4800/10187], Loss: 3.2918\n",
      "Epoch [6/10], Batch [4900/10187], Loss: 4.0195\n",
      "Epoch [6/10], Batch [5000/10187], Loss: 3.5619\n",
      "Epoch [6/10], Batch [5100/10187], Loss: 4.2180\n",
      "Epoch [6/10], Batch [5200/10187], Loss: 3.8908\n",
      "Epoch [6/10], Batch [5300/10187], Loss: 3.9050\n",
      "Epoch [6/10], Batch [5400/10187], Loss: 3.4626\n",
      "Epoch [6/10], Batch [5500/10187], Loss: 4.1384\n",
      "Epoch [6/10], Batch [5600/10187], Loss: 3.7426\n",
      "Epoch [6/10], Batch [5700/10187], Loss: 3.2317\n",
      "Epoch [6/10], Batch [5800/10187], Loss: 4.3220\n",
      "Epoch [6/10], Batch [5900/10187], Loss: 3.6529\n",
      "Epoch [6/10], Batch [6000/10187], Loss: 3.7524\n",
      "Epoch [6/10], Batch [6100/10187], Loss: 4.1096\n",
      "Epoch [6/10], Batch [6200/10187], Loss: 3.7830\n",
      "Epoch [6/10], Batch [6300/10187], Loss: 3.9639\n",
      "Epoch [6/10], Batch [6400/10187], Loss: 4.0322\n",
      "Epoch [6/10], Batch [6500/10187], Loss: 3.9656\n",
      "Epoch [6/10], Batch [6600/10187], Loss: 4.0230\n",
      "Epoch [6/10], Batch [6700/10187], Loss: 3.4716\n",
      "Epoch [6/10], Batch [6800/10187], Loss: 3.9382\n",
      "Epoch [6/10], Batch [6900/10187], Loss: 3.9512\n",
      "Epoch [6/10], Batch [7000/10187], Loss: 4.3107\n",
      "Epoch [6/10], Batch [7100/10187], Loss: 3.6575\n",
      "Epoch [6/10], Batch [7200/10187], Loss: 4.8531\n",
      "Epoch [6/10], Batch [7300/10187], Loss: 3.6457\n",
      "Epoch [6/10], Batch [7400/10187], Loss: 4.5194\n",
      "Epoch [6/10], Batch [7500/10187], Loss: 3.6958\n",
      "Epoch [6/10], Batch [7600/10187], Loss: 3.6681\n",
      "Epoch [6/10], Batch [7700/10187], Loss: 3.7431\n",
      "Epoch [6/10], Batch [7800/10187], Loss: 3.8831\n",
      "Epoch [6/10], Batch [7900/10187], Loss: 4.7192\n",
      "Epoch [6/10], Batch [8000/10187], Loss: 3.7849\n",
      "Epoch [6/10], Batch [8100/10187], Loss: 3.5583\n",
      "Epoch [6/10], Batch [8200/10187], Loss: 3.5498\n",
      "Epoch [6/10], Batch [8300/10187], Loss: 4.2428\n",
      "Epoch [6/10], Batch [8400/10187], Loss: 3.6071\n",
      "Epoch [6/10], Batch [8500/10187], Loss: 3.8806\n",
      "Epoch [6/10], Batch [8600/10187], Loss: 4.5453\n",
      "Epoch [6/10], Batch [8700/10187], Loss: 4.1298\n",
      "Epoch [6/10], Batch [8800/10187], Loss: 3.5112\n",
      "Epoch [6/10], Batch [8900/10187], Loss: 4.5374\n",
      "Epoch [6/10], Batch [9000/10187], Loss: 4.8094\n",
      "Epoch [6/10], Batch [9100/10187], Loss: 3.5274\n",
      "Epoch [6/10], Batch [9200/10187], Loss: 3.6712\n",
      "Epoch [6/10], Batch [9300/10187], Loss: 4.1616\n",
      "Epoch [6/10], Batch [9400/10187], Loss: 3.1767\n",
      "Epoch [6/10], Batch [9500/10187], Loss: 3.9243\n",
      "Epoch [6/10], Batch [9600/10187], Loss: 3.6668\n",
      "Epoch [6/10], Batch [9700/10187], Loss: 4.2835\n",
      "Epoch [6/10], Batch [9800/10187], Loss: 3.7913\n",
      "Epoch [6/10], Batch [9900/10187], Loss: 4.3606\n",
      "Epoch [6/10], Batch [10000/10187], Loss: 4.1471\n",
      "Epoch [6/10], Batch [10100/10187], Loss: 4.0446\n",
      "Epoch [6/10] completed, Average Loss: 3.8202\n",
      "Epoch [7/10], Batch [100/10187], Loss: 2.9845\n",
      "Epoch [7/10], Batch [200/10187], Loss: 3.6020\n",
      "Epoch [7/10], Batch [300/10187], Loss: 3.9135\n",
      "Epoch [7/10], Batch [400/10187], Loss: 2.8790\n",
      "Epoch [7/10], Batch [500/10187], Loss: 3.2867\n",
      "Epoch [7/10], Batch [600/10187], Loss: 2.9479\n",
      "Epoch [7/10], Batch [700/10187], Loss: 3.2789\n",
      "Epoch [7/10], Batch [800/10187], Loss: 3.3558\n",
      "Epoch [7/10], Batch [900/10187], Loss: 3.0674\n",
      "Epoch [7/10], Batch [1000/10187], Loss: 3.4658\n",
      "Epoch [7/10], Batch [1100/10187], Loss: 2.8477\n",
      "Epoch [7/10], Batch [1200/10187], Loss: 3.3387\n",
      "Epoch [7/10], Batch [1300/10187], Loss: 3.1663\n",
      "Epoch [7/10], Batch [1400/10187], Loss: 3.7031\n",
      "Epoch [7/10], Batch [1500/10187], Loss: 3.8247\n",
      "Epoch [7/10], Batch [1600/10187], Loss: 4.1364\n",
      "Epoch [7/10], Batch [1700/10187], Loss: 3.0904\n",
      "Epoch [7/10], Batch [1800/10187], Loss: 3.3043\n",
      "Epoch [7/10], Batch [1900/10187], Loss: 4.0024\n",
      "Epoch [7/10], Batch [2000/10187], Loss: 3.6713\n",
      "Epoch [7/10], Batch [2100/10187], Loss: 2.8485\n",
      "Epoch [7/10], Batch [2200/10187], Loss: 3.4440\n",
      "Epoch [7/10], Batch [2300/10187], Loss: 2.5986\n",
      "Epoch [7/10], Batch [2400/10187], Loss: 3.0754\n",
      "Epoch [7/10], Batch [2500/10187], Loss: 3.5608\n",
      "Epoch [7/10], Batch [2600/10187], Loss: 3.4370\n",
      "Epoch [7/10], Batch [2700/10187], Loss: 3.2209\n",
      "Epoch [7/10], Batch [2800/10187], Loss: 3.1204\n",
      "Epoch [7/10], Batch [2900/10187], Loss: 3.4230\n",
      "Epoch [7/10], Batch [3000/10187], Loss: 3.3736\n",
      "Epoch [7/10], Batch [3100/10187], Loss: 3.3838\n",
      "Epoch [7/10], Batch [3200/10187], Loss: 4.0831\n",
      "Epoch [7/10], Batch [3300/10187], Loss: 3.1590\n",
      "Epoch [7/10], Batch [3400/10187], Loss: 3.2129\n",
      "Epoch [7/10], Batch [3500/10187], Loss: 3.8628\n",
      "Epoch [7/10], Batch [3600/10187], Loss: 4.0586\n",
      "Epoch [7/10], Batch [3700/10187], Loss: 4.4182\n",
      "Epoch [7/10], Batch [3800/10187], Loss: 3.7992\n",
      "Epoch [7/10], Batch [3900/10187], Loss: 3.2494\n",
      "Epoch [7/10], Batch [4000/10187], Loss: 2.9631\n",
      "Epoch [7/10], Batch [4100/10187], Loss: 3.8041\n",
      "Epoch [7/10], Batch [4200/10187], Loss: 3.9434\n",
      "Epoch [7/10], Batch [4300/10187], Loss: 3.7284\n",
      "Epoch [7/10], Batch [4400/10187], Loss: 3.3947\n",
      "Epoch [7/10], Batch [4500/10187], Loss: 3.0134\n",
      "Epoch [7/10], Batch [4600/10187], Loss: 3.8885\n",
      "Epoch [7/10], Batch [4700/10187], Loss: 4.1455\n",
      "Epoch [7/10], Batch [4800/10187], Loss: 3.9468\n",
      "Epoch [7/10], Batch [4900/10187], Loss: 4.1621\n",
      "Epoch [7/10], Batch [5000/10187], Loss: 3.2670\n",
      "Epoch [7/10], Batch [5100/10187], Loss: 3.7032\n",
      "Epoch [7/10], Batch [5200/10187], Loss: 3.7022\n",
      "Epoch [7/10], Batch [5300/10187], Loss: 4.2119\n",
      "Epoch [7/10], Batch [5400/10187], Loss: 3.6737\n",
      "Epoch [7/10], Batch [5500/10187], Loss: 3.7994\n",
      "Epoch [7/10], Batch [5600/10187], Loss: 3.9575\n",
      "Epoch [7/10], Batch [5700/10187], Loss: 3.9713\n",
      "Epoch [7/10], Batch [5800/10187], Loss: 3.3331\n",
      "Epoch [7/10], Batch [5900/10187], Loss: 3.5138\n",
      "Epoch [7/10], Batch [6000/10187], Loss: 3.7618\n",
      "Epoch [7/10], Batch [6100/10187], Loss: 4.1261\n",
      "Epoch [7/10], Batch [6200/10187], Loss: 3.1628\n",
      "Epoch [7/10], Batch [6300/10187], Loss: 3.2771\n",
      "Epoch [7/10], Batch [6400/10187], Loss: 4.0678\n",
      "Epoch [7/10], Batch [6500/10187], Loss: 3.5089\n",
      "Epoch [7/10], Batch [6600/10187], Loss: 3.2760\n",
      "Epoch [7/10], Batch [6700/10187], Loss: 3.5649\n",
      "Epoch [7/10], Batch [6800/10187], Loss: 3.2123\n",
      "Epoch [7/10], Batch [6900/10187], Loss: 3.3444\n",
      "Epoch [7/10], Batch [7000/10187], Loss: 4.0682\n",
      "Epoch [7/10], Batch [7100/10187], Loss: 3.9982\n",
      "Epoch [7/10], Batch [7200/10187], Loss: 4.2257\n",
      "Epoch [7/10], Batch [7300/10187], Loss: 3.4564\n",
      "Epoch [7/10], Batch [7400/10187], Loss: 3.8614\n",
      "Epoch [7/10], Batch [7500/10187], Loss: 3.9931\n",
      "Epoch [7/10], Batch [7600/10187], Loss: 4.2477\n",
      "Epoch [7/10], Batch [7700/10187], Loss: 3.6786\n",
      "Epoch [7/10], Batch [7800/10187], Loss: 3.6746\n",
      "Epoch [7/10], Batch [7900/10187], Loss: 3.9288\n",
      "Epoch [7/10], Batch [8000/10187], Loss: 4.2046\n",
      "Epoch [7/10], Batch [8100/10187], Loss: 4.4643\n",
      "Epoch [7/10], Batch [8200/10187], Loss: 4.2663\n",
      "Epoch [7/10], Batch [8300/10187], Loss: 3.7097\n",
      "Epoch [7/10], Batch [8400/10187], Loss: 3.6000\n",
      "Epoch [7/10], Batch [8500/10187], Loss: 3.7721\n",
      "Epoch [7/10], Batch [8600/10187], Loss: 3.7333\n",
      "Epoch [7/10], Batch [8700/10187], Loss: 3.3249\n",
      "Epoch [7/10], Batch [8800/10187], Loss: 3.8838\n",
      "Epoch [7/10], Batch [8900/10187], Loss: 3.1562\n",
      "Epoch [7/10], Batch [9000/10187], Loss: 3.7175\n",
      "Epoch [7/10], Batch [9100/10187], Loss: 3.5347\n",
      "Epoch [7/10], Batch [9200/10187], Loss: 4.0054\n",
      "Epoch [7/10], Batch [9300/10187], Loss: 3.9461\n",
      "Epoch [7/10], Batch [9400/10187], Loss: 3.6247\n",
      "Epoch [7/10], Batch [9500/10187], Loss: 3.8245\n",
      "Epoch [7/10], Batch [9600/10187], Loss: 3.4436\n",
      "Epoch [7/10], Batch [9700/10187], Loss: 3.9017\n",
      "Epoch [7/10], Batch [9800/10187], Loss: 3.9364\n",
      "Epoch [7/10], Batch [9900/10187], Loss: 3.3562\n",
      "Epoch [7/10], Batch [10000/10187], Loss: 3.4039\n",
      "Epoch [7/10], Batch [10100/10187], Loss: 3.7246\n",
      "Epoch [7/10] completed, Average Loss: 3.5989\n",
      "Epoch [8/10], Batch [100/10187], Loss: 2.8509\n",
      "Epoch [8/10], Batch [200/10187], Loss: 3.3150\n",
      "Epoch [8/10], Batch [300/10187], Loss: 2.5407\n",
      "Epoch [8/10], Batch [400/10187], Loss: 3.2787\n",
      "Epoch [8/10], Batch [500/10187], Loss: 2.5084\n",
      "Epoch [8/10], Batch [600/10187], Loss: 3.0934\n",
      "Epoch [8/10], Batch [700/10187], Loss: 2.5448\n",
      "Epoch [8/10], Batch [800/10187], Loss: 3.6463\n",
      "Epoch [8/10], Batch [900/10187], Loss: 3.3983\n",
      "Epoch [8/10], Batch [1000/10187], Loss: 2.6872\n",
      "Epoch [8/10], Batch [1100/10187], Loss: 3.1946\n",
      "Epoch [8/10], Batch [1200/10187], Loss: 3.4179\n",
      "Epoch [8/10], Batch [1300/10187], Loss: 2.7487\n",
      "Epoch [8/10], Batch [1400/10187], Loss: 2.9087\n",
      "Epoch [8/10], Batch [1500/10187], Loss: 4.2494\n",
      "Epoch [8/10], Batch [1600/10187], Loss: 2.8474\n",
      "Epoch [8/10], Batch [1700/10187], Loss: 2.9132\n",
      "Epoch [8/10], Batch [1800/10187], Loss: 3.0539\n",
      "Epoch [8/10], Batch [1900/10187], Loss: 3.7788\n",
      "Epoch [8/10], Batch [2000/10187], Loss: 3.6275\n",
      "Epoch [8/10], Batch [2100/10187], Loss: 3.5039\n",
      "Epoch [8/10], Batch [2200/10187], Loss: 3.4787\n",
      "Epoch [8/10], Batch [2300/10187], Loss: 3.4303\n",
      "Epoch [8/10], Batch [2400/10187], Loss: 3.5371\n",
      "Epoch [8/10], Batch [2500/10187], Loss: 3.6472\n",
      "Epoch [8/10], Batch [2600/10187], Loss: 3.2412\n",
      "Epoch [8/10], Batch [2700/10187], Loss: 3.4663\n",
      "Epoch [8/10], Batch [2800/10187], Loss: 3.5234\n",
      "Epoch [8/10], Batch [2900/10187], Loss: 3.2855\n",
      "Epoch [8/10], Batch [3000/10187], Loss: 2.8467\n",
      "Epoch [8/10], Batch [3100/10187], Loss: 3.1387\n",
      "Epoch [8/10], Batch [3200/10187], Loss: 3.4883\n",
      "Epoch [8/10], Batch [3300/10187], Loss: 3.8758\n",
      "Epoch [8/10], Batch [3400/10187], Loss: 3.4140\n",
      "Epoch [8/10], Batch [3500/10187], Loss: 3.3653\n",
      "Epoch [8/10], Batch [3600/10187], Loss: 3.6360\n",
      "Epoch [8/10], Batch [3700/10187], Loss: 2.8804\n",
      "Epoch [8/10], Batch [3800/10187], Loss: 3.4223\n",
      "Epoch [8/10], Batch [3900/10187], Loss: 3.9915\n",
      "Epoch [8/10], Batch [4000/10187], Loss: 3.3312\n",
      "Epoch [8/10], Batch [4100/10187], Loss: 3.5008\n",
      "Epoch [8/10], Batch [4200/10187], Loss: 2.9463\n",
      "Epoch [8/10], Batch [4300/10187], Loss: 3.1950\n",
      "Epoch [8/10], Batch [4400/10187], Loss: 3.7209\n",
      "Epoch [8/10], Batch [4500/10187], Loss: 3.1570\n",
      "Epoch [8/10], Batch [4600/10187], Loss: 2.3383\n",
      "Epoch [8/10], Batch [4700/10187], Loss: 3.8450\n",
      "Epoch [8/10], Batch [4800/10187], Loss: 3.3830\n",
      "Epoch [8/10], Batch [4900/10187], Loss: 3.4025\n",
      "Epoch [8/10], Batch [5000/10187], Loss: 3.2253\n",
      "Epoch [8/10], Batch [5100/10187], Loss: 3.3659\n",
      "Epoch [8/10], Batch [5200/10187], Loss: 3.5456\n",
      "Epoch [8/10], Batch [5300/10187], Loss: 3.3843\n",
      "Epoch [8/10], Batch [5400/10187], Loss: 3.2043\n",
      "Epoch [8/10], Batch [5500/10187], Loss: 3.3783\n",
      "Epoch [8/10], Batch [5600/10187], Loss: 3.0822\n",
      "Epoch [8/10], Batch [5700/10187], Loss: 3.9271\n",
      "Epoch [8/10], Batch [5800/10187], Loss: 3.8086\n",
      "Epoch [8/10], Batch [5900/10187], Loss: 3.0108\n",
      "Epoch [8/10], Batch [6000/10187], Loss: 2.9867\n",
      "Epoch [8/10], Batch [6100/10187], Loss: 3.8149\n",
      "Epoch [8/10], Batch [6200/10187], Loss: 2.7928\n",
      "Epoch [8/10], Batch [6300/10187], Loss: 2.9249\n",
      "Epoch [8/10], Batch [6400/10187], Loss: 3.6337\n",
      "Epoch [8/10], Batch [6500/10187], Loss: 3.6201\n",
      "Epoch [8/10], Batch [6600/10187], Loss: 3.0103\n",
      "Epoch [8/10], Batch [6700/10187], Loss: 4.0927\n",
      "Epoch [8/10], Batch [6800/10187], Loss: 3.6979\n",
      "Epoch [8/10], Batch [6900/10187], Loss: 3.6554\n",
      "Epoch [8/10], Batch [7000/10187], Loss: 2.8413\n",
      "Epoch [8/10], Batch [7100/10187], Loss: 3.1512\n",
      "Epoch [8/10], Batch [7200/10187], Loss: 3.2311\n",
      "Epoch [8/10], Batch [7300/10187], Loss: 3.4221\n",
      "Epoch [8/10], Batch [7400/10187], Loss: 3.3410\n",
      "Epoch [8/10], Batch [7500/10187], Loss: 3.6670\n",
      "Epoch [8/10], Batch [7600/10187], Loss: 3.8470\n",
      "Epoch [8/10], Batch [7700/10187], Loss: 3.5303\n",
      "Epoch [8/10], Batch [7800/10187], Loss: 2.8142\n",
      "Epoch [8/10], Batch [7900/10187], Loss: 3.5347\n",
      "Epoch [8/10], Batch [8000/10187], Loss: 3.3691\n",
      "Epoch [8/10], Batch [8100/10187], Loss: 3.5998\n",
      "Epoch [8/10], Batch [8200/10187], Loss: 3.2318\n",
      "Epoch [8/10], Batch [8300/10187], Loss: 3.3869\n",
      "Epoch [8/10], Batch [8400/10187], Loss: 3.2388\n",
      "Epoch [8/10], Batch [8500/10187], Loss: 3.5157\n",
      "Epoch [8/10], Batch [8600/10187], Loss: 3.1152\n",
      "Epoch [8/10], Batch [8700/10187], Loss: 3.6242\n",
      "Epoch [8/10], Batch [8800/10187], Loss: 3.8549\n",
      "Epoch [8/10], Batch [8900/10187], Loss: 3.5742\n",
      "Epoch [8/10], Batch [9000/10187], Loss: 3.9604\n",
      "Epoch [8/10], Batch [9100/10187], Loss: 3.6110\n",
      "Epoch [8/10], Batch [9200/10187], Loss: 4.1968\n",
      "Epoch [8/10], Batch [9300/10187], Loss: 3.1586\n",
      "Epoch [8/10], Batch [9400/10187], Loss: 3.6196\n",
      "Epoch [8/10], Batch [9500/10187], Loss: 3.5143\n",
      "Epoch [8/10], Batch [9600/10187], Loss: 3.1348\n",
      "Epoch [8/10], Batch [9700/10187], Loss: 4.0955\n",
      "Epoch [8/10], Batch [9800/10187], Loss: 3.3336\n",
      "Epoch [8/10], Batch [9900/10187], Loss: 3.8938\n",
      "Epoch [8/10], Batch [10000/10187], Loss: 3.3679\n",
      "Epoch [8/10], Batch [10100/10187], Loss: 3.3297\n",
      "Epoch [8/10] completed, Average Loss: 3.4013\n",
      "Epoch [9/10], Batch [100/10187], Loss: 2.5183\n",
      "Epoch [9/10], Batch [200/10187], Loss: 2.3041\n",
      "Epoch [9/10], Batch [300/10187], Loss: 2.5008\n",
      "Epoch [9/10], Batch [400/10187], Loss: 3.0481\n",
      "Epoch [9/10], Batch [500/10187], Loss: 2.4185\n",
      "Epoch [9/10], Batch [600/10187], Loss: 3.2527\n",
      "Epoch [9/10], Batch [700/10187], Loss: 3.1910\n",
      "Epoch [9/10], Batch [800/10187], Loss: 3.2017\n",
      "Epoch [9/10], Batch [900/10187], Loss: 3.5086\n",
      "Epoch [9/10], Batch [1000/10187], Loss: 2.9735\n",
      "Epoch [9/10], Batch [1100/10187], Loss: 3.2743\n",
      "Epoch [9/10], Batch [1200/10187], Loss: 2.8649\n",
      "Epoch [9/10], Batch [1300/10187], Loss: 2.7348\n",
      "Epoch [9/10], Batch [1400/10187], Loss: 2.3798\n",
      "Epoch [9/10], Batch [1500/10187], Loss: 3.3726\n",
      "Epoch [9/10], Batch [1600/10187], Loss: 3.5558\n",
      "Epoch [9/10], Batch [1700/10187], Loss: 2.5832\n",
      "Epoch [9/10], Batch [1800/10187], Loss: 2.8678\n",
      "Epoch [9/10], Batch [1900/10187], Loss: 2.8272\n",
      "Epoch [9/10], Batch [2000/10187], Loss: 3.0665\n",
      "Epoch [9/10], Batch [2100/10187], Loss: 2.8899\n",
      "Epoch [9/10], Batch [2200/10187], Loss: 2.9279\n",
      "Epoch [9/10], Batch [2300/10187], Loss: 2.8742\n",
      "Epoch [9/10], Batch [2400/10187], Loss: 2.5064\n",
      "Epoch [9/10], Batch [2500/10187], Loss: 3.1792\n",
      "Epoch [9/10], Batch [2600/10187], Loss: 3.3533\n",
      "Epoch [9/10], Batch [2700/10187], Loss: 2.9579\n",
      "Epoch [9/10], Batch [2800/10187], Loss: 2.9452\n",
      "Epoch [9/10], Batch [2900/10187], Loss: 3.5861\n",
      "Epoch [9/10], Batch [3000/10187], Loss: 2.8935\n",
      "Epoch [9/10], Batch [3100/10187], Loss: 2.9673\n",
      "Epoch [9/10], Batch [3200/10187], Loss: 3.7467\n",
      "Epoch [9/10], Batch [3300/10187], Loss: 3.4701\n",
      "Epoch [9/10], Batch [3400/10187], Loss: 3.0789\n",
      "Epoch [9/10], Batch [3500/10187], Loss: 3.3049\n",
      "Epoch [9/10], Batch [3600/10187], Loss: 4.0320\n",
      "Epoch [9/10], Batch [3700/10187], Loss: 3.4616\n",
      "Epoch [9/10], Batch [3800/10187], Loss: 2.8384\n",
      "Epoch [9/10], Batch [3900/10187], Loss: 3.0666\n",
      "Epoch [9/10], Batch [4000/10187], Loss: 3.6093\n",
      "Epoch [9/10], Batch [4100/10187], Loss: 2.9951\n",
      "Epoch [9/10], Batch [4200/10187], Loss: 2.7030\n",
      "Epoch [9/10], Batch [4300/10187], Loss: 3.5335\n",
      "Epoch [9/10], Batch [4400/10187], Loss: 3.3913\n",
      "Epoch [9/10], Batch [4500/10187], Loss: 3.4063\n",
      "Epoch [9/10], Batch [4600/10187], Loss: 3.2000\n",
      "Epoch [9/10], Batch [4700/10187], Loss: 3.9966\n",
      "Epoch [9/10], Batch [4800/10187], Loss: 2.7928\n",
      "Epoch [9/10], Batch [4900/10187], Loss: 3.4859\n",
      "Epoch [9/10], Batch [5000/10187], Loss: 2.9667\n",
      "Epoch [9/10], Batch [5100/10187], Loss: 3.4717\n",
      "Epoch [9/10], Batch [5200/10187], Loss: 4.1047\n",
      "Epoch [9/10], Batch [5300/10187], Loss: 3.2120\n",
      "Epoch [9/10], Batch [5400/10187], Loss: 3.1697\n",
      "Epoch [9/10], Batch [5500/10187], Loss: 3.4851\n",
      "Epoch [9/10], Batch [5600/10187], Loss: 3.1504\n",
      "Epoch [9/10], Batch [5700/10187], Loss: 3.9808\n",
      "Epoch [9/10], Batch [5800/10187], Loss: 3.4471\n",
      "Epoch [9/10], Batch [5900/10187], Loss: 3.1913\n",
      "Epoch [9/10], Batch [6000/10187], Loss: 3.1702\n",
      "Epoch [9/10], Batch [6100/10187], Loss: 3.2217\n",
      "Epoch [9/10], Batch [6200/10187], Loss: 3.6693\n",
      "Epoch [9/10], Batch [6300/10187], Loss: 3.5047\n",
      "Epoch [9/10], Batch [6400/10187], Loss: 3.0397\n",
      "Epoch [9/10], Batch [6500/10187], Loss: 3.3805\n",
      "Epoch [9/10], Batch [6600/10187], Loss: 3.3227\n",
      "Epoch [9/10], Batch [6700/10187], Loss: 2.9331\n",
      "Epoch [9/10], Batch [6800/10187], Loss: 3.1618\n",
      "Epoch [9/10], Batch [6900/10187], Loss: 3.3596\n",
      "Epoch [9/10], Batch [7000/10187], Loss: 3.0796\n",
      "Epoch [9/10], Batch [7100/10187], Loss: 3.9930\n",
      "Epoch [9/10], Batch [7200/10187], Loss: 3.0889\n",
      "Epoch [9/10], Batch [7300/10187], Loss: 3.9410\n",
      "Epoch [9/10], Batch [7400/10187], Loss: 3.7567\n",
      "Epoch [9/10], Batch [7500/10187], Loss: 3.6726\n",
      "Epoch [9/10], Batch [7600/10187], Loss: 3.3210\n",
      "Epoch [9/10], Batch [7700/10187], Loss: 3.1592\n",
      "Epoch [9/10], Batch [7800/10187], Loss: 3.3704\n",
      "Epoch [9/10], Batch [7900/10187], Loss: 3.2637\n",
      "Epoch [9/10], Batch [8000/10187], Loss: 3.2622\n",
      "Epoch [9/10], Batch [8100/10187], Loss: 2.8513\n",
      "Epoch [9/10], Batch [8200/10187], Loss: 2.9796\n",
      "Epoch [9/10], Batch [8300/10187], Loss: 3.4271\n",
      "Epoch [9/10], Batch [8400/10187], Loss: 3.3904\n",
      "Epoch [9/10], Batch [8500/10187], Loss: 3.4919\n",
      "Epoch [9/10], Batch [8600/10187], Loss: 3.9805\n",
      "Epoch [9/10], Batch [8700/10187], Loss: 3.0382\n",
      "Epoch [9/10], Batch [8800/10187], Loss: 3.3227\n",
      "Epoch [9/10], Batch [8900/10187], Loss: 3.8321\n",
      "Epoch [9/10], Batch [9000/10187], Loss: 3.7841\n",
      "Epoch [9/10], Batch [9100/10187], Loss: 2.8074\n",
      "Epoch [9/10], Batch [9200/10187], Loss: 3.3961\n",
      "Epoch [9/10], Batch [9300/10187], Loss: 3.4196\n",
      "Epoch [9/10], Batch [9400/10187], Loss: 4.1815\n",
      "Epoch [9/10], Batch [9500/10187], Loss: 2.9128\n",
      "Epoch [9/10], Batch [9600/10187], Loss: 4.1069\n",
      "Epoch [9/10], Batch [9700/10187], Loss: 3.6317\n",
      "Epoch [9/10], Batch [9800/10187], Loss: 3.4094\n",
      "Epoch [9/10], Batch [9900/10187], Loss: 2.8617\n",
      "Epoch [9/10], Batch [10000/10187], Loss: 3.1339\n",
      "Epoch [9/10], Batch [10100/10187], Loss: 3.5850\n",
      "Epoch [9/10] completed, Average Loss: 3.2221\n",
      "Epoch [10/10], Batch [100/10187], Loss: 2.1641\n",
      "Epoch [10/10], Batch [200/10187], Loss: 3.0408\n",
      "Epoch [10/10], Batch [300/10187], Loss: 2.2249\n",
      "Epoch [10/10], Batch [400/10187], Loss: 2.8626\n",
      "Epoch [10/10], Batch [500/10187], Loss: 2.2998\n",
      "Epoch [10/10], Batch [600/10187], Loss: 2.5227\n",
      "Epoch [10/10], Batch [700/10187], Loss: 2.5198\n",
      "Epoch [10/10], Batch [800/10187], Loss: 2.9998\n",
      "Epoch [10/10], Batch [900/10187], Loss: 2.7440\n",
      "Epoch [10/10], Batch [1000/10187], Loss: 2.5998\n",
      "Epoch [10/10], Batch [1100/10187], Loss: 2.7008\n",
      "Epoch [10/10], Batch [1200/10187], Loss: 2.9451\n",
      "Epoch [10/10], Batch [1300/10187], Loss: 2.6078\n",
      "Epoch [10/10], Batch [1400/10187], Loss: 2.8032\n",
      "Epoch [10/10], Batch [1500/10187], Loss: 3.1429\n",
      "Epoch [10/10], Batch [1600/10187], Loss: 3.2024\n",
      "Epoch [10/10], Batch [1700/10187], Loss: 2.5520\n",
      "Epoch [10/10], Batch [1800/10187], Loss: 2.5352\n",
      "Epoch [10/10], Batch [1900/10187], Loss: 2.5312\n",
      "Epoch [10/10], Batch [2000/10187], Loss: 3.4014\n",
      "Epoch [10/10], Batch [2100/10187], Loss: 3.0961\n",
      "Epoch [10/10], Batch [2200/10187], Loss: 3.3610\n",
      "Epoch [10/10], Batch [2300/10187], Loss: 3.0509\n",
      "Epoch [10/10], Batch [2400/10187], Loss: 3.2248\n",
      "Epoch [10/10], Batch [2500/10187], Loss: 2.5802\n",
      "Epoch [10/10], Batch [2600/10187], Loss: 2.7050\n",
      "Epoch [10/10], Batch [2700/10187], Loss: 2.6427\n",
      "Epoch [10/10], Batch [2800/10187], Loss: 2.6320\n",
      "Epoch [10/10], Batch [2900/10187], Loss: 4.0247\n",
      "Epoch [10/10], Batch [3000/10187], Loss: 3.2000\n",
      "Epoch [10/10], Batch [3100/10187], Loss: 2.6891\n",
      "Epoch [10/10], Batch [3200/10187], Loss: 3.1242\n",
      "Epoch [10/10], Batch [3300/10187], Loss: 3.0894\n",
      "Epoch [10/10], Batch [3400/10187], Loss: 2.7577\n",
      "Epoch [10/10], Batch [3500/10187], Loss: 2.3223\n",
      "Epoch [10/10], Batch [3600/10187], Loss: 2.7352\n",
      "Epoch [10/10], Batch [3700/10187], Loss: 3.6177\n",
      "Epoch [10/10], Batch [3800/10187], Loss: 2.9920\n",
      "Epoch [10/10], Batch [3900/10187], Loss: 3.5317\n",
      "Epoch [10/10], Batch [4000/10187], Loss: 3.1258\n",
      "Epoch [10/10], Batch [4100/10187], Loss: 2.9246\n",
      "Epoch [10/10], Batch [4200/10187], Loss: 2.9545\n",
      "Epoch [10/10], Batch [4300/10187], Loss: 2.4550\n",
      "Epoch [10/10], Batch [4400/10187], Loss: 2.7056\n",
      "Epoch [10/10], Batch [4500/10187], Loss: 2.4855\n",
      "Epoch [10/10], Batch [4600/10187], Loss: 3.8524\n",
      "Epoch [10/10], Batch [4700/10187], Loss: 3.7017\n",
      "Epoch [10/10], Batch [4800/10187], Loss: 2.9188\n",
      "Epoch [10/10], Batch [4900/10187], Loss: 3.4261\n",
      "Epoch [10/10], Batch [5000/10187], Loss: 2.9541\n",
      "Epoch [10/10], Batch [5100/10187], Loss: 2.9328\n",
      "Epoch [10/10], Batch [5200/10187], Loss: 3.1430\n",
      "Epoch [10/10], Batch [5300/10187], Loss: 3.3634\n",
      "Epoch [10/10], Batch [5400/10187], Loss: 3.4029\n",
      "Epoch [10/10], Batch [5500/10187], Loss: 3.0300\n",
      "Epoch [10/10], Batch [5600/10187], Loss: 3.2454\n",
      "Epoch [10/10], Batch [5700/10187], Loss: 3.5072\n",
      "Epoch [10/10], Batch [5800/10187], Loss: 3.3225\n",
      "Epoch [10/10], Batch [5900/10187], Loss: 3.4339\n",
      "Epoch [10/10], Batch [6000/10187], Loss: 2.6060\n",
      "Epoch [10/10], Batch [6100/10187], Loss: 3.2166\n",
      "Epoch [10/10], Batch [6200/10187], Loss: 2.5790\n",
      "Epoch [10/10], Batch [6300/10187], Loss: 2.2990\n",
      "Epoch [10/10], Batch [6400/10187], Loss: 3.3311\n",
      "Epoch [10/10], Batch [6500/10187], Loss: 3.2791\n",
      "Epoch [10/10], Batch [6600/10187], Loss: 3.6120\n",
      "Epoch [10/10], Batch [6700/10187], Loss: 3.0242\n",
      "Epoch [10/10], Batch [6800/10187], Loss: 3.8866\n",
      "Epoch [10/10], Batch [6900/10187], Loss: 3.7271\n",
      "Epoch [10/10], Batch [7000/10187], Loss: 3.0605\n",
      "Epoch [10/10], Batch [7100/10187], Loss: 3.0185\n",
      "Epoch [10/10], Batch [7200/10187], Loss: 3.2106\n",
      "Epoch [10/10], Batch [7300/10187], Loss: 3.5735\n",
      "Epoch [10/10], Batch [7400/10187], Loss: 3.5994\n",
      "Epoch [10/10], Batch [7500/10187], Loss: 3.9710\n",
      "Epoch [10/10], Batch [7600/10187], Loss: 3.3552\n",
      "Epoch [10/10], Batch [7700/10187], Loss: 3.4386\n",
      "Epoch [10/10], Batch [7800/10187], Loss: 3.5244\n",
      "Epoch [10/10], Batch [7900/10187], Loss: 3.4656\n",
      "Epoch [10/10], Batch [8000/10187], Loss: 3.0605\n",
      "Epoch [10/10], Batch [8100/10187], Loss: 3.6363\n",
      "Epoch [10/10], Batch [8200/10187], Loss: 3.6613\n",
      "Epoch [10/10], Batch [8300/10187], Loss: 3.4768\n",
      "Epoch [10/10], Batch [8400/10187], Loss: 2.9759\n",
      "Epoch [10/10], Batch [8500/10187], Loss: 3.0901\n",
      "Epoch [10/10], Batch [8600/10187], Loss: 3.1338\n",
      "Epoch [10/10], Batch [8700/10187], Loss: 3.4620\n",
      "Epoch [10/10], Batch [8800/10187], Loss: 3.9507\n",
      "Epoch [10/10], Batch [8900/10187], Loss: 3.4431\n",
      "Epoch [10/10], Batch [9000/10187], Loss: 3.7779\n",
      "Epoch [10/10], Batch [9100/10187], Loss: 4.2574\n",
      "Epoch [10/10], Batch [9200/10187], Loss: 4.1692\n",
      "Epoch [10/10], Batch [9300/10187], Loss: 3.2912\n",
      "Epoch [10/10], Batch [9400/10187], Loss: 3.3531\n",
      "Epoch [10/10], Batch [9500/10187], Loss: 2.9090\n",
      "Epoch [10/10], Batch [9600/10187], Loss: 3.3077\n",
      "Epoch [10/10], Batch [9700/10187], Loss: 3.4847\n",
      "Epoch [10/10], Batch [9800/10187], Loss: 3.9504\n",
      "Epoch [10/10], Batch [9900/10187], Loss: 3.1834\n",
      "Epoch [10/10], Batch [10000/10187], Loss: 3.3879\n",
      "Epoch [10/10], Batch [10100/10187], Loss: 3.3957\n",
      "Epoch [10/10] completed, Average Loss: 3.0657\n",
      "Model training completed.\n",
      "Predicted next word: my\n"
     ]
    }
   ],
   "source": [
    "# Create custom Dataset\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, sequences):\n",
    "        self.sequences = sequences\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sequence, target = self.sequences[idx]\n",
    "        return torch.tensor(sequence), torch.tensor(target)\n",
    "\n",
    "# Define LSTM model\n",
    "class NextWordLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
    "        super(NextWordLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        lstm_out = lstm_out[:, -1, :]  # Take the output of the last LSTM cell\n",
    "        out = self.fc(lstm_out)\n",
    "        return out\n",
    "\n",
    "# Training loop\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch_idx, (sequences, targets) in enumerate(train_loader):\n",
    "            sequences, targets = sequences.to(device), targets.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(sequences)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Log progress every 100 batches\n",
    "            if (batch_idx + 1) % 100 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "        \n",
    "        # Log epoch summary\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}] completed, Average Loss: {avg_loss:.4f}')\n",
    "\n",
    "# Predict the next word\n",
    "def predict_next_word(model, sequence, word_to_idx, idx_to_word):\n",
    "    model.eval()\n",
    "    sequence = torch.tensor(sequence).unsqueeze(0).to(device)  # Add batch dimension\n",
    "    with torch.no_grad():\n",
    "        output = model(sequence)\n",
    "        predicted_idx = torch.argmax(output, dim=1).item()\n",
    "    return idx_to_word[predicted_idx]\n",
    "\n",
    "def get_csv_files(folder_path):\n",
    "    return [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "def read_csv_files(folder_path):\n",
    "    csv_files = get_csv_files(folder_path)[:5]\n",
    "    if not csv_files:\n",
    "        raise FileNotFoundError(f\"No CSV files found in {folder_path}\")\n",
    "    \n",
    "    all_texts = []\n",
    "    for file_path in csv_files:\n",
    "        df = pd.read_csv(file_path)\n",
    "        if 'text' in df.columns:\n",
    "            all_texts.extend(df['text'].tolist())\n",
    "        elif 'Content' in df.columns:\n",
    "            all_texts.extend(df['Content'].tolist())\n",
    "        else:\n",
    "            print(f\"Warning: No 'text' or 'Content' column found in {file_path}\")\n",
    "    \n",
    "    return all_texts\n",
    "\n",
    "# Read data from all CSV files in the 'x' and 'emails' folders\n",
    "x_folder_path = '../../data/x'\n",
    "emails_folder_path = '../../data/emails'\n",
    "\n",
    "texts = read_csv_files(x_folder_path)\n",
    "\n",
    "print(f\"Loaded {len(texts)} text samples from CSV files.\")\n",
    "\n",
    "# Build vocabulary with a cap of 5000 most common words\n",
    "word_to_idx, tokenized_texts = build_vocab(texts)\n",
    "word_to_idx = dict(sorted(word_to_idx.items(), key=lambda x: x[1])[:5000])\n",
    "word_to_idx['<UNK>'] = len(word_to_idx)  # Add unknown token\n",
    "\n",
    "# Filter sequences to only include words in the vocabulary\n",
    "def filter_sequences(tokenized_texts, word_to_idx, seq_length=10):\n",
    "    filtered_sequences = []\n",
    "    for tokens in tokenized_texts:\n",
    "        if len(tokens) < seq_length + 1:\n",
    "            continue\n",
    "        for i in range(seq_length, len(tokens)):\n",
    "            seq = tokens[i-seq_length:i]\n",
    "            target = tokens[i]\n",
    "            if all(word in word_to_idx for word in seq) and target in word_to_idx:\n",
    "                encoded_seq = [word_to_idx[word] for word in seq]\n",
    "                encoded_target = word_to_idx[target]\n",
    "                filtered_sequences.append((encoded_seq, encoded_target))\n",
    "    return filtered_sequences\n",
    "\n",
    "sequences = filter_sequences(tokenized_texts, word_to_idx, seq_length=4)\n",
    "\n",
    "\n",
    "print(f\"Vocabulary size: {len(word_to_idx)}\")\n",
    "print(f\"Number of sequences: {len(sequences)}\")\n",
    "\n",
    "# Create Dataset and DataLoader\n",
    "dataset = TextDataset(sequences)\n",
    "train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Define the model, loss function, and optimizer\n",
    "vocab_size = len(word_to_idx)\n",
    "embed_size = 128\n",
    "hidden_size = 256\n",
    "num_layers = 2\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = NextWordLSTM(vocab_size, embed_size, hidden_size, num_layers).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"Starting model training...\")\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, criterion, optimizer, num_epochs=10)\n",
    "\n",
    "print(\"Model training completed.\")\n",
    "\n",
    "# Test prediction (example sequence)\n",
    "example_sequence = [word_to_idx['so'], word_to_idx['sad'], word_to_idx['to'], word_to_idx['learn']]  # Sequence from data\n",
    "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "predicted_word = predict_next_word(model, example_sequence, word_to_idx, idx_to_word)\n",
    "print(f'Predicted next word: {predicted_word}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "street\n"
     ]
    }
   ],
   "source": [
    "indata = [word_to_idx[words] for words in \"what a waste of time i was hoping\".split()]\n",
    "print(predict_next_word(model, indata, word_to_idx, idx_to_word))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to next_word_lstm_model.pth\n",
      "Vocabulary saved to vocabulary.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# Save the trained model\n",
    "model_save_path = 'next_word_lstm_model.pth'\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "# Save the vocabulary (word_to_idx dictionary)<\n",
    "vocab_save_path = 'vocabulary.json'\n",
    "with open(vocab_save_path, 'w') as f:\n",
    "    json.dump(word_to_idx, f)\n",
    "print(f\"Vocabulary saved to {vocab_save_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
