{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Preprocessing: Clean and Tokenize Text Data\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)  # Remove URLs\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)  # Remove user names\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)  # Remove special characters and numbers\n",
    "    text = text.lower().strip()  # Lowercase and strip whitespaces\n",
    "    return text\n",
    "\n",
    "# Tokenize the text\n",
    "def tokenize_text(text):\n",
    "    return text.split()\n",
    "\n",
    "# Build a vocabulary and tokenize the dataset\n",
    "def build_vocab(texts):\n",
    "    tokenized_texts = [tokenize_text(clean_text(text)) for text in texts]\n",
    "    all_words = [word for text in tokenized_texts for word in text]\n",
    "    word_counts = Counter(all_words)\n",
    "    sorted_words = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "    \n",
    "    # Create a mapping from word to index\n",
    "    word_to_idx = {word: idx+1 for idx, word in enumerate(sorted_words)}\n",
    "    word_to_idx['<PAD>'] = 0  # Padding index\n",
    "    return word_to_idx, tokenized_texts\n",
    "\n",
    "# Convert sequences of words to sequences of integers\n",
    "def encode_sequences(tokenized_texts, word_to_idx, seq_length=4):\n",
    "    sequences = []\n",
    "    for tokens in tokenized_texts:\n",
    "        if len(tokens) < seq_length:\n",
    "            continue\n",
    "        for i in range(seq_length, len(tokens)):\n",
    "            seq = tokens[i-seq_length:i]  # Input sequence of words\n",
    "            target = tokens[i]  # Target word (next word)\n",
    "            encoded_seq = [word_to_idx[word] for word in seq]\n",
    "            encoded_target = word_to_idx[target]\n",
    "            sequences.append((encoded_seq, encoded_target))\n",
    "    return sequences\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Batch [100/15033], Loss: 4.0246\n",
      "Epoch [1/10], Batch [200/15033], Loss: 4.7928\n",
      "Epoch [1/10], Batch [300/15033], Loss: 3.7313\n",
      "Epoch [1/10], Batch [400/15033], Loss: 3.4175\n",
      "Epoch [1/10], Batch [500/15033], Loss: 3.8200\n",
      "Epoch [1/10], Batch [600/15033], Loss: 2.7125\n",
      "Epoch [1/10], Batch [700/15033], Loss: 3.2242\n",
      "Epoch [1/10], Batch [800/15033], Loss: 3.0812\n",
      "Epoch [1/10], Batch [900/15033], Loss: 3.1804\n",
      "Epoch [1/10], Batch [1000/15033], Loss: 2.9325\n",
      "Epoch [1/10], Batch [1100/15033], Loss: 4.0897\n",
      "Epoch [1/10], Batch [1200/15033], Loss: 3.5993\n",
      "Epoch [1/10], Batch [1300/15033], Loss: 2.9626\n",
      "Epoch [1/10], Batch [1400/15033], Loss: 2.8005\n",
      "Epoch [1/10], Batch [1500/15033], Loss: 3.3586\n",
      "Epoch [1/10], Batch [1600/15033], Loss: 3.8237\n",
      "Epoch [1/10], Batch [1700/15033], Loss: 3.6323\n",
      "Epoch [1/10], Batch [1800/15033], Loss: 2.5344\n",
      "Epoch [1/10], Batch [1900/15033], Loss: 3.8406\n",
      "Epoch [1/10], Batch [2000/15033], Loss: 3.2437\n",
      "Epoch [1/10], Batch [2100/15033], Loss: 3.3370\n",
      "Epoch [1/10], Batch [2200/15033], Loss: 3.7295\n",
      "Epoch [1/10], Batch [2300/15033], Loss: 2.6634\n",
      "Epoch [1/10], Batch [2400/15033], Loss: 2.6925\n",
      "Epoch [1/10], Batch [2500/15033], Loss: 3.2937\n",
      "Epoch [1/10], Batch [2600/15033], Loss: 2.7136\n",
      "Epoch [1/10], Batch [2700/15033], Loss: 4.0877\n",
      "Epoch [1/10], Batch [2800/15033], Loss: 3.7081\n",
      "Epoch [1/10], Batch [2900/15033], Loss: 3.0540\n",
      "Epoch [1/10], Batch [3000/15033], Loss: 3.8870\n",
      "Epoch [1/10], Batch [3100/15033], Loss: 3.0483\n",
      "Epoch [1/10], Batch [3200/15033], Loss: 3.5396\n",
      "Epoch [1/10], Batch [3300/15033], Loss: 2.8536\n",
      "Epoch [1/10], Batch [3400/15033], Loss: 2.2152\n",
      "Epoch [1/10], Batch [3500/15033], Loss: 2.8186\n",
      "Epoch [1/10], Batch [3600/15033], Loss: 3.0502\n",
      "Epoch [1/10], Batch [3700/15033], Loss: 2.6809\n",
      "Epoch [1/10], Batch [3800/15033], Loss: 3.6700\n",
      "Epoch [1/10], Batch [3900/15033], Loss: 4.0262\n",
      "Epoch [1/10], Batch [4000/15033], Loss: 3.5309\n",
      "Epoch [1/10], Batch [4100/15033], Loss: 3.6424\n",
      "Epoch [1/10], Batch [4200/15033], Loss: 3.6077\n",
      "Epoch [1/10], Batch [4300/15033], Loss: 3.3018\n",
      "Epoch [1/10], Batch [4400/15033], Loss: 3.4484\n",
      "Epoch [1/10], Batch [4500/15033], Loss: 4.0140\n",
      "Epoch [1/10], Batch [4600/15033], Loss: 3.9537\n",
      "Epoch [1/10], Batch [4700/15033], Loss: 4.5780\n",
      "Epoch [1/10], Batch [4800/15033], Loss: 3.5506\n",
      "Epoch [1/10], Batch [4900/15033], Loss: 2.9280\n",
      "Epoch [1/10], Batch [5000/15033], Loss: 3.0975\n",
      "Epoch [1/10], Batch [5100/15033], Loss: 3.2764\n",
      "Epoch [1/10], Batch [5200/15033], Loss: 2.7685\n",
      "Epoch [1/10], Batch [5300/15033], Loss: 4.0118\n",
      "Epoch [1/10], Batch [5400/15033], Loss: 3.1435\n",
      "Epoch [1/10], Batch [5500/15033], Loss: 3.5665\n",
      "Epoch [1/10], Batch [5600/15033], Loss: 3.3943\n",
      "Epoch [1/10], Batch [5700/15033], Loss: 2.8018\n",
      "Epoch [1/10], Batch [5800/15033], Loss: 4.0560\n",
      "Epoch [1/10], Batch [5900/15033], Loss: 2.2867\n",
      "Epoch [1/10], Batch [6000/15033], Loss: 2.2434\n",
      "Epoch [1/10], Batch [6100/15033], Loss: 3.0307\n",
      "Epoch [1/10], Batch [6200/15033], Loss: 4.6791\n",
      "Epoch [1/10], Batch [6300/15033], Loss: 2.9826\n",
      "Epoch [1/10], Batch [6400/15033], Loss: 3.9657\n",
      "Epoch [1/10], Batch [6500/15033], Loss: 3.7478\n",
      "Epoch [1/10], Batch [6600/15033], Loss: 2.6840\n",
      "Epoch [1/10], Batch [6700/15033], Loss: 4.1096\n",
      "Epoch [1/10], Batch [6800/15033], Loss: 2.9030\n",
      "Epoch [1/10], Batch [6900/15033], Loss: 3.3013\n",
      "Epoch [1/10], Batch [7000/15033], Loss: 3.5624\n",
      "Epoch [1/10], Batch [7100/15033], Loss: 2.6162\n",
      "Epoch [1/10], Batch [7200/15033], Loss: 2.6972\n",
      "Epoch [1/10], Batch [7300/15033], Loss: 3.2951\n",
      "Epoch [1/10], Batch [7400/15033], Loss: 3.5319\n",
      "Epoch [1/10], Batch [7500/15033], Loss: 3.3550\n",
      "Epoch [1/10], Batch [7600/15033], Loss: 3.6319\n",
      "Epoch [1/10], Batch [7700/15033], Loss: 3.8361\n",
      "Epoch [1/10], Batch [7800/15033], Loss: 2.8023\n",
      "Epoch [1/10], Batch [7900/15033], Loss: 3.0916\n",
      "Epoch [1/10], Batch [8000/15033], Loss: 3.0326\n",
      "Epoch [1/10], Batch [8100/15033], Loss: 4.7421\n",
      "Epoch [1/10], Batch [8200/15033], Loss: 3.1351\n",
      "Epoch [1/10], Batch [8300/15033], Loss: 3.7844\n",
      "Epoch [1/10], Batch [8400/15033], Loss: 3.1068\n",
      "Epoch [1/10], Batch [8500/15033], Loss: 2.2556\n",
      "Epoch [1/10], Batch [8600/15033], Loss: 2.5966\n",
      "Epoch [1/10], Batch [8700/15033], Loss: 3.7099\n",
      "Epoch [1/10], Batch [8800/15033], Loss: 2.7474\n",
      "Epoch [1/10], Batch [8900/15033], Loss: 5.1206\n",
      "Epoch [1/10], Batch [9000/15033], Loss: 4.9742\n",
      "Epoch [1/10], Batch [9100/15033], Loss: 3.1605\n",
      "Epoch [1/10], Batch [9200/15033], Loss: 2.6527\n",
      "Epoch [1/10], Batch [9300/15033], Loss: 3.3058\n",
      "Epoch [1/10], Batch [9400/15033], Loss: 4.2482\n",
      "Epoch [1/10], Batch [9500/15033], Loss: 3.7106\n",
      "Epoch [1/10], Batch [9600/15033], Loss: 3.8982\n",
      "Epoch [1/10], Batch [9700/15033], Loss: 3.6358\n",
      "Epoch [1/10], Batch [9800/15033], Loss: 3.8730\n",
      "Epoch [1/10], Batch [9900/15033], Loss: 4.1932\n",
      "Epoch [1/10], Batch [10000/15033], Loss: 3.4190\n",
      "Epoch [1/10], Batch [10100/15033], Loss: 2.9571\n",
      "Epoch [1/10], Batch [10200/15033], Loss: 3.4842\n",
      "Epoch [1/10], Batch [10300/15033], Loss: 4.0578\n",
      "Epoch [1/10], Batch [10400/15033], Loss: 3.5082\n",
      "Epoch [1/10], Batch [10500/15033], Loss: 3.7753\n",
      "Epoch [1/10], Batch [10600/15033], Loss: 2.0516\n",
      "Epoch [1/10], Batch [10700/15033], Loss: 2.9506\n",
      "Epoch [1/10], Batch [10800/15033], Loss: 2.7991\n",
      "Epoch [1/10], Batch [10900/15033], Loss: 3.0389\n",
      "Epoch [1/10], Batch [11000/15033], Loss: 3.1664\n",
      "Epoch [1/10], Batch [11100/15033], Loss: 3.3049\n",
      "Epoch [1/10], Batch [11200/15033], Loss: 2.8162\n",
      "Epoch [1/10], Batch [11300/15033], Loss: 3.4245\n",
      "Epoch [1/10], Batch [11400/15033], Loss: 3.2293\n",
      "Epoch [1/10], Batch [11500/15033], Loss: 3.1807\n",
      "Epoch [1/10], Batch [11600/15033], Loss: 2.8270\n",
      "Epoch [1/10], Batch [11700/15033], Loss: 3.3003\n",
      "Epoch [1/10], Batch [11800/15033], Loss: 3.1059\n",
      "Epoch [1/10], Batch [11900/15033], Loss: 3.5147\n",
      "Epoch [1/10], Batch [12000/15033], Loss: 3.4051\n",
      "Epoch [1/10], Batch [12100/15033], Loss: 3.9253\n",
      "Epoch [1/10], Batch [12200/15033], Loss: 3.2851\n",
      "Epoch [1/10], Batch [12300/15033], Loss: 3.5152\n",
      "Epoch [1/10], Batch [12400/15033], Loss: 2.9837\n",
      "Epoch [1/10], Batch [12500/15033], Loss: 3.6527\n",
      "Epoch [1/10], Batch [12600/15033], Loss: 2.5252\n",
      "Epoch [1/10], Batch [12700/15033], Loss: 2.7052\n",
      "Epoch [1/10], Batch [12800/15033], Loss: 3.4841\n",
      "Epoch [1/10], Batch [12900/15033], Loss: 3.0216\n",
      "Epoch [1/10], Batch [13000/15033], Loss: 4.6716\n",
      "Epoch [1/10], Batch [13100/15033], Loss: 4.9661\n",
      "Epoch [1/10], Batch [13200/15033], Loss: 2.9632\n",
      "Epoch [1/10], Batch [13300/15033], Loss: 3.1746\n",
      "Epoch [1/10], Batch [13400/15033], Loss: 4.0306\n",
      "Epoch [1/10], Batch [13500/15033], Loss: 4.3379\n",
      "Epoch [1/10], Batch [13600/15033], Loss: 3.1494\n",
      "Epoch [1/10], Batch [13700/15033], Loss: 2.8797\n",
      "Epoch [1/10], Batch [13800/15033], Loss: 2.9526\n",
      "Epoch [1/10], Batch [13900/15033], Loss: 3.3109\n",
      "Epoch [1/10], Batch [14000/15033], Loss: 2.7126\n",
      "Epoch [1/10], Batch [14100/15033], Loss: 3.7059\n",
      "Epoch [1/10], Batch [14200/15033], Loss: 3.6692\n",
      "Epoch [1/10], Batch [14300/15033], Loss: 3.6027\n",
      "Epoch [1/10], Batch [14400/15033], Loss: 2.9018\n",
      "Epoch [1/10], Batch [14500/15033], Loss: 2.4190\n",
      "Epoch [1/10], Batch [14600/15033], Loss: 3.4443\n",
      "Epoch [1/10], Batch [14700/15033], Loss: 2.5936\n",
      "Epoch [1/10], Batch [14800/15033], Loss: 2.7491\n",
      "Epoch [1/10], Batch [14900/15033], Loss: 2.9132\n",
      "Epoch [1/10], Batch [15000/15033], Loss: 2.5404\n",
      "Epoch [1/10] completed, Average Loss: 3.3526\n",
      "Epoch [2/10], Batch [100/15033], Loss: 3.3472\n",
      "Epoch [2/10], Batch [200/15033], Loss: 3.3841\n",
      "Epoch [2/10], Batch [300/15033], Loss: 3.2450\n",
      "Epoch [2/10], Batch [400/15033], Loss: 3.1043\n",
      "Epoch [2/10], Batch [500/15033], Loss: 3.3162\n",
      "Epoch [2/10], Batch [600/15033], Loss: 2.9262\n",
      "Epoch [2/10], Batch [700/15033], Loss: 3.0486\n",
      "Epoch [2/10], Batch [800/15033], Loss: 3.1302\n",
      "Epoch [2/10], Batch [900/15033], Loss: 3.5322\n",
      "Epoch [2/10], Batch [1000/15033], Loss: 3.5954\n",
      "Epoch [2/10], Batch [1100/15033], Loss: 2.8491\n",
      "Epoch [2/10], Batch [1200/15033], Loss: 3.4777\n",
      "Epoch [2/10], Batch [1300/15033], Loss: 2.9157\n",
      "Epoch [2/10], Batch [1400/15033], Loss: 3.4169\n",
      "Epoch [2/10], Batch [1500/15033], Loss: 4.2093\n",
      "Epoch [2/10], Batch [1600/15033], Loss: 3.1972\n",
      "Epoch [2/10], Batch [1700/15033], Loss: 3.6808\n",
      "Epoch [2/10], Batch [1800/15033], Loss: 3.9658\n",
      "Epoch [2/10], Batch [1900/15033], Loss: 3.8852\n",
      "Epoch [2/10], Batch [2000/15033], Loss: 3.1951\n",
      "Epoch [2/10], Batch [2100/15033], Loss: 3.6968\n",
      "Epoch [2/10], Batch [2200/15033], Loss: 2.6240\n",
      "Epoch [2/10], Batch [2300/15033], Loss: 4.0667\n",
      "Epoch [2/10], Batch [2400/15033], Loss: 3.0979\n",
      "Epoch [2/10], Batch [2500/15033], Loss: 2.9627\n",
      "Epoch [2/10], Batch [2600/15033], Loss: 4.5042\n",
      "Epoch [2/10], Batch [2700/15033], Loss: 2.2020\n",
      "Epoch [2/10], Batch [2800/15033], Loss: 2.4750\n",
      "Epoch [2/10], Batch [2900/15033], Loss: 5.0980\n",
      "Epoch [2/10], Batch [3000/15033], Loss: 4.5933\n",
      "Epoch [2/10], Batch [3100/15033], Loss: 3.5332\n",
      "Epoch [2/10], Batch [3200/15033], Loss: 3.8522\n",
      "Epoch [2/10], Batch [3300/15033], Loss: 3.3219\n",
      "Epoch [2/10], Batch [3400/15033], Loss: 3.3500\n",
      "Epoch [2/10], Batch [3500/15033], Loss: 2.4280\n",
      "Epoch [2/10], Batch [3600/15033], Loss: 3.1384\n",
      "Epoch [2/10], Batch [3700/15033], Loss: 2.9494\n",
      "Epoch [2/10], Batch [3800/15033], Loss: 3.6407\n",
      "Epoch [2/10], Batch [3900/15033], Loss: 4.3175\n",
      "Epoch [2/10], Batch [4000/15033], Loss: 3.9513\n",
      "Epoch [2/10], Batch [4100/15033], Loss: 3.4741\n",
      "Epoch [2/10], Batch [4200/15033], Loss: 3.3416\n",
      "Epoch [2/10], Batch [4300/15033], Loss: 3.0984\n",
      "Epoch [2/10], Batch [4400/15033], Loss: 3.1228\n",
      "Epoch [2/10], Batch [4500/15033], Loss: 3.0592\n",
      "Epoch [2/10], Batch [4600/15033], Loss: 3.6152\n",
      "Epoch [2/10], Batch [4700/15033], Loss: 3.4467\n",
      "Epoch [2/10], Batch [4800/15033], Loss: 4.2891\n",
      "Epoch [2/10], Batch [4900/15033], Loss: 3.7239\n",
      "Epoch [2/10], Batch [5000/15033], Loss: 2.9209\n",
      "Epoch [2/10], Batch [5100/15033], Loss: 3.2974\n",
      "Epoch [2/10], Batch [5200/15033], Loss: 3.2475\n",
      "Epoch [2/10], Batch [5300/15033], Loss: 2.9956\n",
      "Epoch [2/10], Batch [5400/15033], Loss: 2.4340\n",
      "Epoch [2/10], Batch [5500/15033], Loss: 3.5381\n",
      "Epoch [2/10], Batch [5600/15033], Loss: 2.6839\n",
      "Epoch [2/10], Batch [5700/15033], Loss: 3.3762\n",
      "Epoch [2/10], Batch [5800/15033], Loss: 2.8116\n",
      "Epoch [2/10], Batch [5900/15033], Loss: 3.5133\n",
      "Epoch [2/10], Batch [6000/15033], Loss: 3.3174\n",
      "Epoch [2/10], Batch [6100/15033], Loss: 3.1454\n",
      "Epoch [2/10], Batch [6200/15033], Loss: 3.7316\n",
      "Epoch [2/10], Batch [6300/15033], Loss: 3.1981\n",
      "Epoch [2/10], Batch [6400/15033], Loss: 5.1221\n",
      "Epoch [2/10], Batch [6500/15033], Loss: 3.5827\n",
      "Epoch [2/10], Batch [6600/15033], Loss: 3.0808\n",
      "Epoch [2/10], Batch [6700/15033], Loss: 3.9951\n",
      "Epoch [2/10], Batch [6800/15033], Loss: 4.1173\n",
      "Epoch [2/10], Batch [6900/15033], Loss: 2.9822\n",
      "Epoch [2/10], Batch [7000/15033], Loss: 2.8378\n",
      "Epoch [2/10], Batch [7100/15033], Loss: 4.1915\n",
      "Epoch [2/10], Batch [7200/15033], Loss: 4.0181\n",
      "Epoch [2/10], Batch [7300/15033], Loss: 3.0598\n",
      "Epoch [2/10], Batch [7400/15033], Loss: 3.1925\n",
      "Epoch [2/10], Batch [7500/15033], Loss: 3.1925\n",
      "Epoch [2/10], Batch [7600/15033], Loss: 3.4879\n",
      "Epoch [2/10], Batch [7700/15033], Loss: 3.0320\n",
      "Epoch [2/10], Batch [7800/15033], Loss: 2.9255\n",
      "Epoch [2/10], Batch [7900/15033], Loss: 3.0584\n",
      "Epoch [2/10], Batch [8000/15033], Loss: 3.5292\n",
      "Epoch [2/10], Batch [8100/15033], Loss: 3.9261\n",
      "Epoch [2/10], Batch [8200/15033], Loss: 2.8207\n",
      "Epoch [2/10], Batch [8300/15033], Loss: 3.2417\n",
      "Epoch [2/10], Batch [8400/15033], Loss: 3.7372\n",
      "Epoch [2/10], Batch [8500/15033], Loss: 3.1046\n",
      "Epoch [2/10], Batch [8600/15033], Loss: 4.0546\n",
      "Epoch [2/10], Batch [8700/15033], Loss: 4.7116\n",
      "Epoch [2/10], Batch [8800/15033], Loss: 3.0618\n",
      "Epoch [2/10], Batch [8900/15033], Loss: 2.9138\n",
      "Epoch [2/10], Batch [9000/15033], Loss: 3.4258\n",
      "Epoch [2/10], Batch [9100/15033], Loss: 3.5235\n",
      "Epoch [2/10], Batch [9200/15033], Loss: 3.1168\n",
      "Epoch [2/10], Batch [9300/15033], Loss: 3.3120\n",
      "Epoch [2/10], Batch [9400/15033], Loss: 2.8696\n",
      "Epoch [2/10], Batch [9500/15033], Loss: 4.4470\n",
      "Epoch [2/10], Batch [9600/15033], Loss: 3.1762\n",
      "Epoch [2/10], Batch [9700/15033], Loss: 2.7752\n",
      "Epoch [2/10], Batch [9800/15033], Loss: 3.2707\n",
      "Epoch [2/10], Batch [9900/15033], Loss: 3.8079\n",
      "Epoch [2/10], Batch [10000/15033], Loss: 4.2494\n",
      "Epoch [2/10], Batch [10100/15033], Loss: 2.0299\n",
      "Epoch [2/10], Batch [10200/15033], Loss: 3.2584\n",
      "Epoch [2/10], Batch [10300/15033], Loss: 3.6651\n",
      "Epoch [2/10], Batch [10400/15033], Loss: 2.3487\n",
      "Epoch [2/10], Batch [10500/15033], Loss: 2.6869\n",
      "Epoch [2/10], Batch [10600/15033], Loss: 2.9122\n",
      "Epoch [2/10], Batch [10700/15033], Loss: 3.4381\n",
      "Epoch [2/10], Batch [10800/15033], Loss: 2.8991\n",
      "Epoch [2/10], Batch [10900/15033], Loss: 3.2504\n",
      "Epoch [2/10], Batch [11000/15033], Loss: 2.8115\n",
      "Epoch [2/10], Batch [11100/15033], Loss: 3.4904\n",
      "Epoch [2/10], Batch [11200/15033], Loss: 4.2065\n",
      "Epoch [2/10], Batch [11300/15033], Loss: 4.8397\n",
      "Epoch [2/10], Batch [11400/15033], Loss: 3.6526\n",
      "Epoch [2/10], Batch [11500/15033], Loss: 2.2373\n",
      "Epoch [2/10], Batch [11600/15033], Loss: 3.7298\n",
      "Epoch [2/10], Batch [11700/15033], Loss: 5.2828\n",
      "Epoch [2/10], Batch [11800/15033], Loss: 2.8760\n",
      "Epoch [2/10], Batch [11900/15033], Loss: 2.7872\n",
      "Epoch [2/10], Batch [12000/15033], Loss: 3.4384\n",
      "Epoch [2/10], Batch [12100/15033], Loss: 2.7071\n",
      "Epoch [2/10], Batch [12200/15033], Loss: 4.7640\n",
      "Epoch [2/10], Batch [12300/15033], Loss: 2.8818\n",
      "Epoch [2/10], Batch [12400/15033], Loss: 5.0566\n",
      "Epoch [2/10], Batch [12500/15033], Loss: 3.1062\n",
      "Epoch [2/10], Batch [12600/15033], Loss: 3.1092\n",
      "Epoch [2/10], Batch [12700/15033], Loss: 2.6725\n",
      "Epoch [2/10], Batch [12800/15033], Loss: 2.6124\n",
      "Epoch [2/10], Batch [12900/15033], Loss: 3.4849\n",
      "Epoch [2/10], Batch [13000/15033], Loss: 3.0589\n",
      "Epoch [2/10], Batch [13100/15033], Loss: 3.5693\n",
      "Epoch [2/10], Batch [13200/15033], Loss: 3.7362\n",
      "Epoch [2/10], Batch [13300/15033], Loss: 2.8425\n",
      "Epoch [2/10], Batch [13400/15033], Loss: 3.7661\n",
      "Epoch [2/10], Batch [13500/15033], Loss: 3.1391\n",
      "Epoch [2/10], Batch [13600/15033], Loss: 3.7549\n",
      "Epoch [2/10], Batch [13700/15033], Loss: 2.9221\n",
      "Epoch [2/10], Batch [13800/15033], Loss: 5.8712\n",
      "Epoch [2/10], Batch [13900/15033], Loss: 3.3522\n",
      "Epoch [2/10], Batch [14000/15033], Loss: 2.8803\n",
      "Epoch [2/10], Batch [14100/15033], Loss: 4.8211\n",
      "Epoch [2/10], Batch [14200/15033], Loss: 2.9640\n",
      "Epoch [2/10], Batch [14300/15033], Loss: 3.1364\n",
      "Epoch [2/10], Batch [14400/15033], Loss: 2.5343\n",
      "Epoch [2/10], Batch [14500/15033], Loss: 3.9942\n",
      "Epoch [2/10], Batch [14600/15033], Loss: 4.3032\n",
      "Epoch [2/10], Batch [14700/15033], Loss: 3.4072\n",
      "Epoch [2/10], Batch [14800/15033], Loss: 3.4063\n",
      "Epoch [2/10], Batch [14900/15033], Loss: 4.5250\n",
      "Epoch [2/10], Batch [15000/15033], Loss: 2.3080\n",
      "Epoch [2/10] completed, Average Loss: 3.3276\n",
      "Epoch [3/10], Batch [100/15033], Loss: 3.0794\n",
      "Epoch [3/10], Batch [200/15033], Loss: 3.3598\n",
      "Epoch [3/10], Batch [300/15033], Loss: 2.8729\n",
      "Epoch [3/10], Batch [400/15033], Loss: 3.7224\n",
      "Epoch [3/10], Batch [500/15033], Loss: 3.8181\n",
      "Epoch [3/10], Batch [600/15033], Loss: 2.9352\n",
      "Epoch [3/10], Batch [700/15033], Loss: 2.8161\n",
      "Epoch [3/10], Batch [800/15033], Loss: 2.7050\n",
      "Epoch [3/10], Batch [900/15033], Loss: 2.8993\n",
      "Epoch [3/10], Batch [1000/15033], Loss: 3.2580\n",
      "Epoch [3/10], Batch [1100/15033], Loss: 3.9523\n",
      "Epoch [3/10], Batch [1200/15033], Loss: 3.3161\n",
      "Epoch [3/10], Batch [1300/15033], Loss: 2.9563\n",
      "Epoch [3/10], Batch [1400/15033], Loss: 3.1852\n",
      "Epoch [3/10], Batch [1500/15033], Loss: 4.2088\n",
      "Epoch [3/10], Batch [1600/15033], Loss: 2.7856\n",
      "Epoch [3/10], Batch [1700/15033], Loss: 3.0793\n",
      "Epoch [3/10], Batch [1800/15033], Loss: 4.2361\n",
      "Epoch [3/10], Batch [1900/15033], Loss: 3.4668\n",
      "Epoch [3/10], Batch [2000/15033], Loss: 2.6036\n",
      "Epoch [3/10], Batch [2100/15033], Loss: 5.1961\n",
      "Epoch [3/10], Batch [2200/15033], Loss: 4.1908\n",
      "Epoch [3/10], Batch [2300/15033], Loss: 2.8471\n",
      "Epoch [3/10], Batch [2400/15033], Loss: 3.8364\n",
      "Epoch [3/10], Batch [2500/15033], Loss: 2.9599\n",
      "Epoch [3/10], Batch [2600/15033], Loss: 5.0228\n",
      "Epoch [3/10], Batch [2700/15033], Loss: 2.9368\n",
      "Epoch [3/10], Batch [2800/15033], Loss: 3.4562\n",
      "Epoch [3/10], Batch [2900/15033], Loss: 3.2517\n",
      "Epoch [3/10], Batch [3000/15033], Loss: 2.8589\n",
      "Epoch [3/10], Batch [3100/15033], Loss: 3.3825\n",
      "Epoch [3/10], Batch [3200/15033], Loss: 3.3460\n",
      "Epoch [3/10], Batch [3300/15033], Loss: 3.1381\n",
      "Epoch [3/10], Batch [3400/15033], Loss: 3.5499\n",
      "Epoch [3/10], Batch [3500/15033], Loss: 3.1639\n",
      "Epoch [3/10], Batch [3600/15033], Loss: 2.7742\n",
      "Epoch [3/10], Batch [3700/15033], Loss: 4.2942\n",
      "Epoch [3/10], Batch [3800/15033], Loss: 3.1332\n",
      "Epoch [3/10], Batch [3900/15033], Loss: 2.1482\n",
      "Epoch [3/10], Batch [4000/15033], Loss: 3.9155\n",
      "Epoch [3/10], Batch [4100/15033], Loss: 3.1378\n",
      "Epoch [3/10], Batch [4200/15033], Loss: 3.3118\n",
      "Epoch [3/10], Batch [4300/15033], Loss: 2.7191\n",
      "Epoch [3/10], Batch [4400/15033], Loss: 2.4362\n",
      "Epoch [3/10], Batch [4500/15033], Loss: 3.5354\n",
      "Epoch [3/10], Batch [4600/15033], Loss: 3.5026\n",
      "Epoch [3/10], Batch [4700/15033], Loss: 2.7255\n",
      "Epoch [3/10], Batch [4800/15033], Loss: 3.4845\n",
      "Epoch [3/10], Batch [4900/15033], Loss: 2.3558\n",
      "Epoch [3/10], Batch [5000/15033], Loss: 3.7005\n",
      "Epoch [3/10], Batch [5100/15033], Loss: 3.3921\n",
      "Epoch [3/10], Batch [5200/15033], Loss: 4.9554\n",
      "Epoch [3/10], Batch [5300/15033], Loss: 4.4785\n",
      "Epoch [3/10], Batch [5400/15033], Loss: 3.9022\n",
      "Epoch [3/10], Batch [5500/15033], Loss: 2.8655\n",
      "Epoch [3/10], Batch [5600/15033], Loss: 2.7835\n",
      "Epoch [3/10], Batch [5700/15033], Loss: 3.4292\n",
      "Epoch [3/10], Batch [5800/15033], Loss: 3.2935\n",
      "Epoch [3/10], Batch [5900/15033], Loss: 4.1211\n",
      "Epoch [3/10], Batch [6000/15033], Loss: 3.6884\n",
      "Epoch [3/10], Batch [6100/15033], Loss: 2.8821\n",
      "Epoch [3/10], Batch [6200/15033], Loss: 2.9692\n",
      "Epoch [3/10], Batch [6300/15033], Loss: 3.0255\n",
      "Epoch [3/10], Batch [6400/15033], Loss: 3.5546\n",
      "Epoch [3/10], Batch [6500/15033], Loss: 3.1556\n",
      "Epoch [3/10], Batch [6600/15033], Loss: 3.0964\n",
      "Epoch [3/10], Batch [6700/15033], Loss: 2.5934\n",
      "Epoch [3/10], Batch [6800/15033], Loss: 3.3410\n",
      "Epoch [3/10], Batch [6900/15033], Loss: 3.1694\n",
      "Epoch [3/10], Batch [7000/15033], Loss: 3.6538\n",
      "Epoch [3/10], Batch [7100/15033], Loss: 3.8664\n",
      "Epoch [3/10], Batch [7200/15033], Loss: 3.5635\n",
      "Epoch [3/10], Batch [7300/15033], Loss: 3.0634\n",
      "Epoch [3/10], Batch [7400/15033], Loss: 3.2212\n",
      "Epoch [3/10], Batch [7500/15033], Loss: 2.7773\n",
      "Epoch [3/10], Batch [7600/15033], Loss: 4.4221\n",
      "Epoch [3/10], Batch [7700/15033], Loss: 2.7114\n",
      "Epoch [3/10], Batch [7800/15033], Loss: 3.0721\n",
      "Epoch [3/10], Batch [7900/15033], Loss: 3.7815\n",
      "Epoch [3/10], Batch [8000/15033], Loss: 4.0393\n",
      "Epoch [3/10], Batch [8100/15033], Loss: 3.3885\n",
      "Epoch [3/10], Batch [8200/15033], Loss: 2.8102\n",
      "Epoch [3/10], Batch [8300/15033], Loss: 3.1727\n",
      "Epoch [3/10], Batch [8400/15033], Loss: 2.5274\n",
      "Epoch [3/10], Batch [8500/15033], Loss: 3.7542\n",
      "Epoch [3/10], Batch [8600/15033], Loss: 3.0405\n",
      "Epoch [3/10], Batch [8700/15033], Loss: 3.2113\n",
      "Epoch [3/10], Batch [8800/15033], Loss: 4.2087\n",
      "Epoch [3/10], Batch [8900/15033], Loss: 2.2740\n",
      "Epoch [3/10], Batch [9000/15033], Loss: 3.2422\n",
      "Epoch [3/10], Batch [9100/15033], Loss: 3.5110\n",
      "Epoch [3/10], Batch [9200/15033], Loss: 3.2108\n",
      "Epoch [3/10], Batch [9300/15033], Loss: 2.8066\n",
      "Epoch [3/10], Batch [9400/15033], Loss: 3.6913\n",
      "Epoch [3/10], Batch [9500/15033], Loss: 2.3621\n",
      "Epoch [3/10], Batch [9600/15033], Loss: 2.9314\n",
      "Epoch [3/10], Batch [9700/15033], Loss: 3.7122\n",
      "Epoch [3/10], Batch [9800/15033], Loss: 3.3366\n",
      "Epoch [3/10], Batch [9900/15033], Loss: 2.6538\n",
      "Epoch [3/10], Batch [10000/15033], Loss: 3.0166\n",
      "Epoch [3/10], Batch [10100/15033], Loss: 3.5524\n",
      "Epoch [3/10], Batch [10200/15033], Loss: 2.9278\n",
      "Epoch [3/10], Batch [10300/15033], Loss: 3.5665\n",
      "Epoch [3/10], Batch [10400/15033], Loss: 3.4025\n",
      "Epoch [3/10], Batch [10500/15033], Loss: 2.4356\n",
      "Epoch [3/10], Batch [10600/15033], Loss: 2.7861\n",
      "Epoch [3/10], Batch [10700/15033], Loss: 2.8059\n",
      "Epoch [3/10], Batch [10800/15033], Loss: 3.8740\n",
      "Epoch [3/10], Batch [10900/15033], Loss: 2.9324\n",
      "Epoch [3/10], Batch [11000/15033], Loss: 2.8278\n",
      "Epoch [3/10], Batch [11100/15033], Loss: 3.2133\n",
      "Epoch [3/10], Batch [11200/15033], Loss: 3.8284\n",
      "Epoch [3/10], Batch [11300/15033], Loss: 3.1283\n",
      "Epoch [3/10], Batch [11400/15033], Loss: 4.1829\n",
      "Epoch [3/10], Batch [11500/15033], Loss: 3.6502\n",
      "Epoch [3/10], Batch [11600/15033], Loss: 3.3814\n",
      "Epoch [3/10], Batch [11700/15033], Loss: 2.8049\n",
      "Epoch [3/10], Batch [11800/15033], Loss: 3.4968\n",
      "Epoch [3/10], Batch [11900/15033], Loss: 2.5996\n",
      "Epoch [3/10], Batch [12000/15033], Loss: 3.9562\n",
      "Epoch [3/10], Batch [12100/15033], Loss: 4.3818\n",
      "Epoch [3/10], Batch [12200/15033], Loss: 3.5956\n",
      "Epoch [3/10], Batch [12300/15033], Loss: 2.3545\n",
      "Epoch [3/10], Batch [12400/15033], Loss: 2.8240\n",
      "Epoch [3/10], Batch [12500/15033], Loss: 3.7234\n",
      "Epoch [3/10], Batch [12600/15033], Loss: 3.0502\n",
      "Epoch [3/10], Batch [12700/15033], Loss: 3.5039\n",
      "Epoch [3/10], Batch [12800/15033], Loss: 3.3417\n",
      "Epoch [3/10], Batch [12900/15033], Loss: 2.7911\n",
      "Epoch [3/10], Batch [13000/15033], Loss: 3.2216\n",
      "Epoch [3/10], Batch [13100/15033], Loss: 3.1927\n",
      "Epoch [3/10], Batch [13200/15033], Loss: 3.2675\n",
      "Epoch [3/10], Batch [13300/15033], Loss: 4.2279\n",
      "Epoch [3/10], Batch [13400/15033], Loss: 2.8370\n",
      "Epoch [3/10], Batch [13500/15033], Loss: 3.1919\n",
      "Epoch [3/10], Batch [13600/15033], Loss: 2.6504\n",
      "Epoch [3/10], Batch [13700/15033], Loss: 2.4052\n",
      "Epoch [3/10], Batch [13800/15033], Loss: 2.8241\n",
      "Epoch [3/10], Batch [13900/15033], Loss: 3.9571\n",
      "Epoch [3/10], Batch [14000/15033], Loss: 4.0862\n",
      "Epoch [3/10], Batch [14100/15033], Loss: 3.2932\n",
      "Epoch [3/10], Batch [14200/15033], Loss: 2.6215\n",
      "Epoch [3/10], Batch [14300/15033], Loss: 2.9019\n",
      "Epoch [3/10], Batch [14400/15033], Loss: 3.3026\n",
      "Epoch [3/10], Batch [14500/15033], Loss: 3.3228\n",
      "Epoch [3/10], Batch [14600/15033], Loss: 3.7181\n",
      "Epoch [3/10], Batch [14700/15033], Loss: 3.8823\n",
      "Epoch [3/10], Batch [14800/15033], Loss: 3.2457\n",
      "Epoch [3/10], Batch [14900/15033], Loss: 2.7567\n",
      "Epoch [3/10], Batch [15000/15033], Loss: 2.7817\n",
      "Epoch [3/10] completed, Average Loss: 3.3051\n",
      "Epoch [4/10], Batch [100/15033], Loss: 3.4324\n",
      "Epoch [4/10], Batch [200/15033], Loss: 2.9006\n",
      "Epoch [4/10], Batch [300/15033], Loss: 2.7189\n",
      "Epoch [4/10], Batch [400/15033], Loss: 3.8424\n",
      "Epoch [4/10], Batch [500/15033], Loss: 3.9675\n",
      "Epoch [4/10], Batch [600/15033], Loss: 4.9394\n",
      "Epoch [4/10], Batch [700/15033], Loss: 4.2257\n",
      "Epoch [4/10], Batch [800/15033], Loss: 3.4818\n",
      "Epoch [4/10], Batch [900/15033], Loss: 2.7342\n",
      "Epoch [4/10], Batch [1000/15033], Loss: 2.9626\n",
      "Epoch [4/10], Batch [1100/15033], Loss: 3.0025\n",
      "Epoch [4/10], Batch [1200/15033], Loss: 3.5024\n",
      "Epoch [4/10], Batch [1300/15033], Loss: 3.5249\n",
      "Epoch [4/10], Batch [1400/15033], Loss: 3.4045\n",
      "Epoch [4/10], Batch [1500/15033], Loss: 3.5147\n",
      "Epoch [4/10], Batch [1600/15033], Loss: 2.6789\n",
      "Epoch [4/10], Batch [1700/15033], Loss: 3.7275\n",
      "Epoch [4/10], Batch [1800/15033], Loss: 3.4866\n",
      "Epoch [4/10], Batch [1900/15033], Loss: 3.3592\n",
      "Epoch [4/10], Batch [2000/15033], Loss: 3.3410\n",
      "Epoch [4/10], Batch [2100/15033], Loss: 3.1077\n",
      "Epoch [4/10], Batch [2200/15033], Loss: 5.1471\n",
      "Epoch [4/10], Batch [2300/15033], Loss: 3.5561\n",
      "Epoch [4/10], Batch [2400/15033], Loss: 2.8272\n",
      "Epoch [4/10], Batch [2500/15033], Loss: 4.1196\n",
      "Epoch [4/10], Batch [2600/15033], Loss: 4.9620\n",
      "Epoch [4/10], Batch [2700/15033], Loss: 3.7340\n",
      "Epoch [4/10], Batch [2800/15033], Loss: 3.2513\n",
      "Epoch [4/10], Batch [2900/15033], Loss: 4.2474\n",
      "Epoch [4/10], Batch [3000/15033], Loss: 3.0043\n",
      "Epoch [4/10], Batch [3100/15033], Loss: 3.1471\n",
      "Epoch [4/10], Batch [3200/15033], Loss: 3.7233\n",
      "Epoch [4/10], Batch [3300/15033], Loss: 2.2862\n",
      "Epoch [4/10], Batch [3400/15033], Loss: 4.4222\n",
      "Epoch [4/10], Batch [3500/15033], Loss: 3.1153\n",
      "Epoch [4/10], Batch [3600/15033], Loss: 2.8465\n",
      "Epoch [4/10], Batch [3700/15033], Loss: 2.6947\n",
      "Epoch [4/10], Batch [3800/15033], Loss: 2.7331\n",
      "Epoch [4/10], Batch [3900/15033], Loss: 2.7217\n",
      "Epoch [4/10], Batch [4000/15033], Loss: 3.5073\n",
      "Epoch [4/10], Batch [4100/15033], Loss: 3.6571\n",
      "Epoch [4/10], Batch [4200/15033], Loss: 3.0249\n",
      "Epoch [4/10], Batch [4300/15033], Loss: 3.1668\n",
      "Epoch [4/10], Batch [4400/15033], Loss: 3.3564\n",
      "Epoch [4/10], Batch [4500/15033], Loss: 2.8231\n",
      "Epoch [4/10], Batch [4600/15033], Loss: 3.5456\n",
      "Epoch [4/10], Batch [4700/15033], Loss: 3.0970\n",
      "Epoch [4/10], Batch [4800/15033], Loss: 3.0785\n",
      "Epoch [4/10], Batch [4900/15033], Loss: 3.5034\n",
      "Epoch [4/10], Batch [5000/15033], Loss: 3.0638\n",
      "Epoch [4/10], Batch [5100/15033], Loss: 3.3058\n",
      "Epoch [4/10], Batch [5200/15033], Loss: 2.7490\n",
      "Epoch [4/10], Batch [5300/15033], Loss: 3.0052\n",
      "Epoch [4/10], Batch [5400/15033], Loss: 3.0374\n",
      "Epoch [4/10], Batch [5500/15033], Loss: 3.8548\n",
      "Epoch [4/10], Batch [5600/15033], Loss: 3.3119\n",
      "Epoch [4/10], Batch [5700/15033], Loss: 3.7193\n",
      "Epoch [4/10], Batch [5800/15033], Loss: 2.4184\n",
      "Epoch [4/10], Batch [5900/15033], Loss: 3.8173\n",
      "Epoch [4/10], Batch [6000/15033], Loss: 3.4374\n",
      "Epoch [4/10], Batch [6100/15033], Loss: 3.1053\n",
      "Epoch [4/10], Batch [6200/15033], Loss: 4.6264\n",
      "Epoch [4/10], Batch [6300/15033], Loss: 3.0379\n",
      "Epoch [4/10], Batch [6400/15033], Loss: 3.6335\n",
      "Epoch [4/10], Batch [6500/15033], Loss: 3.2689\n",
      "Epoch [4/10], Batch [6600/15033], Loss: 3.1519\n",
      "Epoch [4/10], Batch [6700/15033], Loss: 2.0927\n",
      "Epoch [4/10], Batch [6800/15033], Loss: 3.4822\n",
      "Epoch [4/10], Batch [6900/15033], Loss: 5.0238\n",
      "Epoch [4/10], Batch [7000/15033], Loss: 2.5700\n",
      "Epoch [4/10], Batch [7100/15033], Loss: 3.3544\n",
      "Epoch [4/10], Batch [7200/15033], Loss: 3.2129\n",
      "Epoch [4/10], Batch [7300/15033], Loss: 3.4631\n",
      "Epoch [4/10], Batch [7400/15033], Loss: 2.7089\n",
      "Epoch [4/10], Batch [7500/15033], Loss: 3.7050\n",
      "Epoch [4/10], Batch [7600/15033], Loss: 3.0092\n",
      "Epoch [4/10], Batch [7700/15033], Loss: 2.4900\n",
      "Epoch [4/10], Batch [7800/15033], Loss: 3.4101\n",
      "Epoch [4/10], Batch [7900/15033], Loss: 2.6958\n",
      "Epoch [4/10], Batch [8000/15033], Loss: 2.9668\n",
      "Epoch [4/10], Batch [8100/15033], Loss: 2.3139\n",
      "Epoch [4/10], Batch [8200/15033], Loss: 2.8659\n",
      "Epoch [4/10], Batch [8300/15033], Loss: 3.9645\n",
      "Epoch [4/10], Batch [8400/15033], Loss: 2.7243\n",
      "Epoch [4/10], Batch [8500/15033], Loss: 3.2813\n",
      "Epoch [4/10], Batch [8600/15033], Loss: 2.6159\n",
      "Epoch [4/10], Batch [8700/15033], Loss: 2.8531\n",
      "Epoch [4/10], Batch [8800/15033], Loss: 2.6267\n",
      "Epoch [4/10], Batch [8900/15033], Loss: 3.2685\n",
      "Epoch [4/10], Batch [9000/15033], Loss: 3.1631\n",
      "Epoch [4/10], Batch [9100/15033], Loss: 3.6029\n",
      "Epoch [4/10], Batch [9200/15033], Loss: 3.1970\n",
      "Epoch [4/10], Batch [9300/15033], Loss: 3.2001\n",
      "Epoch [4/10], Batch [9400/15033], Loss: 2.9677\n",
      "Epoch [4/10], Batch [9500/15033], Loss: 3.1608\n",
      "Epoch [4/10], Batch [9600/15033], Loss: 3.4704\n",
      "Epoch [4/10], Batch [9700/15033], Loss: 3.6003\n",
      "Epoch [4/10], Batch [9800/15033], Loss: 3.2867\n",
      "Epoch [4/10], Batch [9900/15033], Loss: 2.3231\n",
      "Epoch [4/10], Batch [10000/15033], Loss: 3.1416\n",
      "Epoch [4/10], Batch [10100/15033], Loss: 2.5810\n",
      "Epoch [4/10], Batch [10200/15033], Loss: 4.1983\n",
      "Epoch [4/10], Batch [10300/15033], Loss: 3.7773\n",
      "Epoch [4/10], Batch [10400/15033], Loss: 3.0091\n",
      "Epoch [4/10], Batch [10500/15033], Loss: 3.7447\n",
      "Epoch [4/10], Batch [10600/15033], Loss: 3.4101\n",
      "Epoch [4/10], Batch [10700/15033], Loss: 2.8785\n",
      "Epoch [4/10], Batch [10800/15033], Loss: 3.9286\n",
      "Epoch [4/10], Batch [10900/15033], Loss: 2.7859\n",
      "Epoch [4/10], Batch [11000/15033], Loss: 3.4341\n",
      "Epoch [4/10], Batch [11100/15033], Loss: 2.9532\n",
      "Epoch [4/10], Batch [11200/15033], Loss: 2.9543\n",
      "Epoch [4/10], Batch [11300/15033], Loss: 3.6812\n",
      "Epoch [4/10], Batch [11400/15033], Loss: 2.6744\n",
      "Epoch [4/10], Batch [11500/15033], Loss: 3.6310\n",
      "Epoch [4/10], Batch [11600/15033], Loss: 3.4308\n",
      "Epoch [4/10], Batch [11700/15033], Loss: 2.9107\n",
      "Epoch [4/10], Batch [11800/15033], Loss: 4.5149\n",
      "Epoch [4/10], Batch [11900/15033], Loss: 2.3422\n",
      "Epoch [4/10], Batch [12000/15033], Loss: 2.5901\n",
      "Epoch [4/10], Batch [12100/15033], Loss: 2.4824\n",
      "Epoch [4/10], Batch [12200/15033], Loss: 4.4771\n",
      "Epoch [4/10], Batch [12300/15033], Loss: 2.7513\n",
      "Epoch [4/10], Batch [12400/15033], Loss: 2.9654\n",
      "Epoch [4/10], Batch [12500/15033], Loss: 2.7281\n",
      "Epoch [4/10], Batch [12600/15033], Loss: 2.1031\n",
      "Epoch [4/10], Batch [12700/15033], Loss: 2.8974\n",
      "Epoch [4/10], Batch [12800/15033], Loss: 4.5689\n",
      "Epoch [4/10], Batch [12900/15033], Loss: 2.7376\n",
      "Epoch [4/10], Batch [13000/15033], Loss: 3.3353\n",
      "Epoch [4/10], Batch [13100/15033], Loss: 2.8310\n",
      "Epoch [4/10], Batch [13200/15033], Loss: 2.3051\n",
      "Epoch [4/10], Batch [13300/15033], Loss: 2.9382\n",
      "Epoch [4/10], Batch [13400/15033], Loss: 3.1181\n",
      "Epoch [4/10], Batch [13500/15033], Loss: 3.8322\n",
      "Epoch [4/10], Batch [13600/15033], Loss: 3.9177\n",
      "Epoch [4/10], Batch [13700/15033], Loss: 4.2105\n",
      "Epoch [4/10], Batch [13800/15033], Loss: 2.6496\n",
      "Epoch [4/10], Batch [13900/15033], Loss: 4.3169\n",
      "Epoch [4/10], Batch [14000/15033], Loss: 2.9826\n",
      "Epoch [4/10], Batch [14100/15033], Loss: 3.3052\n",
      "Epoch [4/10], Batch [14200/15033], Loss: 2.7134\n",
      "Epoch [4/10], Batch [14300/15033], Loss: 3.3084\n",
      "Epoch [4/10], Batch [14400/15033], Loss: 2.9202\n",
      "Epoch [4/10], Batch [14500/15033], Loss: 4.9850\n",
      "Epoch [4/10], Batch [14600/15033], Loss: 3.3901\n",
      "Epoch [4/10], Batch [14700/15033], Loss: 3.1492\n",
      "Epoch [4/10], Batch [14800/15033], Loss: 3.0932\n",
      "Epoch [4/10], Batch [14900/15033], Loss: 3.1103\n",
      "Epoch [4/10], Batch [15000/15033], Loss: 2.9583\n",
      "Epoch [4/10] completed, Average Loss: 3.2817\n",
      "Epoch [5/10], Batch [100/15033], Loss: 3.7133\n",
      "Epoch [5/10], Batch [200/15033], Loss: 2.3985\n",
      "Epoch [5/10], Batch [300/15033], Loss: 2.9339\n",
      "Epoch [5/10], Batch [400/15033], Loss: 3.7308\n",
      "Epoch [5/10], Batch [500/15033], Loss: 3.2163\n",
      "Epoch [5/10], Batch [600/15033], Loss: 4.0760\n",
      "Epoch [5/10], Batch [700/15033], Loss: 4.1299\n",
      "Epoch [5/10], Batch [800/15033], Loss: 2.6517\n",
      "Epoch [5/10], Batch [900/15033], Loss: 3.4125\n",
      "Epoch [5/10], Batch [1000/15033], Loss: 3.1890\n",
      "Epoch [5/10], Batch [1100/15033], Loss: 3.7853\n",
      "Epoch [5/10], Batch [1200/15033], Loss: 3.2076\n",
      "Epoch [5/10], Batch [1300/15033], Loss: 3.4322\n",
      "Epoch [5/10], Batch [1400/15033], Loss: 2.9943\n",
      "Epoch [5/10], Batch [1500/15033], Loss: 3.8907\n",
      "Epoch [5/10], Batch [1600/15033], Loss: 3.2850\n",
      "Epoch [5/10], Batch [1700/15033], Loss: 2.7812\n",
      "Epoch [5/10], Batch [1800/15033], Loss: 3.1764\n",
      "Epoch [5/10], Batch [1900/15033], Loss: 3.6091\n",
      "Epoch [5/10], Batch [2000/15033], Loss: 4.3943\n",
      "Epoch [5/10], Batch [2100/15033], Loss: 2.8738\n",
      "Epoch [5/10], Batch [2200/15033], Loss: 2.5042\n",
      "Epoch [5/10], Batch [2300/15033], Loss: 3.7075\n",
      "Epoch [5/10], Batch [2400/15033], Loss: 3.4117\n",
      "Epoch [5/10], Batch [2500/15033], Loss: 2.8767\n",
      "Epoch [5/10], Batch [2600/15033], Loss: 2.9429\n",
      "Epoch [5/10], Batch [2700/15033], Loss: 3.0467\n",
      "Epoch [5/10], Batch [2800/15033], Loss: 3.8872\n",
      "Epoch [5/10], Batch [2900/15033], Loss: 2.7350\n",
      "Epoch [5/10], Batch [3000/15033], Loss: 3.0009\n",
      "Epoch [5/10], Batch [3100/15033], Loss: 2.3966\n",
      "Epoch [5/10], Batch [3200/15033], Loss: 2.5483\n",
      "Epoch [5/10], Batch [3300/15033], Loss: 3.5797\n",
      "Epoch [5/10], Batch [3400/15033], Loss: 3.1142\n",
      "Epoch [5/10], Batch [3500/15033], Loss: 3.3156\n",
      "Epoch [5/10], Batch [3600/15033], Loss: 3.7092\n",
      "Epoch [5/10], Batch [3700/15033], Loss: 4.2244\n",
      "Epoch [5/10], Batch [3800/15033], Loss: 3.2817\n",
      "Epoch [5/10], Batch [3900/15033], Loss: 2.4892\n",
      "Epoch [5/10], Batch [4000/15033], Loss: 3.3609\n",
      "Epoch [5/10], Batch [4100/15033], Loss: 2.7476\n",
      "Epoch [5/10], Batch [4200/15033], Loss: 2.8625\n",
      "Epoch [5/10], Batch [4300/15033], Loss: 3.3684\n",
      "Epoch [5/10], Batch [4400/15033], Loss: 2.7563\n",
      "Epoch [5/10], Batch [4500/15033], Loss: 3.3056\n",
      "Epoch [5/10], Batch [4600/15033], Loss: 3.0219\n",
      "Epoch [5/10], Batch [4700/15033], Loss: 2.6439\n",
      "Epoch [5/10], Batch [4800/15033], Loss: 3.1795\n",
      "Epoch [5/10], Batch [4900/15033], Loss: 3.5478\n",
      "Epoch [5/10], Batch [5000/15033], Loss: 3.0005\n",
      "Epoch [5/10], Batch [5100/15033], Loss: 3.6369\n",
      "Epoch [5/10], Batch [5200/15033], Loss: 3.7033\n",
      "Epoch [5/10], Batch [5300/15033], Loss: 4.2119\n",
      "Epoch [5/10], Batch [5400/15033], Loss: 4.0059\n",
      "Epoch [5/10], Batch [5500/15033], Loss: 3.5366\n",
      "Epoch [5/10], Batch [5600/15033], Loss: 2.4736\n",
      "Epoch [5/10], Batch [5700/15033], Loss: 3.3546\n",
      "Epoch [5/10], Batch [5800/15033], Loss: 3.1076\n",
      "Epoch [5/10], Batch [5900/15033], Loss: 3.5842\n",
      "Epoch [5/10], Batch [6000/15033], Loss: 2.5640\n",
      "Epoch [5/10], Batch [6100/15033], Loss: 3.0235\n",
      "Epoch [5/10], Batch [6200/15033], Loss: 2.9358\n",
      "Epoch [5/10], Batch [6300/15033], Loss: 3.3605\n",
      "Epoch [5/10], Batch [6400/15033], Loss: 3.8819\n",
      "Epoch [5/10], Batch [6500/15033], Loss: 3.0495\n",
      "Epoch [5/10], Batch [6600/15033], Loss: 2.8669\n",
      "Epoch [5/10], Batch [6700/15033], Loss: 3.2144\n",
      "Epoch [5/10], Batch [6800/15033], Loss: 3.0313\n",
      "Epoch [5/10], Batch [6900/15033], Loss: 4.5121\n",
      "Epoch [5/10], Batch [7000/15033], Loss: 3.4089\n",
      "Epoch [5/10], Batch [7100/15033], Loss: 3.3525\n",
      "Epoch [5/10], Batch [7200/15033], Loss: 2.5818\n",
      "Epoch [5/10], Batch [7300/15033], Loss: 3.4183\n",
      "Epoch [5/10], Batch [7400/15033], Loss: 3.2950\n",
      "Epoch [5/10], Batch [7500/15033], Loss: 3.0159\n",
      "Epoch [5/10], Batch [7600/15033], Loss: 4.4655\n",
      "Epoch [5/10], Batch [7700/15033], Loss: 4.3881\n",
      "Epoch [5/10], Batch [7800/15033], Loss: 3.1884\n",
      "Epoch [5/10], Batch [7900/15033], Loss: 2.4838\n",
      "Epoch [5/10], Batch [8000/15033], Loss: 3.9465\n",
      "Epoch [5/10], Batch [8100/15033], Loss: 2.7824\n",
      "Epoch [5/10], Batch [8200/15033], Loss: 3.1352\n",
      "Epoch [5/10], Batch [8300/15033], Loss: 2.6679\n",
      "Epoch [5/10], Batch [8400/15033], Loss: 2.4624\n",
      "Epoch [5/10], Batch [8500/15033], Loss: 2.2029\n",
      "Epoch [5/10], Batch [8600/15033], Loss: 2.2938\n",
      "Epoch [5/10], Batch [8700/15033], Loss: 3.1751\n",
      "Epoch [5/10], Batch [8800/15033], Loss: 3.5269\n",
      "Epoch [5/10], Batch [8900/15033], Loss: 4.2765\n",
      "Epoch [5/10], Batch [9000/15033], Loss: 4.0901\n",
      "Epoch [5/10], Batch [9100/15033], Loss: 2.8370\n",
      "Epoch [5/10], Batch [9200/15033], Loss: 4.2373\n",
      "Epoch [5/10], Batch [9300/15033], Loss: 2.9371\n",
      "Epoch [5/10], Batch [9400/15033], Loss: 4.2332\n",
      "Epoch [5/10], Batch [9500/15033], Loss: 2.8129\n",
      "Epoch [5/10], Batch [9600/15033], Loss: 4.5565\n",
      "Epoch [5/10], Batch [9700/15033], Loss: 4.0124\n",
      "Epoch [5/10], Batch [9800/15033], Loss: 4.0560\n",
      "Epoch [5/10], Batch [9900/15033], Loss: 2.4716\n",
      "Epoch [5/10], Batch [10000/15033], Loss: 2.2297\n",
      "Epoch [5/10], Batch [10100/15033], Loss: 3.2317\n",
      "Epoch [5/10], Batch [10200/15033], Loss: 3.3982\n",
      "Epoch [5/10], Batch [10300/15033], Loss: 2.3524\n",
      "Epoch [5/10], Batch [10400/15033], Loss: 2.8585\n",
      "Epoch [5/10], Batch [10500/15033], Loss: 2.2639\n",
      "Epoch [5/10], Batch [10600/15033], Loss: 3.3516\n",
      "Epoch [5/10], Batch [10700/15033], Loss: 2.3347\n",
      "Epoch [5/10], Batch [10800/15033], Loss: 3.1577\n",
      "Epoch [5/10], Batch [10900/15033], Loss: 3.7231\n",
      "Epoch [5/10], Batch [11000/15033], Loss: 2.7985\n",
      "Epoch [5/10], Batch [11100/15033], Loss: 3.4281\n",
      "Epoch [5/10], Batch [11200/15033], Loss: 3.6138\n",
      "Epoch [5/10], Batch [11300/15033], Loss: 2.5867\n",
      "Epoch [5/10], Batch [11400/15033], Loss: 3.5704\n",
      "Epoch [5/10], Batch [11500/15033], Loss: 3.7067\n",
      "Epoch [5/10], Batch [11600/15033], Loss: 2.9442\n",
      "Epoch [5/10], Batch [11700/15033], Loss: 2.3693\n",
      "Epoch [5/10], Batch [11800/15033], Loss: 2.5381\n",
      "Epoch [5/10], Batch [11900/15033], Loss: 3.0014\n",
      "Epoch [5/10], Batch [12000/15033], Loss: 3.7539\n",
      "Epoch [5/10], Batch [12100/15033], Loss: 2.7203\n",
      "Epoch [5/10], Batch [12200/15033], Loss: 2.5434\n",
      "Epoch [5/10], Batch [12300/15033], Loss: 3.0443\n",
      "Epoch [5/10], Batch [12400/15033], Loss: 2.8155\n",
      "Epoch [5/10], Batch [12500/15033], Loss: 3.6457\n",
      "Epoch [5/10], Batch [12600/15033], Loss: 2.6058\n",
      "Epoch [5/10], Batch [12700/15033], Loss: 3.4283\n",
      "Epoch [5/10], Batch [12800/15033], Loss: 1.9598\n",
      "Epoch [5/10], Batch [12900/15033], Loss: 2.4264\n",
      "Epoch [5/10], Batch [13000/15033], Loss: 2.9576\n",
      "Epoch [5/10], Batch [13100/15033], Loss: 3.3690\n",
      "Epoch [5/10], Batch [13200/15033], Loss: 3.0646\n",
      "Epoch [5/10], Batch [13300/15033], Loss: 3.1656\n",
      "Epoch [5/10], Batch [13400/15033], Loss: 3.3921\n",
      "Epoch [5/10], Batch [13500/15033], Loss: 3.0212\n",
      "Epoch [5/10], Batch [13600/15033], Loss: 3.8371\n",
      "Epoch [5/10], Batch [13700/15033], Loss: 4.4361\n",
      "Epoch [5/10], Batch [13800/15033], Loss: 2.8470\n",
      "Epoch [5/10], Batch [13900/15033], Loss: 3.0091\n",
      "Epoch [5/10], Batch [14000/15033], Loss: 3.2735\n",
      "Epoch [5/10], Batch [14100/15033], Loss: 2.3242\n",
      "Epoch [5/10], Batch [14200/15033], Loss: 2.9106\n",
      "Epoch [5/10], Batch [14300/15033], Loss: 2.8012\n",
      "Epoch [5/10], Batch [14400/15033], Loss: 3.5115\n",
      "Epoch [5/10], Batch [14500/15033], Loss: 3.4984\n",
      "Epoch [5/10], Batch [14600/15033], Loss: 3.6278\n",
      "Epoch [5/10], Batch [14700/15033], Loss: 3.0956\n",
      "Epoch [5/10], Batch [14800/15033], Loss: 2.7072\n",
      "Epoch [5/10], Batch [14900/15033], Loss: 3.2767\n",
      "Epoch [5/10], Batch [15000/15033], Loss: 4.3489\n",
      "Epoch [5/10] completed, Average Loss: 3.2578\n",
      "Epoch [6/10], Batch [100/15033], Loss: 4.3277\n",
      "Epoch [6/10], Batch [200/15033], Loss: 2.8960\n",
      "Epoch [6/10], Batch [300/15033], Loss: 3.9521\n",
      "Epoch [6/10], Batch [400/15033], Loss: 3.4498\n",
      "Epoch [6/10], Batch [500/15033], Loss: 3.0710\n",
      "Epoch [6/10], Batch [600/15033], Loss: 2.8935\n",
      "Epoch [6/10], Batch [700/15033], Loss: 3.2377\n",
      "Epoch [6/10], Batch [800/15033], Loss: 2.1567\n",
      "Epoch [6/10], Batch [900/15033], Loss: 3.1293\n",
      "Epoch [6/10], Batch [1000/15033], Loss: 2.1027\n",
      "Epoch [6/10], Batch [1100/15033], Loss: 3.5451\n",
      "Epoch [6/10], Batch [1200/15033], Loss: 2.9069\n",
      "Epoch [6/10], Batch [1300/15033], Loss: 3.8291\n",
      "Epoch [6/10], Batch [1400/15033], Loss: 3.9848\n",
      "Epoch [6/10], Batch [1500/15033], Loss: 2.6432\n",
      "Epoch [6/10], Batch [1600/15033], Loss: 2.5872\n",
      "Epoch [6/10], Batch [1700/15033], Loss: 2.9063\n",
      "Epoch [6/10], Batch [1800/15033], Loss: 3.4830\n",
      "Epoch [6/10], Batch [1900/15033], Loss: 4.1927\n",
      "Epoch [6/10], Batch [2000/15033], Loss: 4.3243\n",
      "Epoch [6/10], Batch [2100/15033], Loss: 4.3664\n",
      "Epoch [6/10], Batch [2200/15033], Loss: 3.0776\n",
      "Epoch [6/10], Batch [2300/15033], Loss: 2.6411\n",
      "Epoch [6/10], Batch [2400/15033], Loss: 3.6069\n",
      "Epoch [6/10], Batch [2500/15033], Loss: 3.0853\n",
      "Epoch [6/10], Batch [2600/15033], Loss: 2.8841\n",
      "Epoch [6/10], Batch [2700/15033], Loss: 3.3118\n",
      "Epoch [6/10], Batch [2800/15033], Loss: 2.4249\n",
      "Epoch [6/10], Batch [2900/15033], Loss: 3.3576\n",
      "Epoch [6/10], Batch [3000/15033], Loss: 3.5026\n",
      "Epoch [6/10], Batch [3100/15033], Loss: 2.9341\n",
      "Epoch [6/10], Batch [3200/15033], Loss: 3.4755\n",
      "Epoch [6/10], Batch [3300/15033], Loss: 3.2380\n",
      "Epoch [6/10], Batch [3400/15033], Loss: 3.7690\n",
      "Epoch [6/10], Batch [3500/15033], Loss: 3.3999\n",
      "Epoch [6/10], Batch [3600/15033], Loss: 3.5179\n",
      "Epoch [6/10], Batch [3700/15033], Loss: 2.8445\n",
      "Epoch [6/10], Batch [3800/15033], Loss: 3.2646\n",
      "Epoch [6/10], Batch [3900/15033], Loss: 4.5811\n",
      "Epoch [6/10], Batch [4000/15033], Loss: 3.8625\n",
      "Epoch [6/10], Batch [4100/15033], Loss: 3.5376\n",
      "Epoch [6/10], Batch [4200/15033], Loss: 3.7895\n",
      "Epoch [6/10], Batch [4300/15033], Loss: 4.0722\n",
      "Epoch [6/10], Batch [4400/15033], Loss: 3.1074\n",
      "Epoch [6/10], Batch [4500/15033], Loss: 2.6812\n",
      "Epoch [6/10], Batch [4600/15033], Loss: 2.5808\n",
      "Epoch [6/10], Batch [4700/15033], Loss: 4.0356\n",
      "Epoch [6/10], Batch [4800/15033], Loss: 2.9673\n",
      "Epoch [6/10], Batch [4900/15033], Loss: 2.6005\n",
      "Epoch [6/10], Batch [5000/15033], Loss: 3.3055\n",
      "Epoch [6/10], Batch [5100/15033], Loss: 2.5812\n",
      "Epoch [6/10], Batch [5200/15033], Loss: 3.4866\n",
      "Epoch [6/10], Batch [5300/15033], Loss: 4.2351\n",
      "Epoch [6/10], Batch [5400/15033], Loss: 2.8351\n",
      "Epoch [6/10], Batch [5500/15033], Loss: 3.9414\n",
      "Epoch [6/10], Batch [5600/15033], Loss: 2.8828\n",
      "Epoch [6/10], Batch [5700/15033], Loss: 3.9476\n",
      "Epoch [6/10], Batch [5800/15033], Loss: 3.3920\n",
      "Epoch [6/10], Batch [5900/15033], Loss: 2.6857\n",
      "Epoch [6/10], Batch [6000/15033], Loss: 4.2108\n",
      "Epoch [6/10], Batch [6100/15033], Loss: 3.6998\n",
      "Epoch [6/10], Batch [6200/15033], Loss: 2.8193\n",
      "Epoch [6/10], Batch [6300/15033], Loss: 2.8516\n",
      "Epoch [6/10], Batch [6400/15033], Loss: 3.0796\n",
      "Epoch [6/10], Batch [6500/15033], Loss: 2.9516\n",
      "Epoch [6/10], Batch [6600/15033], Loss: 3.1568\n",
      "Epoch [6/10], Batch [6700/15033], Loss: 2.3190\n",
      "Epoch [6/10], Batch [6800/15033], Loss: 4.1313\n",
      "Epoch [6/10], Batch [6900/15033], Loss: 2.4580\n",
      "Epoch [6/10], Batch [7000/15033], Loss: 3.0570\n",
      "Epoch [6/10], Batch [7100/15033], Loss: 2.8981\n",
      "Epoch [6/10], Batch [7200/15033], Loss: 2.8008\n",
      "Epoch [6/10], Batch [7300/15033], Loss: 3.0293\n",
      "Epoch [6/10], Batch [7400/15033], Loss: 3.1761\n",
      "Epoch [6/10], Batch [7500/15033], Loss: 2.4230\n",
      "Epoch [6/10], Batch [7600/15033], Loss: 2.9761\n",
      "Epoch [6/10], Batch [7700/15033], Loss: 4.2258\n",
      "Epoch [6/10], Batch [7800/15033], Loss: 4.2740\n",
      "Epoch [6/10], Batch [7900/15033], Loss: 2.9631\n",
      "Epoch [6/10], Batch [8000/15033], Loss: 2.9134\n",
      "Epoch [6/10], Batch [8100/15033], Loss: 2.1745\n",
      "Epoch [6/10], Batch [8200/15033], Loss: 3.3426\n",
      "Epoch [6/10], Batch [8300/15033], Loss: 3.6131\n",
      "Epoch [6/10], Batch [8400/15033], Loss: 2.1237\n",
      "Epoch [6/10], Batch [8500/15033], Loss: 2.7251\n",
      "Epoch [6/10], Batch [8600/15033], Loss: 3.0782\n",
      "Epoch [6/10], Batch [8700/15033], Loss: 2.5469\n",
      "Epoch [6/10], Batch [8800/15033], Loss: 2.7729\n",
      "Epoch [6/10], Batch [8900/15033], Loss: 2.9147\n",
      "Epoch [6/10], Batch [9000/15033], Loss: 2.9285\n",
      "Epoch [6/10], Batch [9100/15033], Loss: 3.3779\n",
      "Epoch [6/10], Batch [9200/15033], Loss: 4.5690\n",
      "Epoch [6/10], Batch [9300/15033], Loss: 2.7216\n",
      "Epoch [6/10], Batch [9400/15033], Loss: 3.0557\n",
      "Epoch [6/10], Batch [9500/15033], Loss: 2.4147\n",
      "Epoch [6/10], Batch [9600/15033], Loss: 3.8147\n",
      "Epoch [6/10], Batch [9700/15033], Loss: 3.1487\n",
      "Epoch [6/10], Batch [9800/15033], Loss: 2.9263\n",
      "Epoch [6/10], Batch [9900/15033], Loss: 3.1149\n",
      "Epoch [6/10], Batch [10000/15033], Loss: 2.6749\n",
      "Epoch [6/10], Batch [10100/15033], Loss: 3.5746\n",
      "Epoch [6/10], Batch [10200/15033], Loss: 2.1829\n",
      "Epoch [6/10], Batch [10300/15033], Loss: 3.4578\n",
      "Epoch [6/10], Batch [10400/15033], Loss: 2.9391\n",
      "Epoch [6/10], Batch [10500/15033], Loss: 2.4888\n",
      "Epoch [6/10], Batch [10600/15033], Loss: 3.8694\n",
      "Epoch [6/10], Batch [10700/15033], Loss: 4.1518\n",
      "Epoch [6/10], Batch [10800/15033], Loss: 2.7348\n",
      "Epoch [6/10], Batch [10900/15033], Loss: 3.6512\n",
      "Epoch [6/10], Batch [11000/15033], Loss: 2.8774\n",
      "Epoch [6/10], Batch [11100/15033], Loss: 3.4214\n",
      "Epoch [6/10], Batch [11200/15033], Loss: 2.4570\n",
      "Epoch [6/10], Batch [11300/15033], Loss: 3.7573\n",
      "Epoch [6/10], Batch [11400/15033], Loss: 3.9553\n",
      "Epoch [6/10], Batch [11500/15033], Loss: 3.2480\n",
      "Epoch [6/10], Batch [11600/15033], Loss: 4.2516\n",
      "Epoch [6/10], Batch [11700/15033], Loss: 2.2518\n",
      "Epoch [6/10], Batch [11800/15033], Loss: 3.4601\n",
      "Epoch [6/10], Batch [11900/15033], Loss: 3.1855\n",
      "Epoch [6/10], Batch [12000/15033], Loss: 4.5591\n",
      "Epoch [6/10], Batch [12100/15033], Loss: 2.6151\n",
      "Epoch [6/10], Batch [12200/15033], Loss: 3.1409\n",
      "Epoch [6/10], Batch [12300/15033], Loss: 2.3346\n",
      "Epoch [6/10], Batch [12400/15033], Loss: 2.4384\n",
      "Epoch [6/10], Batch [12500/15033], Loss: 3.0214\n",
      "Epoch [6/10], Batch [12600/15033], Loss: 3.5264\n",
      "Epoch [6/10], Batch [12700/15033], Loss: 3.4322\n",
      "Epoch [6/10], Batch [12800/15033], Loss: 2.9999\n",
      "Epoch [6/10], Batch [12900/15033], Loss: 3.4625\n",
      "Epoch [6/10], Batch [13000/15033], Loss: 3.8052\n",
      "Epoch [6/10], Batch [13100/15033], Loss: 2.7111\n",
      "Epoch [6/10], Batch [13200/15033], Loss: 4.2857\n",
      "Epoch [6/10], Batch [13300/15033], Loss: 2.7908\n",
      "Epoch [6/10], Batch [13400/15033], Loss: 2.8380\n",
      "Epoch [6/10], Batch [13500/15033], Loss: 3.2291\n",
      "Epoch [6/10], Batch [13600/15033], Loss: 2.8958\n",
      "Epoch [6/10], Batch [13700/15033], Loss: 3.5846\n",
      "Epoch [6/10], Batch [13800/15033], Loss: 2.9685\n",
      "Epoch [6/10], Batch [13900/15033], Loss: 2.8582\n",
      "Epoch [6/10], Batch [14000/15033], Loss: 3.5348\n",
      "Epoch [6/10], Batch [14100/15033], Loss: 3.1061\n",
      "Epoch [6/10], Batch [14200/15033], Loss: 2.6652\n",
      "Epoch [6/10], Batch [14300/15033], Loss: 3.4561\n",
      "Epoch [6/10], Batch [14400/15033], Loss: 2.5459\n",
      "Epoch [6/10], Batch [14500/15033], Loss: 4.3755\n",
      "Epoch [6/10], Batch [14600/15033], Loss: 3.3172\n",
      "Epoch [6/10], Batch [14700/15033], Loss: 3.7677\n",
      "Epoch [6/10], Batch [14800/15033], Loss: 3.7050\n",
      "Epoch [6/10], Batch [14900/15033], Loss: 3.0800\n",
      "Epoch [6/10], Batch [15000/15033], Loss: 2.1111\n",
      "Epoch [6/10] completed, Average Loss: 3.2406\n",
      "Epoch [7/10], Batch [100/15033], Loss: 3.5731\n",
      "Epoch [7/10], Batch [200/15033], Loss: 3.8662\n",
      "Epoch [7/10], Batch [300/15033], Loss: 2.8330\n",
      "Epoch [7/10], Batch [400/15033], Loss: 3.2237\n",
      "Epoch [7/10], Batch [500/15033], Loss: 2.5900\n",
      "Epoch [7/10], Batch [600/15033], Loss: 3.6043\n",
      "Epoch [7/10], Batch [700/15033], Loss: 3.3194\n",
      "Epoch [7/10], Batch [800/15033], Loss: 3.0074\n",
      "Epoch [7/10], Batch [900/15033], Loss: 3.3926\n",
      "Epoch [7/10], Batch [1000/15033], Loss: 4.1252\n",
      "Epoch [7/10], Batch [1100/15033], Loss: 3.3637\n",
      "Epoch [7/10], Batch [1200/15033], Loss: 3.9136\n",
      "Epoch [7/10], Batch [1300/15033], Loss: 3.1631\n",
      "Epoch [7/10], Batch [1400/15033], Loss: 3.0144\n",
      "Epoch [7/10], Batch [1500/15033], Loss: 3.2022\n",
      "Epoch [7/10], Batch [1600/15033], Loss: 2.4075\n",
      "Epoch [7/10], Batch [1700/15033], Loss: 2.9550\n",
      "Epoch [7/10], Batch [1800/15033], Loss: 3.4461\n",
      "Epoch [7/10], Batch [1900/15033], Loss: 2.8337\n",
      "Epoch [7/10], Batch [2000/15033], Loss: 3.0996\n",
      "Epoch [7/10], Batch [2100/15033], Loss: 3.2136\n",
      "Epoch [7/10], Batch [2200/15033], Loss: 2.7918\n",
      "Epoch [7/10], Batch [2300/15033], Loss: 2.7315\n",
      "Epoch [7/10], Batch [2400/15033], Loss: 2.4556\n",
      "Epoch [7/10], Batch [2500/15033], Loss: 3.4150\n",
      "Epoch [7/10], Batch [2600/15033], Loss: 3.0072\n",
      "Epoch [7/10], Batch [2700/15033], Loss: 4.2728\n",
      "Epoch [7/10], Batch [2800/15033], Loss: 3.9266\n",
      "Epoch [7/10], Batch [2900/15033], Loss: 2.5439\n",
      "Epoch [7/10], Batch [3000/15033], Loss: 3.9756\n",
      "Epoch [7/10], Batch [3100/15033], Loss: 3.7346\n",
      "Epoch [7/10], Batch [3200/15033], Loss: 2.3445\n",
      "Epoch [7/10], Batch [3300/15033], Loss: 4.3549\n",
      "Epoch [7/10], Batch [3400/15033], Loss: 2.9348\n",
      "Epoch [7/10], Batch [3500/15033], Loss: 3.5975\n",
      "Epoch [7/10], Batch [3600/15033], Loss: 5.4662\n",
      "Epoch [7/10], Batch [3700/15033], Loss: 2.8184\n",
      "Epoch [7/10], Batch [3800/15033], Loss: 3.3428\n",
      "Epoch [7/10], Batch [3900/15033], Loss: 3.1923\n",
      "Epoch [7/10], Batch [4000/15033], Loss: 2.8433\n",
      "Epoch [7/10], Batch [4100/15033], Loss: 4.0361\n",
      "Epoch [7/10], Batch [4200/15033], Loss: 4.4682\n",
      "Epoch [7/10], Batch [4300/15033], Loss: 3.9519\n",
      "Epoch [7/10], Batch [4400/15033], Loss: 2.8279\n",
      "Epoch [7/10], Batch [4500/15033], Loss: 2.3531\n",
      "Epoch [7/10], Batch [4600/15033], Loss: 2.7809\n",
      "Epoch [7/10], Batch [4700/15033], Loss: 2.8947\n",
      "Epoch [7/10], Batch [4800/15033], Loss: 2.5592\n",
      "Epoch [7/10], Batch [4900/15033], Loss: 3.6782\n",
      "Epoch [7/10], Batch [5000/15033], Loss: 3.7364\n",
      "Epoch [7/10], Batch [5100/15033], Loss: 3.1214\n",
      "Epoch [7/10], Batch [5200/15033], Loss: 4.7592\n",
      "Epoch [7/10], Batch [5300/15033], Loss: 3.1824\n",
      "Epoch [7/10], Batch [5400/15033], Loss: 3.0844\n",
      "Epoch [7/10], Batch [5500/15033], Loss: 3.0821\n",
      "Epoch [7/10], Batch [5600/15033], Loss: 2.9227\n",
      "Epoch [7/10], Batch [5700/15033], Loss: 4.3880\n",
      "Epoch [7/10], Batch [5800/15033], Loss: 3.4464\n",
      "Epoch [7/10], Batch [5900/15033], Loss: 3.5697\n",
      "Epoch [7/10], Batch [6000/15033], Loss: 2.9381\n",
      "Epoch [7/10], Batch [6100/15033], Loss: 2.7696\n",
      "Epoch [7/10], Batch [6200/15033], Loss: 2.9572\n",
      "Epoch [7/10], Batch [6300/15033], Loss: 2.8225\n",
      "Epoch [7/10], Batch [6400/15033], Loss: 4.0187\n",
      "Epoch [7/10], Batch [6500/15033], Loss: 3.0763\n",
      "Epoch [7/10], Batch [6600/15033], Loss: 2.9166\n",
      "Epoch [7/10], Batch [6700/15033], Loss: 3.3036\n",
      "Epoch [7/10], Batch [6800/15033], Loss: 3.4520\n",
      "Epoch [7/10], Batch [6900/15033], Loss: 2.3479\n",
      "Epoch [7/10], Batch [7000/15033], Loss: 2.7636\n",
      "Epoch [7/10], Batch [7100/15033], Loss: 3.1994\n",
      "Epoch [7/10], Batch [7200/15033], Loss: 3.6332\n",
      "Epoch [7/10], Batch [7300/15033], Loss: 4.2661\n",
      "Epoch [7/10], Batch [7400/15033], Loss: 2.4523\n",
      "Epoch [7/10], Batch [7500/15033], Loss: 2.6886\n",
      "Epoch [7/10], Batch [7600/15033], Loss: 2.7392\n",
      "Epoch [7/10], Batch [7700/15033], Loss: 3.0816\n",
      "Epoch [7/10], Batch [7800/15033], Loss: 3.0416\n",
      "Epoch [7/10], Batch [7900/15033], Loss: 3.4129\n",
      "Epoch [7/10], Batch [8000/15033], Loss: 3.2373\n",
      "Epoch [7/10], Batch [8100/15033], Loss: 3.0943\n",
      "Epoch [7/10], Batch [8200/15033], Loss: 2.5576\n",
      "Epoch [7/10], Batch [8300/15033], Loss: 3.4974\n",
      "Epoch [7/10], Batch [8400/15033], Loss: 3.3467\n",
      "Epoch [7/10], Batch [8500/15033], Loss: 3.7077\n",
      "Epoch [7/10], Batch [8600/15033], Loss: 4.2607\n",
      "Epoch [7/10], Batch [8700/15033], Loss: 3.0848\n",
      "Epoch [7/10], Batch [8800/15033], Loss: 2.5887\n",
      "Epoch [7/10], Batch [8900/15033], Loss: 2.8659\n",
      "Epoch [7/10], Batch [9000/15033], Loss: 3.2005\n",
      "Epoch [7/10], Batch [9100/15033], Loss: 3.5384\n",
      "Epoch [7/10], Batch [9200/15033], Loss: 3.5868\n",
      "Epoch [7/10], Batch [9300/15033], Loss: 3.2066\n",
      "Epoch [7/10], Batch [9400/15033], Loss: 3.5468\n",
      "Epoch [7/10], Batch [9500/15033], Loss: 4.5936\n",
      "Epoch [7/10], Batch [9600/15033], Loss: 2.2344\n",
      "Epoch [7/10], Batch [9700/15033], Loss: 2.9343\n",
      "Epoch [7/10], Batch [9800/15033], Loss: 2.7861\n",
      "Epoch [7/10], Batch [9900/15033], Loss: 3.0317\n",
      "Epoch [7/10], Batch [10000/15033], Loss: 2.7669\n",
      "Epoch [7/10], Batch [10100/15033], Loss: 3.0499\n",
      "Epoch [7/10], Batch [10200/15033], Loss: 4.0055\n",
      "Epoch [7/10], Batch [10300/15033], Loss: 4.3314\n",
      "Epoch [7/10], Batch [10400/15033], Loss: 3.0884\n",
      "Epoch [7/10], Batch [10500/15033], Loss: 4.1815\n",
      "Epoch [7/10], Batch [10600/15033], Loss: 3.1651\n",
      "Epoch [7/10], Batch [10700/15033], Loss: 2.3378\n",
      "Epoch [7/10], Batch [10800/15033], Loss: 2.3482\n",
      "Epoch [7/10], Batch [10900/15033], Loss: 2.4497\n",
      "Epoch [7/10], Batch [11000/15033], Loss: 3.0098\n",
      "Epoch [7/10], Batch [11100/15033], Loss: 3.0691\n",
      "Epoch [7/10], Batch [11200/15033], Loss: 2.5938\n",
      "Epoch [7/10], Batch [11300/15033], Loss: 3.4687\n",
      "Epoch [7/10], Batch [11400/15033], Loss: 2.9659\n",
      "Epoch [7/10], Batch [11500/15033], Loss: 3.2762\n",
      "Epoch [7/10], Batch [11600/15033], Loss: 3.9350\n",
      "Epoch [7/10], Batch [11700/15033], Loss: 2.5299\n",
      "Epoch [7/10], Batch [11800/15033], Loss: 3.4786\n",
      "Epoch [7/10], Batch [11900/15033], Loss: 3.1187\n",
      "Epoch [7/10], Batch [12000/15033], Loss: 3.1214\n",
      "Epoch [7/10], Batch [12100/15033], Loss: 3.2584\n",
      "Epoch [7/10], Batch [12200/15033], Loss: 2.5861\n",
      "Epoch [7/10], Batch [12300/15033], Loss: 4.2861\n",
      "Epoch [7/10], Batch [12400/15033], Loss: 2.7150\n",
      "Epoch [7/10], Batch [12500/15033], Loss: 3.0464\n",
      "Epoch [7/10], Batch [12600/15033], Loss: 2.6743\n",
      "Epoch [7/10], Batch [12700/15033], Loss: 3.6785\n",
      "Epoch [7/10], Batch [12800/15033], Loss: 4.0671\n",
      "Epoch [7/10], Batch [12900/15033], Loss: 2.9180\n",
      "Epoch [7/10], Batch [13000/15033], Loss: 3.4285\n",
      "Epoch [7/10], Batch [13100/15033], Loss: 3.9319\n",
      "Epoch [7/10], Batch [13200/15033], Loss: 2.8635\n",
      "Epoch [7/10], Batch [13300/15033], Loss: 3.9566\n",
      "Epoch [7/10], Batch [13400/15033], Loss: 2.2757\n",
      "Epoch [7/10], Batch [13500/15033], Loss: 4.8665\n",
      "Epoch [7/10], Batch [13600/15033], Loss: 2.4953\n",
      "Epoch [7/10], Batch [13700/15033], Loss: 3.9603\n",
      "Epoch [7/10], Batch [13800/15033], Loss: 2.6218\n",
      "Epoch [7/10], Batch [13900/15033], Loss: 2.3048\n",
      "Epoch [7/10], Batch [14000/15033], Loss: 3.1255\n",
      "Epoch [7/10], Batch [14100/15033], Loss: 2.6336\n",
      "Epoch [7/10], Batch [14200/15033], Loss: 3.2272\n",
      "Epoch [7/10], Batch [14300/15033], Loss: 2.9515\n",
      "Epoch [7/10], Batch [14400/15033], Loss: 3.3593\n",
      "Epoch [7/10], Batch [14500/15033], Loss: 3.2611\n",
      "Epoch [7/10], Batch [14600/15033], Loss: 3.7763\n",
      "Epoch [7/10], Batch [14700/15033], Loss: 3.2564\n",
      "Epoch [7/10], Batch [14800/15033], Loss: 3.5773\n",
      "Epoch [7/10], Batch [14900/15033], Loss: 2.6539\n",
      "Epoch [7/10], Batch [15000/15033], Loss: 4.8345\n",
      "Epoch [7/10] completed, Average Loss: 3.2158\n",
      "Epoch [8/10], Batch [100/15033], Loss: 3.4818\n",
      "Epoch [8/10], Batch [200/15033], Loss: 3.8878\n",
      "Epoch [8/10], Batch [300/15033], Loss: 2.4474\n",
      "Epoch [8/10], Batch [400/15033], Loss: 3.8823\n",
      "Epoch [8/10], Batch [500/15033], Loss: 3.8001\n",
      "Epoch [8/10], Batch [600/15033], Loss: 2.8952\n",
      "Epoch [8/10], Batch [700/15033], Loss: 3.8677\n",
      "Epoch [8/10], Batch [800/15033], Loss: 2.7762\n",
      "Epoch [8/10], Batch [900/15033], Loss: 2.8715\n",
      "Epoch [8/10], Batch [1000/15033], Loss: 5.0450\n",
      "Epoch [8/10], Batch [1100/15033], Loss: 2.6447\n",
      "Epoch [8/10], Batch [1200/15033], Loss: 3.5807\n",
      "Epoch [8/10], Batch [1300/15033], Loss: 2.8051\n",
      "Epoch [8/10], Batch [1400/15033], Loss: 2.7979\n",
      "Epoch [8/10], Batch [1500/15033], Loss: 2.1828\n",
      "Epoch [8/10], Batch [1600/15033], Loss: 3.6862\n",
      "Epoch [8/10], Batch [1700/15033], Loss: 3.0385\n",
      "Epoch [8/10], Batch [1800/15033], Loss: 2.5311\n",
      "Epoch [8/10], Batch [1900/15033], Loss: 4.7570\n",
      "Epoch [8/10], Batch [2000/15033], Loss: 3.8972\n",
      "Epoch [8/10], Batch [2100/15033], Loss: 3.3179\n",
      "Epoch [8/10], Batch [2200/15033], Loss: 2.7938\n",
      "Epoch [8/10], Batch [2300/15033], Loss: 3.6208\n",
      "Epoch [8/10], Batch [2400/15033], Loss: 2.8145\n",
      "Epoch [8/10], Batch [2500/15033], Loss: 2.5532\n",
      "Epoch [8/10], Batch [2600/15033], Loss: 3.5120\n",
      "Epoch [8/10], Batch [2700/15033], Loss: 3.6730\n",
      "Epoch [8/10], Batch [2800/15033], Loss: 3.0793\n",
      "Epoch [8/10], Batch [2900/15033], Loss: 2.9932\n",
      "Epoch [8/10], Batch [3000/15033], Loss: 2.7773\n",
      "Epoch [8/10], Batch [3100/15033], Loss: 2.9649\n",
      "Epoch [8/10], Batch [3200/15033], Loss: 2.8491\n",
      "Epoch [8/10], Batch [3300/15033], Loss: 2.9153\n",
      "Epoch [8/10], Batch [3400/15033], Loss: 3.2768\n",
      "Epoch [8/10], Batch [3500/15033], Loss: 2.4408\n",
      "Epoch [8/10], Batch [3600/15033], Loss: 2.9478\n",
      "Epoch [8/10], Batch [3700/15033], Loss: 4.4451\n",
      "Epoch [8/10], Batch [3800/15033], Loss: 3.1256\n",
      "Epoch [8/10], Batch [3900/15033], Loss: 2.2874\n",
      "Epoch [8/10], Batch [4000/15033], Loss: 2.2932\n",
      "Epoch [8/10], Batch [4100/15033], Loss: 2.2435\n",
      "Epoch [8/10], Batch [4200/15033], Loss: 2.9014\n",
      "Epoch [8/10], Batch [4300/15033], Loss: 3.1219\n",
      "Epoch [8/10], Batch [4400/15033], Loss: 3.6579\n",
      "Epoch [8/10], Batch [4500/15033], Loss: 2.7403\n",
      "Epoch [8/10], Batch [4600/15033], Loss: 2.9568\n",
      "Epoch [8/10], Batch [4700/15033], Loss: 2.9977\n",
      "Epoch [8/10], Batch [4800/15033], Loss: 4.9954\n",
      "Epoch [8/10], Batch [4900/15033], Loss: 2.0959\n",
      "Epoch [8/10], Batch [5000/15033], Loss: 3.6264\n",
      "Epoch [8/10], Batch [5100/15033], Loss: 3.4144\n",
      "Epoch [8/10], Batch [5200/15033], Loss: 4.1268\n",
      "Epoch [8/10], Batch [5300/15033], Loss: 2.9345\n",
      "Epoch [8/10], Batch [5400/15033], Loss: 5.0591\n",
      "Epoch [8/10], Batch [5500/15033], Loss: 3.3750\n",
      "Epoch [8/10], Batch [5600/15033], Loss: 1.7833\n",
      "Epoch [8/10], Batch [5700/15033], Loss: 2.3812\n",
      "Epoch [8/10], Batch [5800/15033], Loss: 3.2209\n",
      "Epoch [8/10], Batch [5900/15033], Loss: 2.7545\n",
      "Epoch [8/10], Batch [6000/15033], Loss: 4.4608\n",
      "Epoch [8/10], Batch [6100/15033], Loss: 3.8764\n",
      "Epoch [8/10], Batch [6200/15033], Loss: 1.9972\n",
      "Epoch [8/10], Batch [6300/15033], Loss: 2.9312\n",
      "Epoch [8/10], Batch [6400/15033], Loss: 2.8720\n",
      "Epoch [8/10], Batch [6500/15033], Loss: 3.9008\n",
      "Epoch [8/10], Batch [6600/15033], Loss: 3.0690\n",
      "Epoch [8/10], Batch [6700/15033], Loss: 2.5002\n",
      "Epoch [8/10], Batch [6800/15033], Loss: 3.0081\n",
      "Epoch [8/10], Batch [6900/15033], Loss: 3.0488\n",
      "Epoch [8/10], Batch [7000/15033], Loss: 2.3972\n",
      "Epoch [8/10], Batch [7100/15033], Loss: 3.0482\n",
      "Epoch [8/10], Batch [7200/15033], Loss: 4.2884\n",
      "Epoch [8/10], Batch [7300/15033], Loss: 2.9784\n",
      "Epoch [8/10], Batch [7400/15033], Loss: 3.2068\n",
      "Epoch [8/10], Batch [7500/15033], Loss: 2.8747\n",
      "Epoch [8/10], Batch [7600/15033], Loss: 2.4172\n",
      "Epoch [8/10], Batch [7700/15033], Loss: 2.5816\n",
      "Epoch [8/10], Batch [7800/15033], Loss: 2.8905\n",
      "Epoch [8/10], Batch [7900/15033], Loss: 3.3374\n",
      "Epoch [8/10], Batch [8000/15033], Loss: 2.9447\n",
      "Epoch [8/10], Batch [8100/15033], Loss: 3.1259\n",
      "Epoch [8/10], Batch [8200/15033], Loss: 2.2922\n",
      "Epoch [8/10], Batch [8300/15033], Loss: 3.0275\n",
      "Epoch [8/10], Batch [8400/15033], Loss: 2.9236\n",
      "Epoch [8/10], Batch [8500/15033], Loss: 3.7093\n",
      "Epoch [8/10], Batch [8600/15033], Loss: 2.0199\n",
      "Epoch [8/10], Batch [8700/15033], Loss: 2.9103\n",
      "Epoch [8/10], Batch [8800/15033], Loss: 4.2482\n",
      "Epoch [8/10], Batch [8900/15033], Loss: 4.4416\n",
      "Epoch [8/10], Batch [9000/15033], Loss: 2.7457\n",
      "Epoch [8/10], Batch [9100/15033], Loss: 2.6839\n",
      "Epoch [8/10], Batch [9200/15033], Loss: 2.4913\n",
      "Epoch [8/10], Batch [9300/15033], Loss: 2.1094\n",
      "Epoch [8/10], Batch [9400/15033], Loss: 2.4901\n",
      "Epoch [8/10], Batch [9500/15033], Loss: 3.3177\n",
      "Epoch [8/10], Batch [9600/15033], Loss: 3.1242\n",
      "Epoch [8/10], Batch [9700/15033], Loss: 3.8208\n",
      "Epoch [8/10], Batch [9800/15033], Loss: 2.8818\n",
      "Epoch [8/10], Batch [9900/15033], Loss: 2.9236\n",
      "Epoch [8/10], Batch [10000/15033], Loss: 3.2520\n",
      "Epoch [8/10], Batch [10100/15033], Loss: 3.2716\n",
      "Epoch [8/10], Batch [10200/15033], Loss: 3.5792\n",
      "Epoch [8/10], Batch [10300/15033], Loss: 4.9696\n",
      "Epoch [8/10], Batch [10400/15033], Loss: 2.3521\n",
      "Epoch [8/10], Batch [10500/15033], Loss: 3.3236\n",
      "Epoch [8/10], Batch [10600/15033], Loss: 3.9583\n",
      "Epoch [8/10], Batch [10700/15033], Loss: 3.4814\n",
      "Epoch [8/10], Batch [10800/15033], Loss: 3.1761\n",
      "Epoch [8/10], Batch [10900/15033], Loss: 2.9627\n",
      "Epoch [8/10], Batch [11000/15033], Loss: 3.1926\n",
      "Epoch [8/10], Batch [11100/15033], Loss: 3.3975\n",
      "Epoch [8/10], Batch [11200/15033], Loss: 3.4983\n",
      "Epoch [8/10], Batch [11300/15033], Loss: 3.5941\n",
      "Epoch [8/10], Batch [11400/15033], Loss: 2.8727\n",
      "Epoch [8/10], Batch [11500/15033], Loss: 4.6196\n",
      "Epoch [8/10], Batch [11600/15033], Loss: 3.2841\n",
      "Epoch [8/10], Batch [11700/15033], Loss: 3.2686\n",
      "Epoch [8/10], Batch [11800/15033], Loss: 4.1742\n",
      "Epoch [8/10], Batch [11900/15033], Loss: 4.6031\n",
      "Epoch [8/10], Batch [12000/15033], Loss: 3.1121\n",
      "Epoch [8/10], Batch [12100/15033], Loss: 2.7068\n",
      "Epoch [8/10], Batch [12200/15033], Loss: 3.4007\n",
      "Epoch [8/10], Batch [12300/15033], Loss: 2.9601\n",
      "Epoch [8/10], Batch [12400/15033], Loss: 2.8926\n",
      "Epoch [8/10], Batch [12500/15033], Loss: 2.2635\n",
      "Epoch [8/10], Batch [12600/15033], Loss: 2.6064\n",
      "Epoch [8/10], Batch [12700/15033], Loss: 1.6078\n",
      "Epoch [8/10], Batch [12800/15033], Loss: 2.8861\n",
      "Epoch [8/10], Batch [12900/15033], Loss: 2.8525\n",
      "Epoch [8/10], Batch [13000/15033], Loss: 2.9798\n",
      "Epoch [8/10], Batch [13100/15033], Loss: 3.4648\n",
      "Epoch [8/10], Batch [13200/15033], Loss: 3.5228\n",
      "Epoch [8/10], Batch [13300/15033], Loss: 3.2254\n",
      "Epoch [8/10], Batch [13400/15033], Loss: 3.4622\n",
      "Epoch [8/10], Batch [13500/15033], Loss: 3.4768\n",
      "Epoch [8/10], Batch [13600/15033], Loss: 3.5563\n",
      "Epoch [8/10], Batch [13700/15033], Loss: 3.1276\n",
      "Epoch [8/10], Batch [13800/15033], Loss: 2.1949\n",
      "Epoch [8/10], Batch [13900/15033], Loss: 2.8218\n",
      "Epoch [8/10], Batch [14000/15033], Loss: 3.3505\n",
      "Epoch [8/10], Batch [14100/15033], Loss: 4.4185\n",
      "Epoch [8/10], Batch [14200/15033], Loss: 3.7389\n",
      "Epoch [8/10], Batch [14300/15033], Loss: 3.1015\n",
      "Epoch [8/10], Batch [14400/15033], Loss: 2.8324\n",
      "Epoch [8/10], Batch [14500/15033], Loss: 2.5674\n",
      "Epoch [8/10], Batch [14600/15033], Loss: 2.6831\n",
      "Epoch [8/10], Batch [14700/15033], Loss: 3.6182\n",
      "Epoch [8/10], Batch [14800/15033], Loss: 3.2997\n",
      "Epoch [8/10], Batch [14900/15033], Loss: 2.2983\n",
      "Epoch [8/10], Batch [15000/15033], Loss: 2.9992\n",
      "Epoch [8/10] completed, Average Loss: 3.1876\n",
      "Epoch [9/10], Batch [100/15033], Loss: 3.5548\n",
      "Epoch [9/10], Batch [200/15033], Loss: 2.4662\n",
      "Epoch [9/10], Batch [300/15033], Loss: 2.8178\n",
      "Epoch [9/10], Batch [400/15033], Loss: 3.9326\n",
      "Epoch [9/10], Batch [500/15033], Loss: 5.7322\n",
      "Epoch [9/10], Batch [600/15033], Loss: 3.7940\n",
      "Epoch [9/10], Batch [700/15033], Loss: 2.9310\n",
      "Epoch [9/10], Batch [800/15033], Loss: 3.2445\n",
      "Epoch [9/10], Batch [900/15033], Loss: 2.5705\n",
      "Epoch [9/10], Batch [1000/15033], Loss: 3.1511\n",
      "Epoch [9/10], Batch [1100/15033], Loss: 2.5161\n",
      "Epoch [9/10], Batch [1200/15033], Loss: 3.3942\n",
      "Epoch [9/10], Batch [1300/15033], Loss: 3.5298\n",
      "Epoch [9/10], Batch [1400/15033], Loss: 3.0074\n",
      "Epoch [9/10], Batch [1500/15033], Loss: 2.7031\n",
      "Epoch [9/10], Batch [1600/15033], Loss: 4.3379\n",
      "Epoch [9/10], Batch [1700/15033], Loss: 2.9608\n",
      "Epoch [9/10], Batch [1800/15033], Loss: 2.2071\n",
      "Epoch [9/10], Batch [1900/15033], Loss: 2.5901\n",
      "Epoch [9/10], Batch [2000/15033], Loss: 3.2375\n",
      "Epoch [9/10], Batch [2100/15033], Loss: 3.3071\n",
      "Epoch [9/10], Batch [2200/15033], Loss: 3.0153\n",
      "Epoch [9/10], Batch [2300/15033], Loss: 3.8358\n",
      "Epoch [9/10], Batch [2400/15033], Loss: 3.1359\n",
      "Epoch [9/10], Batch [2500/15033], Loss: 3.9786\n",
      "Epoch [9/10], Batch [2600/15033], Loss: 3.2655\n",
      "Epoch [9/10], Batch [2700/15033], Loss: 2.5832\n",
      "Epoch [9/10], Batch [2800/15033], Loss: 4.2694\n",
      "Epoch [9/10], Batch [2900/15033], Loss: 3.5011\n",
      "Epoch [9/10], Batch [3000/15033], Loss: 3.0084\n",
      "Epoch [9/10], Batch [3100/15033], Loss: 3.0729\n",
      "Epoch [9/10], Batch [3200/15033], Loss: 2.9034\n",
      "Epoch [9/10], Batch [3300/15033], Loss: 3.2786\n",
      "Epoch [9/10], Batch [3400/15033], Loss: 3.0789\n",
      "Epoch [9/10], Batch [3500/15033], Loss: 3.9965\n",
      "Epoch [9/10], Batch [3600/15033], Loss: 2.9559\n",
      "Epoch [9/10], Batch [3700/15033], Loss: 2.6464\n",
      "Epoch [9/10], Batch [3800/15033], Loss: 3.2027\n",
      "Epoch [9/10], Batch [3900/15033], Loss: 3.3699\n",
      "Epoch [9/10], Batch [4000/15033], Loss: 3.4672\n",
      "Epoch [9/10], Batch [4100/15033], Loss: 2.7945\n",
      "Epoch [9/10], Batch [4200/15033], Loss: 2.3494\n",
      "Epoch [9/10], Batch [4300/15033], Loss: 2.2672\n",
      "Epoch [9/10], Batch [4400/15033], Loss: 3.0034\n",
      "Epoch [9/10], Batch [4500/15033], Loss: 3.0235\n",
      "Epoch [9/10], Batch [4600/15033], Loss: 3.1578\n",
      "Epoch [9/10], Batch [4700/15033], Loss: 2.3316\n",
      "Epoch [9/10], Batch [4800/15033], Loss: 2.9909\n",
      "Epoch [9/10], Batch [4900/15033], Loss: 3.2091\n",
      "Epoch [9/10], Batch [5000/15033], Loss: 2.8511\n",
      "Epoch [9/10], Batch [5100/15033], Loss: 3.1772\n",
      "Epoch [9/10], Batch [5200/15033], Loss: 2.5805\n",
      "Epoch [9/10], Batch [5300/15033], Loss: 2.6208\n",
      "Epoch [9/10], Batch [5400/15033], Loss: 2.6365\n",
      "Epoch [9/10], Batch [5500/15033], Loss: 2.8107\n",
      "Epoch [9/10], Batch [5600/15033], Loss: 2.7844\n",
      "Epoch [9/10], Batch [5700/15033], Loss: 2.4560\n",
      "Epoch [9/10], Batch [5800/15033], Loss: 3.7861\n",
      "Epoch [9/10], Batch [5900/15033], Loss: 3.8548\n",
      "Epoch [9/10], Batch [6000/15033], Loss: 3.9044\n",
      "Epoch [9/10], Batch [6100/15033], Loss: 2.2417\n",
      "Epoch [9/10], Batch [6200/15033], Loss: 2.9209\n",
      "Epoch [9/10], Batch [6300/15033], Loss: 2.2077\n",
      "Epoch [9/10], Batch [6400/15033], Loss: 3.8089\n",
      "Epoch [9/10], Batch [6500/15033], Loss: 3.7092\n",
      "Epoch [9/10], Batch [6600/15033], Loss: 2.5750\n",
      "Epoch [9/10], Batch [6700/15033], Loss: 3.1635\n",
      "Epoch [9/10], Batch [6800/15033], Loss: 2.4758\n",
      "Epoch [9/10], Batch [6900/15033], Loss: 2.0041\n",
      "Epoch [9/10], Batch [7000/15033], Loss: 2.7186\n",
      "Epoch [9/10], Batch [7100/15033], Loss: 2.5815\n",
      "Epoch [9/10], Batch [7200/15033], Loss: 4.2381\n",
      "Epoch [9/10], Batch [7300/15033], Loss: 3.9564\n",
      "Epoch [9/10], Batch [7400/15033], Loss: 4.0249\n",
      "Epoch [9/10], Batch [7500/15033], Loss: 2.2513\n",
      "Epoch [9/10], Batch [7600/15033], Loss: 3.6770\n",
      "Epoch [9/10], Batch [7700/15033], Loss: 3.7779\n",
      "Epoch [9/10], Batch [7800/15033], Loss: 3.4365\n",
      "Epoch [9/10], Batch [7900/15033], Loss: 2.8046\n",
      "Epoch [9/10], Batch [8000/15033], Loss: 2.8432\n",
      "Epoch [9/10], Batch [8100/15033], Loss: 2.9304\n",
      "Epoch [9/10], Batch [8200/15033], Loss: 3.0271\n",
      "Epoch [9/10], Batch [8300/15033], Loss: 3.3688\n",
      "Epoch [9/10], Batch [8400/15033], Loss: 3.1635\n",
      "Epoch [9/10], Batch [8500/15033], Loss: 4.5063\n",
      "Epoch [9/10], Batch [8600/15033], Loss: 2.7385\n",
      "Epoch [9/10], Batch [8700/15033], Loss: 3.9657\n",
      "Epoch [9/10], Batch [8800/15033], Loss: 2.9576\n",
      "Epoch [9/10], Batch [8900/15033], Loss: 3.2318\n",
      "Epoch [9/10], Batch [9000/15033], Loss: 3.8990\n",
      "Epoch [9/10], Batch [9100/15033], Loss: 2.7163\n",
      "Epoch [9/10], Batch [9200/15033], Loss: 5.0822\n",
      "Epoch [9/10], Batch [9300/15033], Loss: 2.3082\n",
      "Epoch [9/10], Batch [9400/15033], Loss: 3.4066\n",
      "Epoch [9/10], Batch [9500/15033], Loss: 4.2568\n",
      "Epoch [9/10], Batch [9600/15033], Loss: 2.8288\n",
      "Epoch [9/10], Batch [9700/15033], Loss: 2.8741\n",
      "Epoch [9/10], Batch [9800/15033], Loss: 2.6067\n",
      "Epoch [9/10], Batch [9900/15033], Loss: 3.6784\n",
      "Epoch [9/10], Batch [10000/15033], Loss: 3.4866\n",
      "Epoch [9/10], Batch [10100/15033], Loss: 2.8567\n",
      "Epoch [9/10], Batch [10200/15033], Loss: 3.3171\n",
      "Epoch [9/10], Batch [10300/15033], Loss: 3.1010\n",
      "Epoch [9/10], Batch [10400/15033], Loss: 3.1933\n",
      "Epoch [9/10], Batch [10500/15033], Loss: 4.3687\n",
      "Epoch [9/10], Batch [10600/15033], Loss: 2.9562\n",
      "Epoch [9/10], Batch [10700/15033], Loss: 3.5180\n",
      "Epoch [9/10], Batch [10800/15033], Loss: 2.9877\n",
      "Epoch [9/10], Batch [10900/15033], Loss: 2.9526\n",
      "Epoch [9/10], Batch [11000/15033], Loss: 2.4544\n",
      "Epoch [9/10], Batch [11100/15033], Loss: 2.5798\n",
      "Epoch [9/10], Batch [11200/15033], Loss: 2.5802\n",
      "Epoch [9/10], Batch [11300/15033], Loss: 3.0317\n",
      "Epoch [9/10], Batch [11400/15033], Loss: 2.7670\n",
      "Epoch [9/10], Batch [11500/15033], Loss: 4.3975\n",
      "Epoch [9/10], Batch [11600/15033], Loss: 3.5351\n",
      "Epoch [9/10], Batch [11700/15033], Loss: 3.7805\n",
      "Epoch [9/10], Batch [11800/15033], Loss: 3.4674\n",
      "Epoch [9/10], Batch [11900/15033], Loss: 2.8577\n",
      "Epoch [9/10], Batch [12000/15033], Loss: 2.0041\n",
      "Epoch [9/10], Batch [12100/15033], Loss: 3.4980\n",
      "Epoch [9/10], Batch [12200/15033], Loss: 3.0681\n",
      "Epoch [9/10], Batch [12300/15033], Loss: 3.3398\n",
      "Epoch [9/10], Batch [12400/15033], Loss: 3.3318\n",
      "Epoch [9/10], Batch [12500/15033], Loss: 3.7306\n",
      "Epoch [9/10], Batch [12600/15033], Loss: 3.4459\n",
      "Epoch [9/10], Batch [12700/15033], Loss: 3.2405\n",
      "Epoch [9/10], Batch [12800/15033], Loss: 4.7626\n",
      "Epoch [9/10], Batch [12900/15033], Loss: 3.4502\n",
      "Epoch [9/10], Batch [13000/15033], Loss: 3.0364\n",
      "Epoch [9/10], Batch [13100/15033], Loss: 2.8960\n",
      "Epoch [9/10], Batch [13200/15033], Loss: 3.5820\n",
      "Epoch [9/10], Batch [13300/15033], Loss: 3.5592\n",
      "Epoch [9/10], Batch [13400/15033], Loss: 2.6210\n",
      "Epoch [9/10], Batch [13500/15033], Loss: 4.0422\n",
      "Epoch [9/10], Batch [13600/15033], Loss: 3.5260\n",
      "Epoch [9/10], Batch [13700/15033], Loss: 2.6464\n",
      "Epoch [9/10], Batch [13800/15033], Loss: 4.1736\n",
      "Epoch [9/10], Batch [13900/15033], Loss: 3.0060\n",
      "Epoch [9/10], Batch [14000/15033], Loss: 4.1196\n",
      "Epoch [9/10], Batch [14100/15033], Loss: 2.5736\n",
      "Epoch [9/10], Batch [14200/15033], Loss: 3.8306\n",
      "Epoch [9/10], Batch [14300/15033], Loss: 3.5592\n",
      "Epoch [9/10], Batch [14400/15033], Loss: 5.0519\n",
      "Epoch [9/10], Batch [14500/15033], Loss: 3.2745\n",
      "Epoch [9/10], Batch [14600/15033], Loss: 2.7266\n",
      "Epoch [9/10], Batch [14700/15033], Loss: 3.2644\n",
      "Epoch [9/10], Batch [14800/15033], Loss: 2.7319\n",
      "Epoch [9/10], Batch [14900/15033], Loss: 3.4164\n",
      "Epoch [9/10], Batch [15000/15033], Loss: 3.5826\n",
      "Epoch [9/10] completed, Average Loss: 3.1523\n",
      "Epoch [10/10], Batch [100/15033], Loss: 2.6731\n",
      "Epoch [10/10], Batch [200/15033], Loss: 3.0353\n",
      "Epoch [10/10], Batch [300/15033], Loss: 2.8684\n",
      "Epoch [10/10], Batch [400/15033], Loss: 3.2116\n",
      "Epoch [10/10], Batch [500/15033], Loss: 3.6120\n",
      "Epoch [10/10], Batch [600/15033], Loss: 2.4110\n",
      "Epoch [10/10], Batch [700/15033], Loss: 3.7861\n",
      "Epoch [10/10], Batch [800/15033], Loss: 3.1384\n",
      "Epoch [10/10], Batch [900/15033], Loss: 2.8395\n",
      "Epoch [10/10], Batch [1000/15033], Loss: 3.7691\n",
      "Epoch [10/10], Batch [1100/15033], Loss: 2.7598\n",
      "Epoch [10/10], Batch [1200/15033], Loss: 3.2767\n",
      "Epoch [10/10], Batch [1300/15033], Loss: 2.7454\n",
      "Epoch [10/10], Batch [1400/15033], Loss: 3.8268\n",
      "Epoch [10/10], Batch [1500/15033], Loss: 2.4912\n",
      "Epoch [10/10], Batch [1600/15033], Loss: 4.7709\n",
      "Epoch [10/10], Batch [1700/15033], Loss: 3.4096\n",
      "Epoch [10/10], Batch [1800/15033], Loss: 3.8891\n",
      "Epoch [10/10], Batch [1900/15033], Loss: 3.8386\n",
      "Epoch [10/10], Batch [2000/15033], Loss: 3.0489\n",
      "Epoch [10/10], Batch [2100/15033], Loss: 3.5426\n",
      "Epoch [10/10], Batch [2200/15033], Loss: 2.6753\n",
      "Epoch [10/10], Batch [2300/15033], Loss: 1.5938\n",
      "Epoch [10/10], Batch [2400/15033], Loss: 3.9303\n",
      "Epoch [10/10], Batch [2500/15033], Loss: 3.1086\n",
      "Epoch [10/10], Batch [2600/15033], Loss: 3.8272\n",
      "Epoch [10/10], Batch [2700/15033], Loss: 3.5135\n",
      "Epoch [10/10], Batch [2800/15033], Loss: 2.6338\n",
      "Epoch [10/10], Batch [2900/15033], Loss: 3.5952\n",
      "Epoch [10/10], Batch [3000/15033], Loss: 2.0422\n",
      "Epoch [10/10], Batch [3100/15033], Loss: 3.4515\n",
      "Epoch [10/10], Batch [3200/15033], Loss: 3.3182\n",
      "Epoch [10/10], Batch [3300/15033], Loss: 3.0163\n",
      "Epoch [10/10], Batch [3400/15033], Loss: 3.4048\n",
      "Epoch [10/10], Batch [3500/15033], Loss: 4.6321\n",
      "Epoch [10/10], Batch [3600/15033], Loss: 2.4264\n",
      "Epoch [10/10], Batch [3700/15033], Loss: 3.7572\n",
      "Epoch [10/10], Batch [3800/15033], Loss: 3.5238\n",
      "Epoch [10/10], Batch [3900/15033], Loss: 2.3648\n",
      "Epoch [10/10], Batch [4000/15033], Loss: 2.3488\n",
      "Epoch [10/10], Batch [4100/15033], Loss: 3.2468\n",
      "Epoch [10/10], Batch [4200/15033], Loss: 3.7699\n",
      "Epoch [10/10], Batch [4300/15033], Loss: 3.3220\n",
      "Epoch [10/10], Batch [4400/15033], Loss: 2.5106\n",
      "Epoch [10/10], Batch [4500/15033], Loss: 2.4017\n",
      "Epoch [10/10], Batch [4600/15033], Loss: 2.9665\n",
      "Epoch [10/10], Batch [4700/15033], Loss: 2.9585\n",
      "Epoch [10/10], Batch [4800/15033], Loss: 2.5035\n",
      "Epoch [10/10], Batch [4900/15033], Loss: 3.2632\n",
      "Epoch [10/10], Batch [5000/15033], Loss: 2.6372\n",
      "Epoch [10/10], Batch [5100/15033], Loss: 3.2057\n",
      "Epoch [10/10], Batch [5200/15033], Loss: 3.1312\n",
      "Epoch [10/10], Batch [5300/15033], Loss: 3.0146\n",
      "Epoch [10/10], Batch [5400/15033], Loss: 2.1978\n",
      "Epoch [10/10], Batch [5500/15033], Loss: 4.5393\n",
      "Epoch [10/10], Batch [5600/15033], Loss: 3.2285\n",
      "Epoch [10/10], Batch [5700/15033], Loss: 2.3540\n",
      "Epoch [10/10], Batch [5800/15033], Loss: 2.6742\n",
      "Epoch [10/10], Batch [5900/15033], Loss: 3.0024\n",
      "Epoch [10/10], Batch [6000/15033], Loss: 3.0617\n",
      "Epoch [10/10], Batch [6100/15033], Loss: 2.7118\n",
      "Epoch [10/10], Batch [6200/15033], Loss: 2.6305\n",
      "Epoch [10/10], Batch [6300/15033], Loss: 2.8687\n",
      "Epoch [10/10], Batch [6400/15033], Loss: 3.1226\n",
      "Epoch [10/10], Batch [6500/15033], Loss: 3.1969\n",
      "Epoch [10/10], Batch [6600/15033], Loss: 2.8495\n",
      "Epoch [10/10], Batch [6700/15033], Loss: 4.1632\n",
      "Epoch [10/10], Batch [6800/15033], Loss: 2.8241\n",
      "Epoch [10/10], Batch [6900/15033], Loss: 2.6572\n",
      "Epoch [10/10], Batch [7000/15033], Loss: 3.0115\n",
      "Epoch [10/10], Batch [7100/15033], Loss: 2.5184\n",
      "Epoch [10/10], Batch [7200/15033], Loss: 2.7340\n",
      "Epoch [10/10], Batch [7300/15033], Loss: 3.6537\n",
      "Epoch [10/10], Batch [7400/15033], Loss: 2.6488\n",
      "Epoch [10/10], Batch [7500/15033], Loss: 2.6088\n",
      "Epoch [10/10], Batch [7600/15033], Loss: 3.3200\n",
      "Epoch [10/10], Batch [7700/15033], Loss: 4.0074\n",
      "Epoch [10/10], Batch [7800/15033], Loss: 3.3167\n",
      "Epoch [10/10], Batch [7900/15033], Loss: 2.5254\n",
      "Epoch [10/10], Batch [8000/15033], Loss: 3.2457\n",
      "Epoch [10/10], Batch [8100/15033], Loss: 4.4089\n",
      "Epoch [10/10], Batch [8200/15033], Loss: 3.6390\n",
      "Epoch [10/10], Batch [8300/15033], Loss: 3.1575\n",
      "Epoch [10/10], Batch [8400/15033], Loss: 3.0605\n",
      "Epoch [10/10], Batch [8500/15033], Loss: 2.2777\n",
      "Epoch [10/10], Batch [8600/15033], Loss: 3.3631\n",
      "Epoch [10/10], Batch [8700/15033], Loss: 3.5847\n",
      "Epoch [10/10], Batch [8800/15033], Loss: 2.6870\n",
      "Epoch [10/10], Batch [8900/15033], Loss: 2.7407\n",
      "Epoch [10/10], Batch [9000/15033], Loss: 3.2288\n",
      "Epoch [10/10], Batch [9100/15033], Loss: 3.2587\n",
      "Epoch [10/10], Batch [9200/15033], Loss: 2.8537\n",
      "Epoch [10/10], Batch [9300/15033], Loss: 3.4287\n",
      "Epoch [10/10], Batch [9400/15033], Loss: 3.0900\n",
      "Epoch [10/10], Batch [9500/15033], Loss: 3.8413\n",
      "Epoch [10/10], Batch [9600/15033], Loss: 2.6993\n",
      "Epoch [10/10], Batch [9700/15033], Loss: 3.6132\n",
      "Epoch [10/10], Batch [9800/15033], Loss: 3.1771\n",
      "Epoch [10/10], Batch [9900/15033], Loss: 2.4925\n",
      "Epoch [10/10], Batch [10000/15033], Loss: 2.6751\n",
      "Epoch [10/10], Batch [10100/15033], Loss: 2.6425\n",
      "Epoch [10/10], Batch [10200/15033], Loss: 4.5234\n",
      "Epoch [10/10], Batch [10300/15033], Loss: 2.7111\n",
      "Epoch [10/10], Batch [10400/15033], Loss: 3.4691\n",
      "Epoch [10/10], Batch [10500/15033], Loss: 4.6581\n",
      "Epoch [10/10], Batch [10600/15033], Loss: 3.6732\n",
      "Epoch [10/10], Batch [10700/15033], Loss: 3.3213\n",
      "Epoch [10/10], Batch [10800/15033], Loss: 3.2407\n",
      "Epoch [10/10], Batch [10900/15033], Loss: 2.3458\n",
      "Epoch [10/10], Batch [11000/15033], Loss: 2.8473\n",
      "Epoch [10/10], Batch [11100/15033], Loss: 3.5405\n",
      "Epoch [10/10], Batch [11200/15033], Loss: 3.9035\n",
      "Epoch [10/10], Batch [11300/15033], Loss: 3.4720\n",
      "Epoch [10/10], Batch [11400/15033], Loss: 2.8821\n",
      "Epoch [10/10], Batch [11500/15033], Loss: 3.5089\n",
      "Epoch [10/10], Batch [11600/15033], Loss: 3.4042\n",
      "Epoch [10/10], Batch [11700/15033], Loss: 3.3443\n",
      "Epoch [10/10], Batch [11800/15033], Loss: 2.9990\n",
      "Epoch [10/10], Batch [11900/15033], Loss: 2.1671\n",
      "Epoch [10/10], Batch [12000/15033], Loss: 2.0230\n",
      "Epoch [10/10], Batch [12100/15033], Loss: 1.6709\n",
      "Epoch [10/10], Batch [12200/15033], Loss: 3.4027\n",
      "Epoch [10/10], Batch [12300/15033], Loss: 2.9401\n",
      "Epoch [10/10], Batch [12400/15033], Loss: 2.9971\n",
      "Epoch [10/10], Batch [12500/15033], Loss: 2.8938\n",
      "Epoch [10/10], Batch [12600/15033], Loss: 2.7466\n",
      "Epoch [10/10], Batch [12700/15033], Loss: 3.5510\n",
      "Epoch [10/10], Batch [12800/15033], Loss: 2.2803\n",
      "Epoch [10/10], Batch [12900/15033], Loss: 3.9731\n",
      "Epoch [10/10], Batch [13000/15033], Loss: 3.0847\n",
      "Epoch [10/10], Batch [13100/15033], Loss: 4.2281\n",
      "Epoch [10/10], Batch [13200/15033], Loss: 3.2006\n",
      "Epoch [10/10], Batch [13300/15033], Loss: 3.4575\n",
      "Epoch [10/10], Batch [13400/15033], Loss: 3.1937\n",
      "Epoch [10/10], Batch [13500/15033], Loss: 3.0686\n",
      "Epoch [10/10], Batch [13600/15033], Loss: 2.7113\n",
      "Epoch [10/10], Batch [13700/15033], Loss: 2.6208\n",
      "Epoch [10/10], Batch [13800/15033], Loss: 3.2274\n",
      "Epoch [10/10], Batch [13900/15033], Loss: 3.9110\n",
      "Epoch [10/10], Batch [14000/15033], Loss: 2.7861\n",
      "Epoch [10/10], Batch [14100/15033], Loss: 3.3072\n",
      "Epoch [10/10], Batch [14200/15033], Loss: 2.8554\n",
      "Epoch [10/10], Batch [14300/15033], Loss: 3.1545\n",
      "Epoch [10/10], Batch [14400/15033], Loss: 2.9727\n",
      "Epoch [10/10], Batch [14500/15033], Loss: 3.5373\n",
      "Epoch [10/10], Batch [14600/15033], Loss: 2.4126\n",
      "Epoch [10/10], Batch [14700/15033], Loss: 2.9143\n",
      "Epoch [10/10], Batch [14800/15033], Loss: 3.6727\n",
      "Epoch [10/10], Batch [14900/15033], Loss: 3.1545\n",
      "Epoch [10/10], Batch [15000/15033], Loss: 3.3998\n",
      "Epoch [10/10] completed, Average Loss: 3.1148\n",
      "Predicted next character:  \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Create custom Dataset for character sequences\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, sequences):\n",
    "        self.sequences = sequences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence, target = self.sequences[idx]\n",
    "        return torch.tensor(sequence), torch.tensor(target)\n",
    "\n",
    "# Define LSTM model for character-level prediction\n",
    "class NextCharLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
    "        super(NextCharLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size)  # Layer normalization for stability\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, lengths=None):\n",
    "        x = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        lstm_out = torch.mean(lstm_out, dim=1)  # Mean pooling across the sequence\n",
    "        lstm_out = self.layer_norm(lstm_out)\n",
    "        out = self.fc(lstm_out)\n",
    "        return out\n",
    "\n",
    "# Training loop with gradient clipping and learning rate scheduling\n",
    "def train_model(model, train_loader, criterion, optimizer, scheduler, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch_idx, (sequences, targets) in enumerate(train_loader):\n",
    "            sequences, targets = sequences.to(device), targets.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(sequences)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # Apply gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Log progress every 100 batches\n",
    "            if (batch_idx + 1) % 100 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "        # Log epoch summary\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}] completed, Average Loss: {avg_loss:.4f}')\n",
    "\n",
    "        # Step the learning rate scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "# Predict the next character\n",
    "def predict_next_char(model, sequence, char_to_idx, idx_to_char):\n",
    "    model.eval()\n",
    "    sequence = torch.tensor(sequence).unsqueeze(0).to(device)  # Add batch dimension\n",
    "    with torch.no_grad():\n",
    "        output = model(sequence)\n",
    "        predicted_idx = torch.argmax(output, dim=1).item()\n",
    "    return idx_to_char[predicted_idx]\n",
    "\n",
    "# Build character-level vocabulary\n",
    "def build_char_vocab(texts):\n",
    "    all_text = ''.join(texts)\n",
    "    chars = sorted(set(all_text))\n",
    "    char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "    idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "    return char_to_idx, idx_to_char\n",
    "\n",
    "# Encode sequences as character indices\n",
    "def encode_char_sequences(texts, char_to_idx, seq_length=100):\n",
    "    sequences = []\n",
    "    for text in texts:\n",
    "        encoded_text = [char_to_idx[char] for char in text]\n",
    "        for i in range(0, len(encoded_text) - seq_length):\n",
    "            input_seq = encoded_text[i:i + seq_length]\n",
    "            target = encoded_text[i + seq_length]\n",
    "            sequences.append((input_seq, target))\n",
    "    return sequences\n",
    "\n",
    "# Read data from all text files in the 'x' folder\n",
    "def get_all_text_files(folder_path):\n",
    "    text_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "    if not text_files:\n",
    "        raise FileNotFoundError(f\"No text files found in {folder_path}\")\n",
    "    return [os.path.join(folder_path, f) for f in text_files]\n",
    "\n",
    "# Load text data\n",
    "text_file_paths = get_all_text_files('/home/m/dev/scaleout/data/x')\n",
    "texts = []\n",
    "max_files = 1\n",
    "\n",
    "for file_path in text_file_paths[:max_files]:\n",
    "    text_data = pd.read_csv(file_path)\n",
    "    texts.extend(text_data['text'].tolist())\n",
    "\n",
    "# Remove empty lines and strip whitespace\n",
    "texts = [text.strip() for text in texts if text.strip()]\n",
    "\n",
    "# Build character-level vocabulary and encode sequences\n",
    "char_to_idx, idx_to_char = build_char_vocab(texts)\n",
    "sequences = encode_char_sequences(texts, char_to_idx, seq_length=100)  # Use character sequences of length 100\n",
    "\n",
    "# Create Dataset and DataLoader\n",
    "dataset = CharDataset(sequences)\n",
    "train_loader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Model parameters\n",
    "vocab_size = len(char_to_idx)\n",
    "embed_size = 128\n",
    "hidden_size = 256\n",
    "num_layers = 2\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize model\n",
    "model = NextCharLSTM(vocab_size, embed_size, hidden_size, num_layers).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.9)\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, criterion, optimizer, scheduler, num_epochs=10)\n",
    "\n",
    "# Test prediction (example sequence)\n",
    "example_sequence = [char_to_idx[ch] for ch in \"hello there, how d\"]\n",
    "predicted_char = predict_next_char(model, example_sequence, char_to_idx, idx_to_char)\n",
    "print(f'Predicted next character: {predicted_char}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted text:  \n"
     ]
    }
   ],
   "source": [
    "example_sequence = [char_to_idx[ch] for ch in \"hello there, how\"]\n",
    "predicted_text = \"\"\n",
    "current_sequence = example_sequence.copy()\n",
    "\n",
    "while True:\n",
    "    predicted_char = predict_next_char(model, current_sequence, char_to_idx, idx_to_char)\n",
    "    predicted_text += predicted_char\n",
    "    \n",
    "    if predicted_char in [' ', '.', '!', '?']:\n",
    "        break\n",
    "    \n",
    "    current_sequence = current_sequence[1:] + [char_to_idx[predicted_char]]\n",
    "\n",
    "print(f'Predicted text: {predicted_text}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to next_word_lstm_model.pth\n",
      "Vocabulary saved to vocabulary.json\n",
      "Vocabulary Length: 56116\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# Save the trained model\n",
    "model_save_path = 'next_word_lstm_model.pth'\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "# Save the vocabulary (word_to_idx dictionary)\n",
    "vocab_save_path = 'vocabulary.json'\n",
    "with open(vocab_save_path, 'w') as f:\n",
    "    json.dump(word_to_idx, f)\n",
    "print(f\"Vocabulary saved to {vocab_save_path}\")\n",
    "print(f\"Vocabulary Length: {len(word_to_idx)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
